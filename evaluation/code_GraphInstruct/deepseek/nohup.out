INFO 04-23 07:35:12 config.py:510] This model supports multiple tasks: {'reward', 'score', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 04-23 07:35:12 config.py:1310] Defaulting to use mp for distributed inference
INFO 04-23 07:35:12 llm_engine.py:234] Initializing an LLM engine (v0.6.6) with config: model='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', speculative_config=None, tokenizer='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[1024,1016,1008,1000,992,984,976,968,960,952,944,936,928,920,912,904,896,888,880,872,864,856,848,840,832,824,816,808,800,792,784,776,768,760,752,744,736,728,720,712,704,696,688,680,672,664,656,648,640,632,624,616,608,600,592,584,576,568,560,552,544,536,528,520,512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":1024}, use_cached_outputs=False, 
WARNING 04-23 07:35:12 multiproc_worker_utils.py:312] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 04-23 07:35:12 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=2644291)[0;0m INFO 04-23 07:35:13 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2644291)[0;0m INFO 04-23 07:35:13 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2644292)[0;0m INFO 04-23 07:35:13 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2644292)[0;0m INFO 04-23 07:35:13 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2644293)[0;0m INFO 04-23 07:35:13 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2644293)[0;0m INFO 04-23 07:35:13 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
INFO 04-23 07:35:13 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2644291)[0;0m INFO 04-23 07:35:15 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2644293)[0;0m INFO 04-23 07:35:15 utils.py:918] Found nccl from library libnccl.so.2
INFO 04-23 07:35:15 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2644292)[0;0m INFO 04-23 07:35:15 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2644291)[0;0m INFO 04-23 07:35:15 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2644293)[0;0m INFO 04-23 07:35:15 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2644292)[0;0m INFO 04-23 07:35:15 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 04-23 07:35:15 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2644291)[0;0m WARNING 04-23 07:35:15 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2644293)[0;0m WARNING 04-23 07:35:15 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2644292)[0;0m WARNING 04-23 07:35:15 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 04-23 07:35:15 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 04-23 07:35:15 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_d0e5d36c'), local_subscribe_port=50779, remote_subscribe_port=None)
INFO 04-23 07:35:15 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2644292)[0;0m INFO 04-23 07:35:15 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2644291)[0;0m INFO 04-23 07:35:15 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2644293)[0;0m INFO 04-23 07:35:15 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.98s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.38s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:11<00:03,  3.70s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.76s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.72s/it]

[1;36m(VllmWorkerProcess pid=2644291)[0;0m INFO 04-23 07:35:30 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2644292)[0;0m INFO 04-23 07:35:30 model_runner.py:1099] Loading model weights took 3.5546 GB
INFO 04-23 07:35:30 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2644293)[0;0m INFO 04-23 07:35:30 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2644292)[0;0m INFO 04-23 07:35:34 worker.py:241] Memory profiling takes 3.98 seconds
[1;36m(VllmWorkerProcess pid=2644292)[0;0m INFO 04-23 07:35:34 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2644292)[0;0m INFO 04-23 07:35:34 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
[1;36m(VllmWorkerProcess pid=2644291)[0;0m INFO 04-23 07:35:34 worker.py:241] Memory profiling takes 3.99 seconds
[1;36m(VllmWorkerProcess pid=2644291)[0;0m INFO 04-23 07:35:34 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2644291)[0;0m INFO 04-23 07:35:34 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
[1;36m(VllmWorkerProcess pid=2644293)[0;0m INFO 04-23 07:35:34 worker.py:241] Memory profiling takes 4.03 seconds
[1;36m(VllmWorkerProcess pid=2644293)[0;0m INFO 04-23 07:35:34 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2644293)[0;0m INFO 04-23 07:35:34 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.57GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.94GiB.
INFO 04-23 07:35:35 worker.py:241] Memory profiling takes 4.21 seconds
INFO 04-23 07:35:35 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
INFO 04-23 07:35:35 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.68GiB; PyTorch activation peak memory takes 5.74GiB; the rest of the memory reserved for KV Cache is 28.05GiB.
INFO 04-23 07:35:35 distributed_gpu_executor.py:57] # GPU blocks: 131317, # CPU blocks: 74898
INFO 04-23 07:35:35 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 64.12x
INFO 04-23 07:35:56 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/131 [00:00<?, ?it/s][1;36m(VllmWorkerProcess pid=2644291)[0;0m INFO 04-23 07:35:58 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2644293)[0;0m INFO 04-23 07:35:58 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2644292)[0;0m INFO 04-23 07:35:58 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   1%|          | 1/131 [00:02<05:24,  2.50s/it]Capturing CUDA graph shapes:   2%|▏         | 2/131 [00:03<03:01,  1.41s/it]Capturing CUDA graph shapes:   2%|▏         | 3/131 [00:03<02:24,  1.13s/it]Capturing CUDA graph shapes:   3%|▎         | 4/131 [00:04<02:01,  1.05it/s]Capturing CUDA graph shapes:   4%|▍         | 5/131 [00:05<01:44,  1.21it/s]Capturing CUDA graph shapes:   5%|▍         | 6/131 [00:05<01:34,  1.32it/s]Capturing CUDA graph shapes:   5%|▌         | 7/131 [00:06<01:27,  1.41it/s]Capturing CUDA graph shapes:   6%|▌         | 8/131 [00:07<01:23,  1.47it/s]Capturing CUDA graph shapes:   7%|▋         | 9/131 [00:07<01:20,  1.52it/s]Capturing CUDA graph shapes:   8%|▊         | 10/131 [00:08<01:16,  1.58it/s]Capturing CUDA graph shapes:   8%|▊         | 11/131 [00:08<01:15,  1.59it/s]Capturing CUDA graph shapes:   9%|▉         | 12/131 [00:09<01:13,  1.63it/s]Capturing CUDA graph shapes:  10%|▉         | 13/131 [00:10<01:12,  1.62it/s]Capturing CUDA graph shapes:  11%|█         | 14/131 [00:10<01:12,  1.62it/s]Capturing CUDA graph shapes:  11%|█▏        | 15/131 [00:11<01:10,  1.64it/s]Capturing CUDA graph shapes:  12%|█▏        | 16/131 [00:11<01:10,  1.63it/s]Capturing CUDA graph shapes:  13%|█▎        | 17/131 [00:12<01:10,  1.62it/s]Capturing CUDA graph shapes:  14%|█▎        | 18/131 [00:13<01:09,  1.63it/s]Capturing CUDA graph shapes:  15%|█▍        | 19/131 [00:13<01:07,  1.65it/s]Capturing CUDA graph shapes:  15%|█▌        | 20/131 [00:14<01:08,  1.63it/s]Capturing CUDA graph shapes:  16%|█▌        | 21/131 [00:14<01:07,  1.64it/s]Capturing CUDA graph shapes:  17%|█▋        | 22/131 [00:15<01:06,  1.63it/s]Capturing CUDA graph shapes:  18%|█▊        | 23/131 [00:16<01:06,  1.63it/s]Capturing CUDA graph shapes:  18%|█▊        | 24/131 [00:16<01:05,  1.64it/s]Capturing CUDA graph shapes:  19%|█▉        | 25/131 [00:17<01:04,  1.64it/s]Capturing CUDA graph shapes:  20%|█▉        | 26/131 [00:17<01:03,  1.66it/s]Capturing CUDA graph shapes:  21%|██        | 27/131 [00:18<01:02,  1.66it/s]Capturing CUDA graph shapes:  21%|██▏       | 28/131 [00:19<01:01,  1.67it/s]Capturing CUDA graph shapes:  22%|██▏       | 29/131 [00:19<01:01,  1.66it/s]Capturing CUDA graph shapes:  23%|██▎       | 30/131 [00:20<01:00,  1.67it/s]Capturing CUDA graph shapes:  24%|██▎       | 31/131 [00:20<01:00,  1.67it/s]Capturing CUDA graph shapes:  24%|██▍       | 32/131 [00:21<00:59,  1.67it/s]Capturing CUDA graph shapes:  25%|██▌       | 33/131 [00:22<00:58,  1.67it/s]Capturing CUDA graph shapes:  26%|██▌       | 34/131 [00:22<00:57,  1.69it/s]Capturing CUDA graph shapes:  27%|██▋       | 35/131 [00:23<00:57,  1.66it/s]Capturing CUDA graph shapes:  27%|██▋       | 36/131 [00:23<00:55,  1.70it/s]Capturing CUDA graph shapes:  28%|██▊       | 37/131 [00:24<00:55,  1.69it/s]Capturing CUDA graph shapes:  29%|██▉       | 38/131 [00:25<00:55,  1.68it/s]Capturing CUDA graph shapes:  30%|██▉       | 39/131 [00:25<00:54,  1.68it/s]Capturing CUDA graph shapes:  31%|███       | 40/131 [00:26<00:53,  1.69it/s]Capturing CUDA graph shapes:  31%|███▏      | 41/131 [00:26<00:53,  1.68it/s]Capturing CUDA graph shapes:  32%|███▏      | 42/131 [00:27<00:52,  1.69it/s]Capturing CUDA graph shapes:  33%|███▎      | 43/131 [00:28<00:51,  1.70it/s]Capturing CUDA graph shapes:  34%|███▎      | 44/131 [00:28<00:51,  1.69it/s]Capturing CUDA graph shapes:  34%|███▍      | 45/131 [00:29<00:50,  1.70it/s]Capturing CUDA graph shapes:  35%|███▌      | 46/131 [00:29<00:49,  1.70it/s]Capturing CUDA graph shapes:  36%|███▌      | 47/131 [00:30<00:49,  1.68it/s]Capturing CUDA graph shapes:  37%|███▋      | 48/131 [00:31<00:48,  1.71it/s]Capturing CUDA graph shapes:  37%|███▋      | 49/131 [00:31<00:47,  1.71it/s]Capturing CUDA graph shapes:  38%|███▊      | 50/131 [00:32<00:47,  1.70it/s]Capturing CUDA graph shapes:  39%|███▉      | 51/131 [00:32<00:46,  1.71it/s]Capturing CUDA graph shapes:  40%|███▉      | 52/131 [00:33<00:46,  1.68it/s]Capturing CUDA graph shapes:  40%|████      | 53/131 [00:33<00:45,  1.70it/s]Capturing CUDA graph shapes:  41%|████      | 54/131 [00:34<00:45,  1.71it/s]Capturing CUDA graph shapes:  42%|████▏     | 55/131 [00:35<00:44,  1.70it/s]Capturing CUDA graph shapes:  43%|████▎     | 56/131 [00:35<00:43,  1.71it/s]Capturing CUDA graph shapes:  44%|████▎     | 57/131 [00:36<00:43,  1.71it/s]Capturing CUDA graph shapes:  44%|████▍     | 58/131 [00:36<00:43,  1.70it/s]Capturing CUDA graph shapes:  45%|████▌     | 59/131 [00:37<00:42,  1.71it/s]Capturing CUDA graph shapes:  46%|████▌     | 60/131 [00:38<00:41,  1.73it/s]Capturing CUDA graph shapes:  47%|████▋     | 61/131 [00:38<00:40,  1.73it/s]Capturing CUDA graph shapes:  47%|████▋     | 62/131 [00:39<00:40,  1.72it/s]Capturing CUDA graph shapes:  48%|████▊     | 63/131 [00:39<00:39,  1.73it/s]Capturing CUDA graph shapes:  49%|████▉     | 64/131 [00:40<00:38,  1.72it/s]Capturing CUDA graph shapes:  50%|████▉     | 65/131 [00:40<00:37,  1.74it/s]Capturing CUDA graph shapes:  50%|█████     | 66/131 [00:41<00:37,  1.75it/s]Capturing CUDA graph shapes:  51%|█████     | 67/131 [00:42<00:36,  1.76it/s]Capturing CUDA graph shapes:  52%|█████▏    | 68/131 [00:42<00:36,  1.73it/s]Capturing CUDA graph shapes:  53%|█████▎    | 69/131 [00:43<00:35,  1.74it/s]Capturing CUDA graph shapes:  53%|█████▎    | 70/131 [00:43<00:35,  1.74it/s]Capturing CUDA graph shapes:  54%|█████▍    | 71/131 [00:44<00:34,  1.74it/s]Capturing CUDA graph shapes:  55%|█████▍    | 72/131 [00:44<00:33,  1.76it/s]Capturing CUDA graph shapes:  56%|█████▌    | 73/131 [00:45<00:33,  1.75it/s]Capturing CUDA graph shapes:  56%|█████▋    | 74/131 [00:46<00:32,  1.76it/s]Capturing CUDA graph shapes:  57%|█████▋    | 75/131 [00:46<00:31,  1.77it/s]Capturing CUDA graph shapes:  58%|█████▊    | 76/131 [00:47<00:31,  1.77it/s]Capturing CUDA graph shapes:  59%|█████▉    | 77/131 [00:47<00:30,  1.78it/s]Capturing CUDA graph shapes:  60%|█████▉    | 78/131 [00:48<00:29,  1.77it/s]Capturing CUDA graph shapes:  60%|██████    | 79/131 [00:48<00:29,  1.77it/s]Capturing CUDA graph shapes:  61%|██████    | 80/131 [00:49<00:28,  1.78it/s]Capturing CUDA graph shapes:  62%|██████▏   | 81/131 [00:50<00:28,  1.78it/s]Capturing CUDA graph shapes:  63%|██████▎   | 82/131 [00:50<00:27,  1.79it/s]Capturing CUDA graph shapes:  63%|██████▎   | 83/131 [00:51<00:26,  1.80it/s]Capturing CUDA graph shapes:  64%|██████▍   | 84/131 [00:51<00:25,  1.84it/s]Capturing CUDA graph shapes:  65%|██████▍   | 85/131 [00:52<00:25,  1.83it/s]Capturing CUDA graph shapes:  66%|██████▌   | 86/131 [00:52<00:24,  1.82it/s]Capturing CUDA graph shapes:  66%|██████▋   | 87/131 [00:53<00:24,  1.81it/s]Capturing CUDA graph shapes:  67%|██████▋   | 88/131 [00:53<00:23,  1.82it/s]Capturing CUDA graph shapes:  68%|██████▊   | 89/131 [00:54<00:23,  1.83it/s]Capturing CUDA graph shapes:  69%|██████▊   | 90/131 [00:54<00:22,  1.84it/s]Capturing CUDA graph shapes:  69%|██████▉   | 91/131 [00:55<00:21,  1.85it/s]Capturing CUDA graph shapes:  70%|███████   | 92/131 [00:55<00:21,  1.83it/s]Capturing CUDA graph shapes:  71%|███████   | 93/131 [00:56<00:20,  1.82it/s]Capturing CUDA graph shapes:  72%|███████▏  | 94/131 [00:57<00:20,  1.82it/s]Capturing CUDA graph shapes:  73%|███████▎  | 95/131 [00:57<00:19,  1.83it/s]Capturing CUDA graph shapes:  73%|███████▎  | 96/131 [00:58<00:19,  1.83it/s]Capturing CUDA graph shapes:  74%|███████▍  | 97/131 [00:58<00:18,  1.83it/s]Capturing CUDA graph shapes:  75%|███████▍  | 98/131 [00:59<00:18,  1.83it/s]Capturing CUDA graph shapes:  76%|███████▌  | 99/131 [00:59<00:17,  1.82it/s]Capturing CUDA graph shapes:  76%|███████▋  | 100/131 [01:00<00:17,  1.82it/s]Capturing CUDA graph shapes:  77%|███████▋  | 101/131 [01:00<00:16,  1.81it/s]Capturing CUDA graph shapes:  78%|███████▊  | 102/131 [01:01<00:15,  1.83it/s]Capturing CUDA graph shapes:  79%|███████▊  | 103/131 [01:02<00:15,  1.83it/s]Capturing CUDA graph shapes:  79%|███████▉  | 104/131 [01:02<00:14,  1.83it/s]Capturing CUDA graph shapes:  80%|████████  | 105/131 [01:03<00:14,  1.84it/s]Capturing CUDA graph shapes:  81%|████████  | 106/131 [01:03<00:13,  1.85it/s]Capturing CUDA graph shapes:  82%|████████▏ | 107/131 [01:04<00:12,  1.86it/s]Capturing CUDA graph shapes:  82%|████████▏ | 108/131 [01:04<00:12,  1.89it/s]Capturing CUDA graph shapes:  83%|████████▎ | 109/131 [01:05<00:11,  1.85it/s]Capturing CUDA graph shapes:  84%|████████▍ | 110/131 [01:05<00:11,  1.85it/s]Capturing CUDA graph shapes:  85%|████████▍ | 111/131 [01:06<00:10,  1.86it/s]Capturing CUDA graph shapes:  85%|████████▌ | 112/131 [01:06<00:10,  1.89it/s]Capturing CUDA graph shapes:  86%|████████▋ | 113/131 [01:07<00:09,  1.86it/s]Capturing CUDA graph shapes:  87%|████████▋ | 114/131 [01:07<00:09,  1.87it/s]Capturing CUDA graph shapes:  88%|████████▊ | 115/131 [01:08<00:08,  1.88it/s]Capturing CUDA graph shapes:  89%|████████▊ | 116/131 [01:08<00:07,  1.91it/s]Capturing CUDA graph shapes:  89%|████████▉ | 117/131 [01:09<00:07,  1.88it/s]Capturing CUDA graph shapes:  90%|█████████ | 118/131 [01:10<00:06,  1.90it/s]Capturing CUDA graph shapes:  91%|█████████ | 119/131 [01:10<00:06,  1.91it/s]Capturing CUDA graph shapes:  92%|█████████▏| 120/131 [01:11<00:05,  1.90it/s]Capturing CUDA graph shapes:  92%|█████████▏| 121/131 [01:11<00:05,  1.88it/s]Capturing CUDA graph shapes:  93%|█████████▎| 122/131 [01:12<00:04,  1.89it/s]Capturing CUDA graph shapes:  94%|█████████▍| 123/131 [01:12<00:04,  1.89it/s]Capturing CUDA graph shapes:  95%|█████████▍| 124/131 [01:13<00:03,  1.90it/s]Capturing CUDA graph shapes:  95%|█████████▌| 125/131 [01:13<00:03,  1.88it/s]Capturing CUDA graph shapes:  96%|█████████▌| 126/131 [01:14<00:02,  1.88it/s]Capturing CUDA graph shapes:  97%|█████████▋| 127/131 [01:14<00:02,  1.92it/s]Capturing CUDA graph shapes:  98%|█████████▊| 128/131 [01:15<00:01,  1.90it/s]Capturing CUDA graph shapes:  98%|█████████▊| 129/131 [01:15<00:01,  1.90it/s]Capturing CUDA graph shapes:  99%|█████████▉| 130/131 [01:16<00:00,  1.87it/s][1;36m(VllmWorkerProcess pid=2644292)[0;0m INFO 04-23 07:37:13 model_runner.py:1535] Graph capturing finished in 75 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2644293)[0;0m INFO 04-23 07:37:13 model_runner.py:1535] Graph capturing finished in 75 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2644291)[0;0m INFO 04-23 07:37:13 model_runner.py:1535] Graph capturing finished in 76 secs, took 3.72 GiB
Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:17<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:17<00:00,  1.70it/s]
INFO 04-23 07:37:14 model_runner.py:1535] Graph capturing finished in 77 secs, took 3.72 GiB
INFO 04-23 07:37:14 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 103.22 seconds
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 18125 examples [00:04, 3874.50 examples/s]Generating train split: 18125 examples [00:04, 3863.82 examples/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclAbilityTestResults.py:127: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: connectivity
Processed prompts:   0%|          | 0/2687 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2687 [01:57<87:58:45, 117.92s/it, est. speed input: 46.41 toks/s, output: 0.02 toks/s]Processed prompts:  38%|███▊      | 1025/2687 [04:09<05:47,  4.78it/s, est. speed input: 8218.72 toks/s, output: 8.21 toks/s]Processed prompts:  69%|██████▉   | 1858/2687 [06:24<02:32,  5.44it/s, est. speed input: 11081.07 toks/s, output: 9.65 toks/s]Processed prompts:  88%|████████▊ | 2355/2687 [07:57<01:01,  5.42it/s, est. speed input: 13454.00 toks/s, output: 9.87 toks/s]Processed prompts: 100%|██████████| 2687/2687 [07:57<00:00,  5.63it/s, est. speed input: 16524.26 toks/s, output: 11.26 toks/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclAbilityTestResults.py:127: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: cycle
Processed prompts:   0%|          | 0/2717 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2717 [02:12<100:18:45, 132.96s/it, est. speed input: 48.21 toks/s, output: 0.02 toks/s]Processed prompts:  26%|██▌       | 700/2717 [04:27<10:55,  3.08it/s, est. speed input: 8207.48 toks/s, output: 5.24 toks/s] Processed prompts:  51%|█████     | 1374/2717 [06:45<05:42,  3.92it/s, est. speed input: 10799.73 toks/s, output: 6.78 toks/s]Processed prompts:  70%|██████▉   | 1896/2717 [09:01<03:31,  3.89it/s, est. speed input: 12075.33 toks/s, output: 7.01 toks/s]Processed prompts:  86%|████████▌ | 2324/2717 [11:17<01:48,  3.61it/s, est. speed input: 12801.22 toks/s, output: 6.86 toks/s]Processed prompts:  99%|█████████▉| 2686/2717 [11:31<00:06,  4.74it/s, est. speed input: 15625.33 toks/s, output: 7.77 toks/s]Processed prompts: 100%|██████████| 2717/2717 [11:31<00:00,  3.93it/s, est. speed input: 15896.12 toks/s, output: 7.86 toks/s]
Evaluating task: flow
Processed prompts:   0%|          | 0/405 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/405 [02:17<15:27:29, 137.75s/it, est. speed input: 11.96 toks/s, output: 0.01 toks/s]Processed prompts:  98%|█████████▊| 398/405 [02:22<00:01,  3.93it/s, est. speed input: 15091.62 toks/s, output: 5.59 toks/s]Processed prompts: 100%|██████████| 405/405 [02:22<00:00,  2.84it/s, est. speed input: 15513.99 toks/s, output: 5.69 toks/s]
Evaluating task: bipartite
Processed prompts:   0%|          | 0/2013 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2013 [02:14<75:13:49, 134.61s/it, est. speed input: 9.96 toks/s, output: 0.01 toks/s]Processed prompts:  26%|██▌       | 520/2013 [04:30<11:00,  2.26it/s, est. speed input: 8013.78 toks/s, output: 3.85 toks/s]Processed prompts:  54%|█████▍    | 1089/2013 [06:46<04:54,  3.14it/s, est. speed input: 10651.82 toks/s, output: 5.36 toks/s]Processed prompts:  73%|███████▎  | 1476/2013 [09:03<02:58,  3.01it/s, est. speed input: 11896.36 toks/s, output: 5.43 toks/s]Processed prompts:  90%|█████████ | 1817/2013 [10:24<00:59,  3.31it/s, est. speed input: 13749.63 toks/s, output: 5.81 toks/s]Processed prompts: 100%|██████████| 2013/2013 [10:24<00:00,  3.22it/s, est. speed input: 15747.98 toks/s, output: 6.44 toks/s]
Evaluating task: hamilton
Processed prompts:   0%|          | 0/2097 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2097 [02:14<78:19:29, 134.53s/it, est. speed input: 8.18 toks/s, output: 0.01 toks/s]Processed prompts:  31%|███       | 645/2097 [04:29<08:35,  2.82it/s, est. speed input: 8131.25 toks/s, output: 4.79 toks/s]Processed prompts:  57%|█████▋    | 1199/2097 [06:47<04:26,  3.37it/s, est. speed input: 10711.27 toks/s, output: 5.89 toks/s]Processed prompts:  72%|███████▏  | 1517/2097 [09:07<03:18,  2.93it/s, est. speed input: 11850.04 toks/s, output: 5.54 toks/s]Processed prompts:  85%|████████▌ | 1791/2097 [11:27<01:58,  2.58it/s, est. speed input: 12535.51 toks/s, output: 5.21 toks/s]Processed prompts:  98%|█████████▊| 2045/2097 [11:54<00:16,  3.20it/s, est. speed input: 15012.47 toks/s, output: 5.72 toks/s]Processed prompts: 100%|██████████| 2097/2097 [11:54<00:00,  2.94it/s, est. speed input: 15586.88 toks/s, output: 5.87 toks/s]
Evaluating task: triangle
Processed prompts:   0%|          | 0/2756 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2756 [02:14<102:39:43, 134.15s/it, est. speed input: 66.09 toks/s, output: 0.01 toks/s]Processed prompts:  22%|██▏       | 600/2756 [04:29<13:44,  2.61it/s, est. speed input: 8106.10 toks/s, output: 4.45 toks/s] Processed prompts:  43%|████▎     | 1197/2756 [06:44<07:32,  3.44it/s, est. speed input: 10781.47 toks/s, output: 5.92 toks/s]Processed prompts:  64%|██████▎   | 1756/2756 [09:00<04:29,  3.71it/s, est. speed input: 12104.42 toks/s, output: 6.50 toks/s]Processed prompts:  79%|███████▉  | 2179/2756 [11:18<02:45,  3.48it/s, est. speed input: 12813.89 toks/s, output: 6.43 toks/s]Processed prompts:  93%|█████████▎| 2553/2756 [12:40<00:54,  3.73it/s, est. speed input: 14245.49 toks/s, output: 6.72 toks/s]Processed prompts: 100%|██████████| 2756/2756 [12:40<00:00,  3.63it/s, est. speed input: 15917.83 toks/s, output: 7.25 toks/s]
Evaluating task: shortest
Processed prompts:   0%|          | 0/1392 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/1392 [02:16<52:54:44, 136.94s/it, est. speed input: 41.87 toks/s, output: 0.01 toks/s]Processed prompts:  27%|██▋       | 378/1392 [04:34<10:25,  1.62it/s, est. speed input: 7803.03 toks/s, output: 2.76 toks/s]Processed prompts:  54%|█████▍    | 757/1392 [06:52<04:57,  2.14it/s, est. speed input: 10364.37 toks/s, output: 3.67 toks/s]Processed prompts:  81%|████████  | 1126/1392 [09:01<01:50,  2.41it/s, est. speed input: 11841.54 toks/s, output: 4.16 toks/s]Processed prompts: 100%|██████████| 1392/1392 [09:01<00:00,  2.57it/s, est. speed input: 15458.17 toks/s, output: 5.14 toks/s]
Evaluating task: topology
Processed prompts:   0%|          | 0/872 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/872 [02:17<33:12:38, 137.27s/it, est. speed input: 67.45 toks/s, output: 0.01 toks/s]Processed prompts:  40%|████      | 352/872 [04:37<05:49,  1.49it/s, est. speed input: 7719.41 toks/s, output: 2.54 toks/s]Processed prompts:  72%|███████▏  | 632/872 [06:57<02:19,  1.72it/s, est. speed input: 10201.64 toks/s, output: 3.03 toks/s]Processed prompts:  98%|█████████▊| 852/872 [07:11<00:07,  2.58it/s, est. speed input: 14762.00 toks/s, output: 3.95 toks/s]Processed prompts: 100%|██████████| 872/872 [07:11<00:00,  2.02it/s, est. speed input: 15218.14 toks/s, output: 4.04 toks/s]
Evaluating task: substructure
Processed prompts:   0%|          | 0/3186 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/3186 [02:18<122:10:20, 138.09s/it, est. speed input: 21.49 toks/s, output: 0.01 toks/s]Processed prompts:  11%|█▏        | 359/3186 [04:36<30:52,  1.53it/s, est. speed input: 7783.93 toks/s, output: 2.60 toks/s] Processed prompts:  32%|███▏      | 1014/3186 [06:54<12:04,  3.00it/s, est. speed input: 10492.27 toks/s, output: 4.89 toks/s]Processed prompts:  44%|████▍     | 1403/3186 [09:12<10:08,  2.93it/s, est. speed input: 11760.33 toks/s, output: 5.08 toks/s]Processed prompts:  60%|██████    | 1916/3186 [11:30<06:36,  3.21it/s, est. speed input: 12526.75 toks/s, output: 5.55 toks/s]Processed prompts:  75%|███████▍  | 2375/3186 [13:48<04:09,  3.25it/s, est. speed input: 13045.71 toks/s, output: 5.73 toks/s]Processed prompts:  88%|████████▊ | 2798/3186 [15:20<01:48,  3.58it/s, est. speed input: 14085.64 toks/s, output: 6.08 toks/s]Processed prompts: 100%|██████████| 3186/3186 [15:20<00:00,  3.46it/s, est. speed input: 15673.59 toks/s, output: 6.92 toks/s]
Accuracy data has been saved to 'task_accuracies.txt'.
INFO 04-23 09:08:01 multiproc_worker_utils.py:140] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=2644292)[0;0m INFO 04-23 09:08:01 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2644293)[0;0m INFO 04-23 09:08:01 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2644291)[0;0m INFO 04-23 09:08:01 multiproc_worker_utils.py:247] Worker exiting
[rank0]:[W423 09:08:23.725410455 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/home/zch/anaconda3/envs/GraphICL/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO 04-23 09:08:44 config.py:510] This model supports multiple tasks: {'generate', 'classify', 'embed', 'score', 'reward'}. Defaulting to 'generate'.
INFO 04-23 09:08:44 config.py:1310] Defaulting to use mp for distributed inference
INFO 04-23 09:08:44 llm_engine.py:234] Initializing an LLM engine (v0.6.6) with config: model='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', speculative_config=None, tokenizer='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[1024,1016,1008,1000,992,984,976,968,960,952,944,936,928,920,912,904,896,888,880,872,864,856,848,840,832,824,816,808,800,792,784,776,768,760,752,744,736,728,720,712,704,696,688,680,672,664,656,648,640,632,624,616,608,600,592,584,576,568,560,552,544,536,528,520,512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":1024}, use_cached_outputs=False, 
WARNING 04-23 09:08:45 multiproc_worker_utils.py:312] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 04-23 09:08:45 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 04-23 09:08:46 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2662886)[0;0m INFO 04-23 09:08:46 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2662886)[0;0m INFO 04-23 09:08:46 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2662885)[0;0m INFO 04-23 09:08:46 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2662885)[0;0m INFO 04-23 09:08:46 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2662887)[0;0m INFO 04-23 09:08:46 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2662887)[0;0m INFO 04-23 09:08:46 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2662887)[0;0m INFO 04-23 09:08:47 utils.py:918] Found nccl from library libnccl.so.2
INFO 04-23 09:08:47 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2662885)[0;0m INFO 04-23 09:08:47 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2662886)[0;0m INFO 04-23 09:08:47 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2662886)[0;0m INFO 04-23 09:08:47 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 04-23 09:08:47 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2662885)[0;0m INFO 04-23 09:08:47 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2662887)[0;0m INFO 04-23 09:08:47 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2662885)[0;0m WARNING 04-23 09:08:47 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2662886)[0;0m WARNING 04-23 09:08:47 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 04-23 09:08:47 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2662887)[0;0m WARNING 04-23 09:08:47 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 04-23 09:08:47 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_5da664c4'), local_subscribe_port=39793, remote_subscribe_port=None)
INFO 04-23 09:08:47 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2662885)[0;0m INFO 04-23 09:08:47 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2662886)[0;0m INFO 04-23 09:08:47 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2662887)[0;0m INFO 04-23 09:08:47 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:02,  1.35s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.07it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.42it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.02it/s]

INFO 04-23 09:08:52 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2662885)[0;0m INFO 04-23 09:08:52 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2662886)[0;0m INFO 04-23 09:08:52 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2662887)[0;0m INFO 04-23 09:08:52 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2662886)[0;0m INFO 04-23 09:08:57 worker.py:241] Memory profiling takes 4.71 seconds
[1;36m(VllmWorkerProcess pid=2662886)[0;0m INFO 04-23 09:08:57 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2662886)[0;0m INFO 04-23 09:08:57 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
[1;36m(VllmWorkerProcess pid=2662885)[0;0m INFO 04-23 09:08:57 worker.py:241] Memory profiling takes 4.71 seconds
[1;36m(VllmWorkerProcess pid=2662885)[0;0m INFO 04-23 09:08:57 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2662885)[0;0m INFO 04-23 09:08:57 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
[1;36m(VllmWorkerProcess pid=2662887)[0;0m INFO 04-23 09:08:57 worker.py:241] Memory profiling takes 4.75 seconds
[1;36m(VllmWorkerProcess pid=2662887)[0;0m INFO 04-23 09:08:57 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2662887)[0;0m INFO 04-23 09:08:57 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.57GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.94GiB.
INFO 04-23 09:08:57 worker.py:241] Memory profiling takes 4.83 seconds
INFO 04-23 09:08:57 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
INFO 04-23 09:08:57 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.68GiB; PyTorch activation peak memory takes 5.74GiB; the rest of the memory reserved for KV Cache is 28.05GiB.
INFO 04-23 09:08:57 distributed_gpu_executor.py:57] # GPU blocks: 131317, # CPU blocks: 74898
INFO 04-23 09:08:57 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 64.12x
[1;36m(VllmWorkerProcess pid=2662887)[0;0m INFO 04-23 09:09:20 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2662885)[0;0m INFO 04-23 09:09:20 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-23 09:09:20 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2662886)[0;0m INFO 04-23 09:09:20 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/131 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|          | 1/131 [00:00<01:22,  1.58it/s]Capturing CUDA graph shapes:   2%|▏         | 2/131 [00:01<01:25,  1.51it/s]Capturing CUDA graph shapes:   2%|▏         | 3/131 [00:01<01:18,  1.63it/s]Capturing CUDA graph shapes:   3%|▎         | 4/131 [00:02<01:17,  1.64it/s]Capturing CUDA graph shapes:   4%|▍         | 5/131 [00:03<01:14,  1.70it/s]Capturing CUDA graph shapes:   5%|▍         | 6/131 [00:03<01:14,  1.67it/s]Capturing CUDA graph shapes:   5%|▌         | 7/131 [00:04<01:12,  1.70it/s]Capturing CUDA graph shapes:   6%|▌         | 8/131 [00:04<01:11,  1.71it/s]Capturing CUDA graph shapes:   7%|▋         | 9/131 [00:05<01:11,  1.72it/s]Capturing CUDA graph shapes:   8%|▊         | 10/131 [00:05<01:10,  1.72it/s]Capturing CUDA graph shapes:   8%|▊         | 11/131 [00:06<01:10,  1.70it/s]Capturing CUDA graph shapes:   9%|▉         | 12/131 [00:07<01:09,  1.72it/s]Capturing CUDA graph shapes:  10%|▉         | 13/131 [00:07<01:07,  1.74it/s]Capturing CUDA graph shapes:  11%|█         | 14/131 [00:08<01:07,  1.72it/s]Capturing CUDA graph shapes:  11%|█▏        | 15/131 [00:08<01:06,  1.74it/s]Capturing CUDA graph shapes:  12%|█▏        | 16/131 [00:09<01:06,  1.72it/s]Capturing CUDA graph shapes:  13%|█▎        | 17/131 [00:10<01:06,  1.72it/s]Capturing CUDA graph shapes:  14%|█▎        | 18/131 [00:10<01:05,  1.72it/s]Capturing CUDA graph shapes:  15%|█▍        | 19/131 [00:11<01:04,  1.74it/s]Capturing CUDA graph shapes:  15%|█▌        | 20/131 [00:11<01:05,  1.70it/s]Capturing CUDA graph shapes:  16%|█▌        | 21/131 [00:12<01:04,  1.70it/s]Capturing CUDA graph shapes:  17%|█▋        | 22/131 [00:12<01:03,  1.72it/s]Capturing CUDA graph shapes:  18%|█▊        | 23/131 [00:13<01:01,  1.74it/s]Capturing CUDA graph shapes:  18%|█▊        | 24/131 [00:14<01:02,  1.71it/s]Capturing CUDA graph shapes:  19%|█▉        | 25/131 [00:14<01:00,  1.74it/s]Capturing CUDA graph shapes:  20%|█▉        | 26/131 [00:15<01:00,  1.74it/s]Capturing CUDA graph shapes:  21%|██        | 27/131 [00:15<00:58,  1.79it/s]Capturing CUDA graph shapes:  21%|██▏       | 28/131 [00:16<00:58,  1.77it/s]Capturing CUDA graph shapes:  22%|██▏       | 29/131 [00:16<00:58,  1.74it/s]Capturing CUDA graph shapes:  23%|██▎       | 30/131 [00:17<00:57,  1.76it/s]Capturing CUDA graph shapes:  24%|██▎       | 31/131 [00:18<00:56,  1.76it/s]Capturing CUDA graph shapes:  24%|██▍       | 32/131 [00:18<00:55,  1.78it/s]Capturing CUDA graph shapes:  25%|██▌       | 33/131 [00:19<00:55,  1.78it/s]Capturing CUDA graph shapes:  26%|██▌       | 34/131 [00:19<00:53,  1.80it/s]Capturing CUDA graph shapes:  27%|██▋       | 35/131 [00:20<00:54,  1.77it/s]Capturing CUDA graph shapes:  27%|██▋       | 36/131 [00:20<00:52,  1.81it/s]Capturing CUDA graph shapes:  28%|██▊       | 37/131 [00:21<00:51,  1.82it/s]Capturing CUDA graph shapes:  29%|██▉       | 38/131 [00:21<00:51,  1.80it/s]Capturing CUDA graph shapes:  30%|██▉       | 39/131 [00:22<00:50,  1.81it/s]Capturing CUDA graph shapes:  31%|███       | 40/131 [00:22<00:49,  1.83it/s]Capturing CUDA graph shapes:  31%|███▏      | 41/131 [00:23<00:49,  1.83it/s]Capturing CUDA graph shapes:  32%|███▏      | 42/131 [00:24<00:48,  1.83it/s]Capturing CUDA graph shapes:  33%|███▎      | 43/131 [00:24<00:47,  1.85it/s]Capturing CUDA graph shapes:  34%|███▎      | 44/131 [00:25<00:48,  1.80it/s]Capturing CUDA graph shapes:  34%|███▍      | 45/131 [00:25<00:47,  1.83it/s]Capturing CUDA graph shapes:  35%|███▌      | 46/131 [00:26<00:46,  1.84it/s]Capturing CUDA graph shapes:  36%|███▌      | 47/131 [00:26<00:46,  1.82it/s]Capturing CUDA graph shapes:  37%|███▋      | 48/131 [00:27<00:45,  1.83it/s]Capturing CUDA graph shapes:  37%|███▋      | 49/131 [00:27<00:44,  1.84it/s]Capturing CUDA graph shapes:  38%|███▊      | 50/131 [00:28<00:43,  1.86it/s]Capturing CUDA graph shapes:  39%|███▉      | 51/131 [00:28<00:43,  1.83it/s]Capturing CUDA graph shapes:  40%|███▉      | 52/131 [00:29<00:43,  1.81it/s]Capturing CUDA graph shapes:  40%|████      | 53/131 [00:30<00:42,  1.84it/s]Capturing CUDA graph shapes:  41%|████      | 54/131 [00:30<00:42,  1.82it/s]Capturing CUDA graph shapes:  42%|████▏     | 55/131 [00:31<00:41,  1.85it/s]Capturing CUDA graph shapes:  43%|████▎     | 56/131 [00:31<00:40,  1.84it/s]Capturing CUDA graph shapes:  44%|████▎     | 57/131 [00:32<00:40,  1.85it/s]Capturing CUDA graph shapes:  44%|████▍     | 58/131 [00:32<00:38,  1.89it/s]Capturing CUDA graph shapes:  45%|████▌     | 59/131 [00:33<00:38,  1.85it/s]Capturing CUDA graph shapes:  46%|████▌     | 60/131 [00:33<00:37,  1.90it/s]Capturing CUDA graph shapes:  47%|████▋     | 61/131 [00:34<00:36,  1.90it/s]Capturing CUDA graph shapes:  47%|████▋     | 62/131 [00:34<00:36,  1.88it/s]Capturing CUDA graph shapes:  48%|████▊     | 63/131 [00:35<00:36,  1.89it/s]Capturing CUDA graph shapes:  49%|████▉     | 64/131 [00:36<00:37,  1.81it/s]Capturing CUDA graph shapes:  50%|████▉     | 65/131 [00:36<00:36,  1.81it/s]Capturing CUDA graph shapes:  50%|█████     | 66/131 [00:37<00:36,  1.79it/s]Capturing CUDA graph shapes:  51%|█████     | 67/131 [00:37<00:35,  1.78it/s]Capturing CUDA graph shapes:  52%|█████▏    | 68/131 [00:38<00:34,  1.82it/s]Capturing CUDA graph shapes:  53%|█████▎    | 69/131 [00:38<00:34,  1.79it/s]Capturing CUDA graph shapes:  53%|█████▎    | 70/131 [00:39<00:33,  1.83it/s]Capturing CUDA graph shapes:  54%|█████▍    | 71/131 [00:39<00:32,  1.84it/s]Capturing CUDA graph shapes:  55%|█████▍    | 72/131 [00:40<00:31,  1.88it/s]Capturing CUDA graph shapes:  56%|█████▌    | 73/131 [00:40<00:30,  1.91it/s]Capturing CUDA graph shapes:  56%|█████▋    | 74/131 [00:41<00:30,  1.86it/s]Capturing CUDA graph shapes:  57%|█████▋    | 75/131 [00:41<00:29,  1.90it/s]Capturing CUDA graph shapes:  58%|█████▊    | 76/131 [00:42<00:28,  1.90it/s]Capturing CUDA graph shapes:  59%|█████▉    | 77/131 [00:42<00:28,  1.93it/s]Capturing CUDA graph shapes:  60%|█████▉    | 78/131 [00:43<00:27,  1.93it/s]Capturing CUDA graph shapes:  60%|██████    | 79/131 [00:43<00:26,  1.96it/s]Capturing CUDA graph shapes:  61%|██████    | 80/131 [00:44<00:26,  1.91it/s]Capturing CUDA graph shapes:  62%|██████▏   | 81/131 [00:45<00:25,  1.94it/s]Capturing CUDA graph shapes:  63%|██████▎   | 82/131 [00:45<00:25,  1.95it/s]Capturing CUDA graph shapes:  63%|██████▎   | 83/131 [00:46<00:24,  1.96it/s]Capturing CUDA graph shapes:  64%|██████▍   | 84/131 [00:46<00:23,  2.00it/s]Capturing CUDA graph shapes:  65%|██████▍   | 85/131 [00:46<00:22,  2.02it/s]Capturing CUDA graph shapes:  66%|██████▌   | 86/131 [00:47<00:22,  1.99it/s]Capturing CUDA graph shapes:  66%|██████▋   | 87/131 [00:48<00:22,  1.98it/s]Capturing CUDA graph shapes:  67%|██████▋   | 88/131 [00:48<00:21,  1.99it/s]Capturing CUDA graph shapes:  68%|██████▊   | 89/131 [00:49<00:21,  1.98it/s]Capturing CUDA graph shapes:  69%|██████▊   | 90/131 [00:49<00:20,  2.00it/s]Capturing CUDA graph shapes:  69%|██████▉   | 91/131 [00:50<00:19,  2.00it/s]Capturing CUDA graph shapes:  70%|███████   | 92/131 [00:50<00:19,  2.00it/s]Capturing CUDA graph shapes:  71%|███████   | 93/131 [00:51<00:18,  2.01it/s]Capturing CUDA graph shapes:  72%|███████▏  | 94/131 [00:51<00:18,  2.02it/s]Capturing CUDA graph shapes:  73%|███████▎  | 95/131 [00:51<00:17,  2.03it/s]Capturing CUDA graph shapes:  73%|███████▎  | 96/131 [00:52<00:17,  1.98it/s]Capturing CUDA graph shapes:  74%|███████▍  | 97/131 [00:52<00:16,  2.05it/s]Capturing CUDA graph shapes:  75%|███████▍  | 98/131 [00:53<00:16,  2.05it/s]Capturing CUDA graph shapes:  76%|███████▌  | 99/131 [00:53<00:15,  2.02it/s]Capturing CUDA graph shapes:  76%|███████▋  | 100/131 [00:54<00:15,  2.02it/s]Capturing CUDA graph shapes:  77%|███████▋  | 101/131 [00:54<00:15,  2.00it/s]Capturing CUDA graph shapes:  78%|███████▊  | 102/131 [00:55<00:14,  2.05it/s]Capturing CUDA graph shapes:  79%|███████▊  | 103/131 [00:55<00:13,  2.04it/s]Capturing CUDA graph shapes:  79%|███████▉  | 104/131 [00:56<00:13,  2.03it/s]Capturing CUDA graph shapes:  80%|████████  | 105/131 [00:56<00:12,  2.07it/s]Capturing CUDA graph shapes:  81%|████████  | 106/131 [00:57<00:12,  2.05it/s]Capturing CUDA graph shapes:  82%|████████▏ | 107/131 [00:57<00:11,  2.10it/s]Capturing CUDA graph shapes:  82%|████████▏ | 108/131 [00:58<00:10,  2.13it/s]Capturing CUDA graph shapes:  83%|████████▎ | 109/131 [00:58<00:10,  2.11it/s]Capturing CUDA graph shapes:  84%|████████▍ | 110/131 [00:59<00:10,  2.07it/s]Capturing CUDA graph shapes:  85%|████████▍ | 111/131 [00:59<00:09,  2.06it/s]Capturing CUDA graph shapes:  85%|████████▌ | 112/131 [01:00<00:09,  2.07it/s]Capturing CUDA graph shapes:  86%|████████▋ | 113/131 [01:00<00:08,  2.10it/s]Capturing CUDA graph shapes:  87%|████████▋ | 114/131 [01:01<00:08,  2.10it/s]Capturing CUDA graph shapes:  88%|████████▊ | 115/131 [01:01<00:07,  2.11it/s]Capturing CUDA graph shapes:  89%|████████▊ | 116/131 [01:02<00:07,  2.13it/s]Capturing CUDA graph shapes:  89%|████████▉ | 117/131 [01:02<00:06,  2.11it/s]Capturing CUDA graph shapes:  90%|█████████ | 118/131 [01:03<00:06,  2.11it/s]Capturing CUDA graph shapes:  91%|█████████ | 119/131 [01:03<00:05,  2.12it/s]Capturing CUDA graph shapes:  92%|█████████▏| 120/131 [01:04<00:05,  2.11it/s]Capturing CUDA graph shapes:  92%|█████████▏| 121/131 [01:04<00:04,  2.08it/s]Capturing CUDA graph shapes:  93%|█████████▎| 122/131 [01:05<00:04,  2.07it/s]Capturing CUDA graph shapes:  94%|█████████▍| 123/131 [01:05<00:03,  2.12it/s]Capturing CUDA graph shapes:  95%|█████████▍| 124/131 [01:05<00:03,  2.08it/s]Capturing CUDA graph shapes:  95%|█████████▌| 125/131 [01:06<00:02,  2.13it/s]Capturing CUDA graph shapes:  96%|█████████▌| 126/131 [01:06<00:02,  2.13it/s]Capturing CUDA graph shapes:  97%|█████████▋| 127/131 [01:07<00:01,  2.10it/s]Capturing CUDA graph shapes:  98%|█████████▊| 128/131 [01:07<00:01,  2.14it/s]Capturing CUDA graph shapes:  98%|█████████▊| 129/131 [01:08<00:00,  2.13it/s]Capturing CUDA graph shapes:  99%|█████████▉| 130/131 [01:08<00:00,  2.13it/s]Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:09<00:00,  1.75it/s]Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:09<00:00,  1.88it/s]
INFO 04-23 09:10:30 model_runner.py:1535] Graph capturing finished in 70 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2662886)[0;0m INFO 04-23 09:10:30 model_runner.py:1535] Graph capturing finished in 70 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2662887)[0;0m INFO 04-23 09:10:30 model_runner.py:1535] Graph capturing finished in 70 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2662885)[0;0m INFO 04-23 09:10:30 model_runner.py:1535] Graph capturing finished in 70 secs, took 3.72 GiB
INFO 04-23 09:10:30 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 97.90 seconds
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 18125 examples [00:07, 2371.84 examples/s]Generating train split: 18125 examples [00:07, 2364.13 examples/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclAbilityTestResults.py:127: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: connectivity
Processed prompts:   0%|          | 0/2687 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2687 [02:12<98:47:14, 132.40s/it, est. speed input: 21.48 toks/s, output: 0.02 toks/s]Processed prompts:  28%|██▊       | 739/2687 [04:25<09:55,  3.27it/s, est. speed input: 8287.59 toks/s, output: 5.56 toks/s]Processed prompts:  55%|█████▍    | 1471/2687 [06:41<04:46,  4.24it/s, est. speed input: 10949.56 toks/s, output: 7.32 toks/s]Processed prompts:  72%|███████▏  | 1941/2687 [08:58<03:10,  3.92it/s, est. speed input: 12156.57 toks/s, output: 7.20 toks/s]Processed prompts:  87%|████████▋ | 2329/2687 [11:07<01:39,  3.60it/s, est. speed input: 13008.17 toks/s, output: 6.98 toks/s]Processed prompts: 100%|██████████| 2687/2687 [11:07<00:00,  4.02it/s, est. speed input: 15999.77 toks/s, output: 8.05 toks/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclAbilityTestResults.py:127: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: cycle
Processed prompts:   0%|          | 0/2717 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2717 [02:16<103:00:22, 136.53s/it, est. speed input: 19.72 toks/s, output: 0.01 toks/s]Processed prompts:  18%|█▊        | 478/2717 [04:31<18:02,  2.07it/s, est. speed input: 7929.93 toks/s, output: 3.52 toks/s] Processed prompts:  35%|███▌      | 955/2717 [06:47<10:44,  2.73it/s, est. speed input: 10581.02 toks/s, output: 4.69 toks/s]Processed prompts:  52%|█████▏    | 1422/2717 [09:04<07:12,  3.00it/s, est. speed input: 11847.21 toks/s, output: 5.22 toks/s]Processed prompts:  68%|██████▊   | 1838/2717 [11:22<04:52,  3.01it/s, est. speed input: 12605.27 toks/s, output: 5.39 toks/s]Processed prompts:  81%|████████  | 2202/2717 [13:40<02:58,  2.88it/s, est. speed input: 13084.65 toks/s, output: 5.37 toks/s]Processed prompts:  93%|█████████▎| 2522/2717 [15:13<01:04,  3.02it/s, est. speed input: 14071.68 toks/s, output: 5.52 toks/s]Processed prompts: 100%|██████████| 2717/2717 [15:13<00:00,  2.97it/s, est. speed input: 15622.34 toks/s, output: 5.95 toks/s]
Evaluating task: flow
Processed prompts:   0%|          | 0/405 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/405 [02:19<15:39:41, 139.56s/it, est. speed input: 61.95 toks/s, output: 0.01 toks/s]Processed prompts:  83%|████████▎ | 338/405 [02:56<00:26,  2.51it/s, est. speed input: 12087.14 toks/s, output: 3.83 toks/s]Processed prompts: 100%|██████████| 405/405 [02:56<00:00,  2.29it/s, est. speed input: 15206.22 toks/s, output: 4.59 toks/s]
Evaluating task: bipartite
Processed prompts:   0%|          | 0/2013 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2013 [02:16<76:08:02, 136.22s/it, est. speed input: 10.81 toks/s, output: 0.01 toks/s]Processed prompts:  20%|█▉        | 402/2013 [04:32<15:28,  1.74it/s, est. speed input: 7873.47 toks/s, output: 2.95 toks/s]Processed prompts:  41%|████      | 825/2013 [06:49<08:25,  2.35it/s, est. speed input: 10449.21 toks/s, output: 4.03 toks/s]Processed prompts:  62%|██████▏   | 1249/2013 [09:07<04:49,  2.64it/s, est. speed input: 11757.94 toks/s, output: 4.56 toks/s]Processed prompts:  80%|███████▉  | 1603/2013 [11:25<02:36,  2.61it/s, est. speed input: 12490.49 toks/s, output: 4.68 toks/s]Processed prompts:  95%|█████████▍| 1909/2013 [12:17<00:33,  3.15it/s, est. speed input: 14497.06 toks/s, output: 5.18 toks/s]Processed prompts: 100%|██████████| 2013/2013 [12:17<00:00,  2.73it/s, est. speed input: 15554.10 toks/s, output: 5.46 toks/s]
Evaluating task: hamilton
Processed prompts:   0%|          | 0/2097 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2097 [02:16<79:20:10, 136.26s/it, est. speed input: 9.63 toks/s, output: 0.01 toks/s]Processed prompts:  23%|██▎       | 490/2097 [04:31<12:36,  2.12it/s, est. speed input: 7964.05 toks/s, output: 3.61 toks/s]Processed prompts:  47%|████▋     | 982/2097 [06:49<06:39,  2.79it/s, est. speed input: 10566.72 toks/s, output: 4.80 toks/s]Processed prompts:  64%|██████▍   | 1338/2097 [09:09<04:42,  2.69it/s, est. speed input: 11761.04 toks/s, output: 4.87 toks/s]Processed prompts:  76%|███████▌  | 1593/2097 [11:30<03:32,  2.37it/s, est. speed input: 12426.29 toks/s, output: 4.62 toks/s]Processed prompts:  87%|████████▋ | 1830/2097 [13:51<02:05,  2.13it/s, est. speed input: 12855.58 toks/s, output: 4.40 toks/s]Processed prompts:  98%|█████████▊| 2058/2097 [14:17<00:14,  2.68it/s, est. speed input: 14921.56 toks/s, output: 4.80 toks/s]Processed prompts: 100%|██████████| 2097/2097 [14:17<00:00,  2.44it/s, est. speed input: 15370.17 toks/s, output: 4.89 toks/s]
Evaluating task: triangle
Processed prompts:   0%|          | 0/2756 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2756 [02:20<107:37:52, 140.64s/it, est. speed input: 63.04 toks/s, output: 0.01 toks/s]Processed prompts:  10%|█         | 277/2756 [04:40<35:31,  1.16it/s, est. speed input: 7611.61 toks/s, output: 1.98 toks/s] Processed prompts:  20%|█▉        | 543/2756 [07:00<24:36,  1.50it/s, est. speed input: 10116.07 toks/s, output: 2.58 toks/s]Processed prompts:  29%|██▉       | 802/2756 [09:21<19:57,  1.63it/s, est. speed input: 11346.73 toks/s, output: 2.86 toks/s]Processed prompts:  39%|███▊      | 1063/2756 [11:41<16:26,  1.72it/s, est. speed input: 12107.73 toks/s, output: 3.03 toks/s]Processed prompts:  48%|████▊     | 1327/2756 [14:01<13:25,  1.77it/s, est. speed input: 12609.69 toks/s, output: 3.15 toks/s]Processed prompts:  57%|█████▋    | 1584/2756 [16:20<10:51,  1.80it/s, est. speed input: 12988.63 toks/s, output: 3.23 toks/s]Processed prompts:  69%|██████▉   | 1895/2756 [18:40<07:26,  1.93it/s, est. speed input: 13254.58 toks/s, output: 3.38 toks/s]Processed prompts:  79%|███████▊  | 2165/2756 [21:00<05:05,  1.93it/s, est. speed input: 13473.52 toks/s, output: 3.44 toks/s]Processed prompts:  88%|████████▊ | 2439/2756 [23:19<02:43,  1.94it/s, est. speed input: 13649.16 toks/s, output: 3.49 toks/s]Processed prompts:  98%|█████████▊| 2701/2756 [23:48<00:21,  2.54it/s, est. speed input: 14852.20 toks/s, output: 3.78 toks/s]Processed prompts: 100%|██████████| 2756/2756 [23:48<00:00,  1.93it/s, est. speed input: 15156.60 toks/s, output: 3.86 toks/s]
Evaluating task: shortest
Processed prompts:   0%|          | 0/1392 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/1392 [02:20<54:15:06, 140.41s/it, est. speed input: 69.38 toks/s, output: 0.01 toks/s]Processed prompts:  18%|█▊        | 246/1392 [04:39<18:27,  1.03it/s, est. speed input: 7575.09 toks/s, output: 1.76 toks/s]Processed prompts:  36%|███▌      | 499/1392 [06:59<10:42,  1.39it/s, est. speed input: 10090.98 toks/s, output: 2.38 toks/s]Processed prompts:  54%|█████▍    | 755/1392 [09:19<06:47,  1.56it/s, est. speed input: 11337.56 toks/s, output: 2.70 toks/s]Processed prompts:  72%|███████▏  | 1007/1392 [11:39<03:53,  1.65it/s, est. speed input: 12092.89 toks/s, output: 2.88 toks/s]Processed prompts:  90%|████████▉ | 1252/1392 [13:03<01:11,  1.95it/s, est. speed input: 13488.05 toks/s, output: 3.20 toks/s]Processed prompts: 100%|██████████| 1392/1392 [13:03<00:00,  1.78it/s, est. speed input: 15091.07 toks/s, output: 3.55 toks/s]
Evaluating task: topology
Processed prompts:   0%|          | 0/872 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/872 [02:19<33:43:30, 139.39s/it, est. speed input: 66.43 toks/s, output: 0.01 toks/s]Processed prompts:  30%|██▉       | 260/872 [04:39<09:20,  1.09it/s, est. speed input: 7618.93 toks/s, output: 1.86 toks/s]Processed prompts:  58%|█████▊    | 510/872 [06:59<04:16,  1.41it/s, est. speed input: 10105.66 toks/s, output: 2.43 toks/s]Processed prompts:  86%|████████▌ | 750/872 [08:17<01:05,  1.86it/s, est. speed input: 12776.76 toks/s, output: 3.02 toks/s]Processed prompts: 100%|██████████| 872/872 [08:17<00:00,  1.75it/s, est. speed input: 15101.99 toks/s, output: 3.51 toks/s]
Evaluating task: substructure
Processed prompts:   0%|          | 0/3186 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/3186 [02:18<122:32:15, 138.50s/it, est. speed input: 21.80 toks/s, output: 0.01 toks/s]Processed prompts:  11%|█▏        | 360/3186 [04:37<30:50,  1.53it/s, est. speed input: 7752.28 toks/s, output: 2.60 toks/s] Processed prompts:  21%|██        | 664/3186 [06:54<22:51,  1.84it/s, est. speed input: 10318.94 toks/s, output: 3.20 toks/s]Processed prompts:  32%|███▏      | 1031/3186 [09:12<16:37,  2.16it/s, est. speed input: 11609.28 toks/s, output: 3.73 toks/s]Processed prompts:  43%|████▎     | 1369/3186 [11:32<13:25,  2.26it/s, est. speed input: 12360.67 toks/s, output: 3.95 toks/s]Processed prompts:  53%|█████▎    | 1691/3186 [13:50<10:55,  2.28it/s, est. speed input: 12870.23 toks/s, output: 4.07 toks/s]Processed prompts:  63%|██████▎   | 2019/3186 [16:08<08:24,  2.31it/s, est. speed input: 13237.99 toks/s, output: 4.17 toks/s]Processed prompts:  73%|███████▎  | 2328/3186 [18:28<06:16,  2.28it/s, est. speed input: 13485.47 toks/s, output: 4.20 toks/s]Processed prompts:  83%|████████▎ | 2655/3186 [20:47<03:50,  2.30it/s, est. speed input: 13687.06 toks/s, output: 4.26 toks/s]Processed prompts:  91%|█████████ | 2907/3186 [22:59<02:07,  2.18it/s, est. speed input: 13914.35 toks/s, output: 4.21 toks/s]Processed prompts: 100%|██████████| 3186/3186 [22:59<00:00,  2.31it/s, est. speed input: 15376.01 toks/s, output: 4.62 toks/s]
Accuracy data has been saved to 'task_accuracies.txt'.
INFO 04-23 11:17:31 multiproc_worker_utils.py:140] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=2662885)[0;0m INFO 04-23 11:17:31 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2662886)[0;0m INFO 04-23 11:17:31 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2662887)[0;0m INFO 04-23 11:17:31 multiproc_worker_utils.py:247] Worker exiting
[rank0]:[W423 11:17:52.450045185 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/home/zch/anaconda3/envs/GraphICL/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO 04-23 11:18:13 config.py:510] This model supports multiple tasks: {'generate', 'embed', 'score', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 04-23 11:18:13 config.py:1310] Defaulting to use mp for distributed inference
INFO 04-23 11:18:13 llm_engine.py:234] Initializing an LLM engine (v0.6.6) with config: model='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', speculative_config=None, tokenizer='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[1024,1016,1008,1000,992,984,976,968,960,952,944,936,928,920,912,904,896,888,880,872,864,856,848,840,832,824,816,808,800,792,784,776,768,760,752,744,736,728,720,712,704,696,688,680,672,664,656,648,640,632,624,616,608,600,592,584,576,568,560,552,544,536,528,520,512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":1024}, use_cached_outputs=False, 
WARNING 04-23 11:18:13 multiproc_worker_utils.py:312] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 04-23 11:18:13 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 04-23 11:18:14 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2716230)[0;0m INFO 04-23 11:18:14 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2716230)[0;0m INFO 04-23 11:18:14 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2716231)[0;0m INFO 04-23 11:18:14 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2716231)[0;0m INFO 04-23 11:18:14 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2716232)[0;0m INFO 04-23 11:18:15 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2716232)[0;0m INFO 04-23 11:18:15 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
INFO 04-23 11:18:15 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2716231)[0;0m INFO 04-23 11:18:15 utils.py:918] Found nccl from library libnccl.so.2
INFO 04-23 11:18:15 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2716231)[0;0m INFO 04-23 11:18:15 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2716230)[0;0m INFO 04-23 11:18:15 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2716232)[0;0m INFO 04-23 11:18:15 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2716230)[0;0m INFO 04-23 11:18:15 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2716232)[0;0m INFO 04-23 11:18:15 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2716230)[0;0m WARNING 04-23 11:18:16 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2716231)[0;0m WARNING 04-23 11:18:16 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 04-23 11:18:16 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2716232)[0;0m WARNING 04-23 11:18:16 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 04-23 11:18:16 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_470eaf81'), local_subscribe_port=58135, remote_subscribe_port=None)
INFO 04-23 11:18:16 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2716230)[0;0m INFO 04-23 11:18:16 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2716232)[0;0m INFO 04-23 11:18:16 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2716231)[0;0m INFO 04-23 11:18:16 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.36it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.48it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.41it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.53it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.49it/s]

[1;36m(VllmWorkerProcess pid=2716230)[0;0m INFO 04-23 11:18:18 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2716231)[0;0m INFO 04-23 11:18:18 model_runner.py:1099] Loading model weights took 3.5546 GB
INFO 04-23 11:18:18 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2716232)[0;0m INFO 04-23 11:18:18 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2716230)[0;0m INFO 04-23 11:18:22 worker.py:241] Memory profiling takes 4.12 seconds
[1;36m(VllmWorkerProcess pid=2716230)[0;0m INFO 04-23 11:18:22 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2716230)[0;0m INFO 04-23 11:18:22 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
[1;36m(VllmWorkerProcess pid=2716231)[0;0m INFO 04-23 11:18:22 worker.py:241] Memory profiling takes 4.12 seconds
[1;36m(VllmWorkerProcess pid=2716231)[0;0m INFO 04-23 11:18:22 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2716231)[0;0m INFO 04-23 11:18:22 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
[1;36m(VllmWorkerProcess pid=2716232)[0;0m INFO 04-23 11:18:22 worker.py:241] Memory profiling takes 4.25 seconds
[1;36m(VllmWorkerProcess pid=2716232)[0;0m INFO 04-23 11:18:22 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2716232)[0;0m INFO 04-23 11:18:22 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.57GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.94GiB.
INFO 04-23 11:18:22 worker.py:241] Memory profiling takes 4.26 seconds
INFO 04-23 11:18:22 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
INFO 04-23 11:18:22 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.68GiB; PyTorch activation peak memory takes 5.74GiB; the rest of the memory reserved for KV Cache is 28.05GiB.
INFO 04-23 11:18:23 distributed_gpu_executor.py:57] # GPU blocks: 131317, # CPU blocks: 74898
INFO 04-23 11:18:23 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 64.12x
[1;36m(VllmWorkerProcess pid=2716232)[0;0m INFO 04-23 11:18:45 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-23 11:18:45 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2716230)[0;0m INFO 04-23 11:18:45 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2716231)[0;0m INFO 04-23 11:18:45 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/131 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|          | 1/131 [00:00<01:19,  1.64it/s]Capturing CUDA graph shapes:   2%|▏         | 2/131 [00:01<01:30,  1.42it/s]Capturing CUDA graph shapes:   2%|▏         | 3/131 [00:02<01:32,  1.39it/s]Capturing CUDA graph shapes:   3%|▎         | 4/131 [00:02<01:31,  1.39it/s]Capturing CUDA graph shapes:   4%|▍         | 5/131 [00:03<01:31,  1.38it/s]Capturing CUDA graph shapes:   5%|▍         | 6/131 [00:04<01:30,  1.39it/s]Capturing CUDA graph shapes:   5%|▌         | 7/131 [00:04<01:28,  1.39it/s]Capturing CUDA graph shapes:   6%|▌         | 8/131 [00:05<01:27,  1.40it/s]Capturing CUDA graph shapes:   7%|▋         | 9/131 [00:06<01:26,  1.41it/s]Capturing CUDA graph shapes:   8%|▊         | 10/131 [00:07<01:26,  1.40it/s]Capturing CUDA graph shapes:   8%|▊         | 11/131 [00:07<01:24,  1.42it/s]Capturing CUDA graph shapes:   9%|▉         | 12/131 [00:08<01:24,  1.40it/s]Capturing CUDA graph shapes:  10%|▉         | 13/131 [00:09<01:24,  1.39it/s]Capturing CUDA graph shapes:  11%|█         | 14/131 [00:09<01:23,  1.40it/s]Capturing CUDA graph shapes:  11%|█▏        | 15/131 [00:10<01:23,  1.38it/s]Capturing CUDA graph shapes:  12%|█▏        | 16/131 [00:11<01:22,  1.40it/s]Capturing CUDA graph shapes:  13%|█▎        | 17/131 [00:12<01:21,  1.39it/s]Capturing CUDA graph shapes:  14%|█▎        | 18/131 [00:12<01:21,  1.38it/s]Capturing CUDA graph shapes:  15%|█▍        | 19/131 [00:13<01:21,  1.38it/s]Capturing CUDA graph shapes:  15%|█▌        | 20/131 [00:14<01:19,  1.39it/s]Capturing CUDA graph shapes:  16%|█▌        | 21/131 [00:15<01:19,  1.38it/s]Capturing CUDA graph shapes:  17%|█▋        | 22/131 [00:15<01:18,  1.38it/s]Capturing CUDA graph shapes:  18%|█▊        | 23/131 [00:16<01:18,  1.38it/s]Capturing CUDA graph shapes:  18%|█▊        | 24/131 [00:17<01:16,  1.39it/s]Capturing CUDA graph shapes:  19%|█▉        | 25/131 [00:17<01:15,  1.41it/s]Capturing CUDA graph shapes:  20%|█▉        | 26/131 [00:18<01:14,  1.41it/s]Capturing CUDA graph shapes:  21%|██        | 27/131 [00:19<01:13,  1.41it/s]Capturing CUDA graph shapes:  21%|██▏       | 28/131 [00:20<01:13,  1.40it/s]Capturing CUDA graph shapes:  22%|██▏       | 29/131 [00:20<01:13,  1.40it/s]Capturing CUDA graph shapes:  23%|██▎       | 30/131 [00:21<01:11,  1.41it/s]Capturing CUDA graph shapes:  24%|██▎       | 31/131 [00:22<01:10,  1.42it/s]Capturing CUDA graph shapes:  24%|██▍       | 32/131 [00:22<01:10,  1.41it/s]Capturing CUDA graph shapes:  25%|██▌       | 33/131 [00:23<01:08,  1.43it/s]Capturing CUDA graph shapes:  26%|██▌       | 34/131 [00:24<01:08,  1.41it/s]Capturing CUDA graph shapes:  27%|██▋       | 35/131 [00:24<01:07,  1.42it/s]Capturing CUDA graph shapes:  27%|██▋       | 36/131 [00:25<01:06,  1.43it/s]Capturing CUDA graph shapes:  28%|██▊       | 37/131 [00:26<01:05,  1.44it/s]Capturing CUDA graph shapes:  29%|██▉       | 38/131 [00:27<01:03,  1.46it/s]Capturing CUDA graph shapes:  30%|██▉       | 39/131 [00:27<01:02,  1.47it/s]Capturing CUDA graph shapes:  31%|███       | 40/131 [00:28<01:02,  1.45it/s]Capturing CUDA graph shapes:  31%|███▏      | 41/131 [00:29<01:02,  1.43it/s]Capturing CUDA graph shapes:  32%|███▏      | 42/131 [00:29<01:01,  1.45it/s]Capturing CUDA graph shapes:  33%|███▎      | 43/131 [00:30<01:00,  1.45it/s]Capturing CUDA graph shapes:  34%|███▎      | 44/131 [00:31<00:59,  1.45it/s]Capturing CUDA graph shapes:  34%|███▍      | 45/131 [00:31<00:59,  1.44it/s]Capturing CUDA graph shapes:  35%|███▌      | 46/131 [00:32<00:58,  1.45it/s]Capturing CUDA graph shapes:  36%|███▌      | 47/131 [00:33<00:58,  1.44it/s]Capturing CUDA graph shapes:  37%|███▋      | 48/131 [00:33<00:56,  1.47it/s]Capturing CUDA graph shapes:  37%|███▋      | 49/131 [00:34<00:56,  1.46it/s]Capturing CUDA graph shapes:  38%|███▊      | 50/131 [00:35<00:55,  1.46it/s]Capturing CUDA graph shapes:  39%|███▉      | 51/131 [00:35<00:54,  1.46it/s]Capturing CUDA graph shapes:  40%|███▉      | 52/131 [00:36<00:52,  1.49it/s]Capturing CUDA graph shapes:  40%|████      | 53/131 [00:37<00:52,  1.47it/s]Capturing CUDA graph shapes:  41%|████      | 54/131 [00:37<00:52,  1.47it/s]Capturing CUDA graph shapes:  42%|████▏     | 55/131 [00:38<00:50,  1.49it/s]Capturing CUDA graph shapes:  43%|████▎     | 56/131 [00:39<00:50,  1.49it/s]Capturing CUDA graph shapes:  44%|████▎     | 57/131 [00:40<00:50,  1.46it/s]Capturing CUDA graph shapes:  44%|████▍     | 58/131 [00:40<00:49,  1.48it/s]Capturing CUDA graph shapes:  45%|████▌     | 59/131 [00:41<00:49,  1.47it/s]Capturing CUDA graph shapes:  46%|████▌     | 60/131 [00:42<00:48,  1.46it/s]Capturing CUDA graph shapes:  47%|████▋     | 61/131 [00:42<00:48,  1.46it/s]Capturing CUDA graph shapes:  47%|████▋     | 62/131 [00:43<00:46,  1.47it/s]Capturing CUDA graph shapes:  48%|████▊     | 63/131 [00:44<00:45,  1.49it/s]Capturing CUDA graph shapes:  49%|████▉     | 64/131 [00:44<00:45,  1.48it/s]Capturing CUDA graph shapes:  50%|████▉     | 65/131 [00:45<00:44,  1.48it/s]Capturing CUDA graph shapes:  50%|█████     | 66/131 [00:46<00:43,  1.49it/s]Capturing CUDA graph shapes:  51%|█████     | 67/131 [00:46<00:42,  1.51it/s]Capturing CUDA graph shapes:  52%|█████▏    | 68/131 [00:47<00:42,  1.49it/s]Capturing CUDA graph shapes:  53%|█████▎    | 69/131 [00:48<00:41,  1.50it/s]Capturing CUDA graph shapes:  53%|█████▎    | 70/131 [00:48<00:40,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 71/131 [00:49<00:39,  1.52it/s]Capturing CUDA graph shapes:  55%|█████▍    | 72/131 [00:50<00:39,  1.50it/s]Capturing CUDA graph shapes:  56%|█████▌    | 73/131 [00:50<00:38,  1.52it/s]Capturing CUDA graph shapes:  56%|█████▋    | 74/131 [00:51<00:37,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 75/131 [00:52<00:36,  1.52it/s]Capturing CUDA graph shapes:  58%|█████▊    | 76/131 [00:52<00:36,  1.52it/s]Capturing CUDA graph shapes:  59%|█████▉    | 77/131 [00:53<00:35,  1.51it/s]Capturing CUDA graph shapes:  60%|█████▉    | 78/131 [00:53<00:34,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 79/131 [00:54<00:34,  1.50it/s]Capturing CUDA graph shapes:  61%|██████    | 80/131 [00:55<00:33,  1.51it/s]Capturing CUDA graph shapes:  62%|██████▏   | 81/131 [00:55<00:33,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 82/131 [00:56<00:32,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 83/131 [00:57<00:31,  1.53it/s]Capturing CUDA graph shapes:  64%|██████▍   | 84/131 [00:57<00:30,  1.56it/s]Capturing CUDA graph shapes:  65%|██████▍   | 85/131 [00:58<00:29,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 86/131 [00:59<00:28,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▋   | 87/131 [00:59<00:27,  1.59it/s]Capturing CUDA graph shapes:  67%|██████▋   | 88/131 [01:00<00:27,  1.58it/s]Capturing CUDA graph shapes:  68%|██████▊   | 89/131 [01:01<00:26,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 90/131 [01:01<00:26,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▉   | 91/131 [01:02<00:25,  1.59it/s]Capturing CUDA graph shapes:  70%|███████   | 92/131 [01:02<00:24,  1.61it/s]Capturing CUDA graph shapes:  71%|███████   | 93/131 [01:03<00:23,  1.59it/s]Capturing CUDA graph shapes:  72%|███████▏  | 94/131 [01:04<00:23,  1.56it/s]Capturing CUDA graph shapes:  73%|███████▎  | 95/131 [01:04<00:22,  1.57it/s]Capturing CUDA graph shapes:  73%|███████▎  | 96/131 [01:05<00:22,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 97/131 [01:06<00:21,  1.55it/s]Capturing CUDA graph shapes:  75%|███████▍  | 98/131 [01:06<00:21,  1.56it/s]Capturing CUDA graph shapes:  76%|███████▌  | 99/131 [01:07<00:20,  1.58it/s]Capturing CUDA graph shapes:  76%|███████▋  | 100/131 [01:08<00:19,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 101/131 [01:08<00:19,  1.57it/s]Capturing CUDA graph shapes:  78%|███████▊  | 102/131 [01:09<00:18,  1.56it/s]Capturing CUDA graph shapes:  79%|███████▊  | 103/131 [01:09<00:17,  1.57it/s]Capturing CUDA graph shapes:  79%|███████▉  | 104/131 [01:10<00:17,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 105/131 [01:11<00:16,  1.57it/s]Capturing CUDA graph shapes:  81%|████████  | 106/131 [01:11<00:15,  1.57it/s]Capturing CUDA graph shapes:  82%|████████▏ | 107/131 [01:12<00:15,  1.56it/s]Capturing CUDA graph shapes:  82%|████████▏ | 108/131 [01:13<00:14,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 109/131 [01:13<00:13,  1.60it/s]Capturing CUDA graph shapes:  84%|████████▍ | 110/131 [01:14<00:13,  1.58it/s]Capturing CUDA graph shapes:  85%|████████▍ | 111/131 [01:14<00:12,  1.61it/s]Capturing CUDA graph shapes:  85%|████████▌ | 112/131 [01:15<00:11,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▋ | 113/131 [01:16<00:11,  1.62it/s]Capturing CUDA graph shapes:  87%|████████▋ | 114/131 [01:16<00:10,  1.63it/s]Capturing CUDA graph shapes:  88%|████████▊ | 115/131 [01:17<00:09,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 116/131 [01:18<00:09,  1.64it/s]Capturing CUDA graph shapes:  89%|████████▉ | 117/131 [01:18<00:08,  1.67it/s]Capturing CUDA graph shapes:  90%|█████████ | 118/131 [01:19<00:07,  1.66it/s]Capturing CUDA graph shapes:  91%|█████████ | 119/131 [01:19<00:07,  1.66it/s]Capturing CUDA graph shapes:  92%|█████████▏| 120/131 [01:20<00:06,  1.62it/s]Capturing CUDA graph shapes:  92%|█████████▏| 121/131 [01:21<00:06,  1.63it/s]Capturing CUDA graph shapes:  93%|█████████▎| 122/131 [01:21<00:05,  1.62it/s]Capturing CUDA graph shapes:  94%|█████████▍| 123/131 [01:22<00:05,  1.59it/s]Capturing CUDA graph shapes:  95%|█████████▍| 124/131 [01:22<00:04,  1.60it/s]Capturing CUDA graph shapes:  95%|█████████▌| 125/131 [01:23<00:03,  1.58it/s]Capturing CUDA graph shapes:  96%|█████████▌| 126/131 [01:24<00:03,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 127/131 [01:24<00:02,  1.59it/s]Capturing CUDA graph shapes:  98%|█████████▊| 128/131 [01:25<00:01,  1.60it/s]Capturing CUDA graph shapes:  98%|█████████▊| 129/131 [01:26<00:01,  1.63it/s]Capturing CUDA graph shapes:  99%|█████████▉| 130/131 [01:26<00:00,  1.62it/s]Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:27<00:00,  1.42it/s]Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:27<00:00,  1.50it/s]
INFO 04-23 11:20:13 model_runner.py:1535] Graph capturing finished in 88 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2716230)[0;0m INFO 04-23 11:20:13 model_runner.py:1535] Graph capturing finished in 88 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2716231)[0;0m INFO 04-23 11:20:13 model_runner.py:1535] Graph capturing finished in 88 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2716232)[0;0m INFO 04-23 11:20:13 model_runner.py:1535] Graph capturing finished in 88 secs, took 3.72 GiB
INFO 04-23 11:20:13 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 114.76 seconds
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 18125 examples [00:01, 9331.88 examples/s]Generating train split: 18125 examples [00:01, 9283.53 examples/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclExamplesTestResults.py:128: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: cycle
Processed prompts:   0%|          | 0/2717 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2717 [02:11<99:09:34, 131.43s/it, est. speed input: 66.46 toks/s, output: 0.02 toks/s]Processed prompts:  37%|███▋      | 1007/2717 [04:24<06:22,  4.47it/s, est. speed input: 8467.22 toks/s, output: 7.61 toks/s]Processed prompts:  66%|██████▌   | 1784/2717 [06:40<03:04,  5.05it/s, est. speed input: 11088.53 toks/s, output: 8.91 toks/s]Processed prompts:  86%|████████▌ | 2331/2717 [08:32<01:17,  4.99it/s, est. speed input: 12886.94 toks/s, output: 9.10 toks/s]Processed prompts: 100%|██████████| 2717/2717 [08:32<00:00,  5.30it/s, est. speed input: 16339.23 toks/s, output: 10.61 toks/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclExamplesTestResults.py:128: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: connectivity
Processed prompts:   0%|          | 0/2687 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2687 [01:23<62:34:16, 83.86s/it, est. speed input: 42.75 toks/s, output: 0.02 toks/s]Processed prompts:  38%|███▊      | 1025/2687 [03:28<04:57,  5.59it/s, est. speed input: 7133.14 toks/s, output: 9.83 toks/s]Processed prompts:  76%|███████▋  | 2049/2687 [05:34<01:33,  6.85it/s, est. speed input: 10730.02 toks/s, output: 12.26 toks/s]Processed prompts: 100%|██████████| 2687/2687 [05:34<00:00,  8.04it/s, est. speed input: 16819.51 toks/s, output: 16.07 toks/s]
Evaluating task: bipartite
Processed prompts:   0%|          | 0/2013 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2013 [02:13<74:33:24, 133.40s/it, est. speed input: 7.17 toks/s, output: 0.01 toks/s]Processed prompts:  38%|███▊      | 769/2013 [04:28<06:10,  3.36it/s, est. speed input: 8182.07 toks/s, output: 5.72 toks/s]Processed prompts:  70%|██████▉   | 1403/2013 [06:44<02:34,  3.96it/s, est. speed input: 10811.77 toks/s, output: 6.93 toks/s]Processed prompts:  93%|█████████▎| 1869/2013 [07:31<00:28,  5.07it/s, est. speed input: 14446.52 toks/s, output: 8.28 toks/s]Processed prompts: 100%|██████████| 2013/2013 [07:31<00:00,  4.46it/s, est. speed input: 16052.16 toks/s, output: 8.92 toks/s]
Evaluating task: topology
Processed prompts:   0%|          | 0/872 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/872 [02:16<33:03:58, 136.67s/it, est. speed input: 67.75 toks/s, output: 0.01 toks/s]Processed prompts:  54%|█████▎    | 467/872 [04:37<03:25,  1.97it/s, est. speed input: 7762.97 toks/s, output: 3.36 toks/s]Processed prompts:  81%|████████  | 706/872 [06:14<01:17,  2.15it/s, est. speed input: 11393.32 toks/s, output: 3.77 toks/s]Processed prompts: 100%|██████████| 872/872 [06:14<00:00,  2.33it/s, est. speed input: 15256.60 toks/s, output: 4.65 toks/s]
Evaluating task: shortest
Processed prompts:   0%|          | 0/1392 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/1392 [02:17<53:11:09, 137.65s/it, est. speed input: 29.07 toks/s, output: 0.01 toks/s]Processed prompts:  28%|██▊       | 392/1392 [04:35<09:58,  1.67it/s, est. speed input: 7770.47 toks/s, output: 2.84 toks/s]Processed prompts:  55%|█████▌    | 770/1392 [06:55<04:49,  2.15it/s, est. speed input: 10291.19 toks/s, output: 3.71 toks/s]Processed prompts:  80%|███████▉  | 1113/1392 [09:14<02:02,  2.27it/s, est. speed input: 11551.08 toks/s, output: 4.01 toks/s]Processed prompts:  99%|█████████▉| 1385/1392 [09:19<00:02,  3.32it/s, est. speed input: 15244.90 toks/s, output: 4.96 toks/s]Processed prompts: 100%|██████████| 1392/1392 [09:19<00:00,  2.49it/s, est. speed input: 15342.31 toks/s, output: 4.98 toks/s]
Evaluating task: triangle
Processed prompts:   0%|          | 0/2756 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2756 [01:48<83:13:26, 108.75s/it, est. speed input: 27.60 toks/s, output: 0.02 toks/s]Processed prompts:  37%|███▋      | 1025/2756 [04:03<05:56,  4.86it/s, est. speed input: 7729.71 toks/s, output: 8.42 toks/s]Processed prompts:  71%|███████   | 1952/2756 [06:18<02:18,  5.82it/s, est. speed input: 10888.09 toks/s, output: 10.32 toks/s]Processed prompts:  94%|█████████▍| 2585/2756 [06:59<00:22,  7.46it/s, est. speed input: 15012.17 toks/s, output: 12.31 toks/s]Processed prompts: 100%|██████████| 2756/2756 [06:59<00:00,  6.56it/s, est. speed input: 16594.50 toks/s, output: 13.13 toks/s]
Evaluating task: flow
Processed prompts:   0%|          | 0/405 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/405 [01:48<12:08:03, 108.13s/it, est. speed input: 19.34 toks/s, output: 0.02 toks/s]Processed prompts: 100%|██████████| 405/405 [01:48<00:00,  3.75it/s, est. speed input: 15638.27 toks/s, output: 7.49 toks/s]
Evaluating task: hamilton
Processed prompts:   0%|          | 0/2097 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2097 [02:14<78:06:03, 134.14s/it, est. speed input: 6.01 toks/s, output: 0.01 toks/s]Processed prompts:  45%|████▌     | 953/2097 [04:30<04:36,  4.13it/s, est. speed input: 8275.79 toks/s, output: 7.04 toks/s]Processed prompts:  70%|██████▉   | 1460/2097 [06:49<02:42,  3.92it/s, est. speed input: 10765.12 toks/s, output: 7.14 toks/s]Processed prompts:  86%|████████▌ | 1802/2097 [08:52<01:24,  3.49it/s, est. speed input: 12264.95 toks/s, output: 6.76 toks/s]Processed prompts: 100%|██████████| 2097/2097 [08:52<00:00,  3.93it/s, est. speed input: 15793.96 toks/s, output: 7.87 toks/s]
Evaluating task: substructure
Processed prompts:   0%|          | 0/3186 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/3186 [02:13<118:32:42, 133.99s/it, est. speed input: 19.08 toks/s, output: 0.01 toks/s]Processed prompts:  23%|██▎       | 744/3186 [04:29<12:33,  3.24it/s, est. speed input: 8220.29 toks/s, output: 5.52 toks/s] Processed prompts:  40%|████      | 1282/3186 [06:46<08:55,  3.56it/s, est. speed input: 10783.66 toks/s, output: 6.31 toks/s]Processed prompts:  57%|█████▋    | 1805/3186 [09:03<06:16,  3.66it/s, est. speed input: 12060.82 toks/s, output: 6.65 toks/s]Processed prompts:  71%|███████▏  | 2273/3186 [11:21<04:16,  3.56it/s, est. speed input: 12789.97 toks/s, output: 6.67 toks/s]Processed prompts:  83%|████████▎ | 2637/3186 [13:41<02:49,  3.23it/s, est. speed input: 13217.32 toks/s, output: 6.42 toks/s]Processed prompts:  93%|█████████▎| 2957/3186 [15:23<01:11,  3.20it/s, est. speed input: 14054.91 toks/s, output: 6.40 toks/s]Processed prompts: 100%|██████████| 3186/3186 [15:23<00:00,  3.45it/s, est. speed input: 15752.28 toks/s, output: 6.90 toks/s]
Accuracy data has been saved to 'task_accuracies.txt'.
INFO 04-23 12:32:33 multiproc_worker_utils.py:140] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=2716230)[0;0m INFO 04-23 12:32:33 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2716231)[0;0m INFO 04-23 12:32:33 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2716232)[0;0m INFO 04-23 12:32:33 multiproc_worker_utils.py:247] Worker exiting
[rank0]:[W423 12:32:52.413806031 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/home/zch/anaconda3/envs/GraphICL/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO 04-23 12:33:12 config.py:510] This model supports multiple tasks: {'generate', 'classify', 'score', 'embed', 'reward'}. Defaulting to 'generate'.
INFO 04-23 12:33:12 config.py:1310] Defaulting to use mp for distributed inference
INFO 04-23 12:33:12 llm_engine.py:234] Initializing an LLM engine (v0.6.6) with config: model='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', speculative_config=None, tokenizer='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[1024,1016,1008,1000,992,984,976,968,960,952,944,936,928,920,912,904,896,888,880,872,864,856,848,840,832,824,816,808,800,792,784,776,768,760,752,744,736,728,720,712,704,696,688,680,672,664,656,648,640,632,624,616,608,600,592,584,576,568,560,552,544,536,528,520,512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":1024}, use_cached_outputs=False, 
WARNING 04-23 12:33:12 multiproc_worker_utils.py:312] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 04-23 12:33:12 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=2718745)[0;0m INFO 04-23 12:33:13 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2718745)[0;0m INFO 04-23 12:33:13 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2718744)[0;0m INFO 04-23 12:33:13 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2718744)[0;0m INFO 04-23 12:33:13 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2718746)[0;0m INFO 04-23 12:33:13 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2718746)[0;0m INFO 04-23 12:33:13 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
INFO 04-23 12:33:14 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2718744)[0;0m INFO 04-23 12:33:15 utils.py:918] Found nccl from library libnccl.so.2
INFO 04-23 12:33:15 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2718745)[0;0m INFO 04-23 12:33:15 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2718746)[0;0m INFO 04-23 12:33:15 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2718744)[0;0m INFO 04-23 12:33:15 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2718745)[0;0m INFO 04-23 12:33:15 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 04-23 12:33:15 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2718746)[0;0m INFO 04-23 12:33:15 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2718744)[0;0m WARNING 04-23 12:33:15 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 04-23 12:33:15 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2718745)[0;0m WARNING 04-23 12:33:15 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2718746)[0;0m WARNING 04-23 12:33:15 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 04-23 12:33:15 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_fabb1355'), local_subscribe_port=33237, remote_subscribe_port=None)
INFO 04-23 12:33:15 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2718744)[0;0m INFO 04-23 12:33:15 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2718745)[0;0m INFO 04-23 12:33:15 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2718746)[0;0m INFO 04-23 12:33:15 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.49it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.61it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.54it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.58it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.57it/s]

[1;36m(VllmWorkerProcess pid=2718744)[0;0m INFO 04-23 12:33:17 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2718745)[0;0m INFO 04-23 12:33:17 model_runner.py:1099] Loading model weights took 3.5546 GB
INFO 04-23 12:33:17 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2718746)[0;0m INFO 04-23 12:33:18 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2718744)[0;0m INFO 04-23 12:33:22 worker.py:241] Memory profiling takes 4.12 seconds
[1;36m(VllmWorkerProcess pid=2718744)[0;0m INFO 04-23 12:33:22 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2718744)[0;0m INFO 04-23 12:33:22 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
[1;36m(VllmWorkerProcess pid=2718745)[0;0m INFO 04-23 12:33:22 worker.py:241] Memory profiling takes 4.13 seconds
[1;36m(VllmWorkerProcess pid=2718745)[0;0m INFO 04-23 12:33:22 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2718745)[0;0m INFO 04-23 12:33:22 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
[1;36m(VllmWorkerProcess pid=2718746)[0;0m INFO 04-23 12:33:22 worker.py:241] Memory profiling takes 4.22 seconds
[1;36m(VllmWorkerProcess pid=2718746)[0;0m INFO 04-23 12:33:22 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2718746)[0;0m INFO 04-23 12:33:22 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.57GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.94GiB.
INFO 04-23 12:33:22 worker.py:241] Memory profiling takes 4.41 seconds
INFO 04-23 12:33:22 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
INFO 04-23 12:33:22 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.68GiB; PyTorch activation peak memory takes 5.74GiB; the rest of the memory reserved for KV Cache is 28.05GiB.
INFO 04-23 12:33:22 distributed_gpu_executor.py:57] # GPU blocks: 131317, # CPU blocks: 74898
INFO 04-23 12:33:22 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 64.12x
[1;36m(VllmWorkerProcess pid=2718746)[0;0m INFO 04-23 12:33:45 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2718744)[0;0m INFO 04-23 12:33:45 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-23 12:33:45 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2718745)[0;0m INFO 04-23 12:33:45 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/131 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|          | 1/131 [00:00<01:45,  1.23it/s]Capturing CUDA graph shapes:   2%|▏         | 2/131 [00:01<01:35,  1.35it/s]Capturing CUDA graph shapes:   2%|▏         | 3/131 [00:02<01:30,  1.41it/s]Capturing CUDA graph shapes:   3%|▎         | 4/131 [00:02<01:31,  1.39it/s]Capturing CUDA graph shapes:   4%|▍         | 5/131 [00:03<01:29,  1.41it/s]Capturing CUDA graph shapes:   5%|▍         | 6/131 [00:04<01:28,  1.42it/s]Capturing CUDA graph shapes:   5%|▌         | 7/131 [00:05<01:28,  1.40it/s]Capturing CUDA graph shapes:   6%|▌         | 8/131 [00:05<01:28,  1.40it/s]Capturing CUDA graph shapes:   7%|▋         | 9/131 [00:06<01:26,  1.41it/s]Capturing CUDA graph shapes:   8%|▊         | 10/131 [00:07<01:26,  1.40it/s]Capturing CUDA graph shapes:   8%|▊         | 11/131 [00:07<01:25,  1.40it/s]Capturing CUDA graph shapes:   9%|▉         | 12/131 [00:08<01:25,  1.39it/s]Capturing CUDA graph shapes:  10%|▉         | 13/131 [00:09<01:24,  1.40it/s]Capturing CUDA graph shapes:  11%|█         | 14/131 [00:10<01:23,  1.40it/s]Capturing CUDA graph shapes:  11%|█▏        | 15/131 [00:10<01:23,  1.39it/s]Capturing CUDA graph shapes:  12%|█▏        | 16/131 [00:11<01:22,  1.40it/s]Capturing CUDA graph shapes:  13%|█▎        | 17/131 [00:12<01:20,  1.41it/s]Capturing CUDA graph shapes:  14%|█▎        | 18/131 [00:12<01:19,  1.41it/s]Capturing CUDA graph shapes:  15%|█▍        | 19/131 [00:13<01:20,  1.40it/s]Capturing CUDA graph shapes:  15%|█▌        | 20/131 [00:14<01:18,  1.41it/s]Capturing CUDA graph shapes:  16%|█▌        | 21/131 [00:15<01:17,  1.41it/s]Capturing CUDA graph shapes:  17%|█▋        | 22/131 [00:15<01:17,  1.40it/s]Capturing CUDA graph shapes:  18%|█▊        | 23/131 [00:16<01:16,  1.41it/s]Capturing CUDA graph shapes:  18%|█▊        | 24/131 [00:17<01:15,  1.42it/s]Capturing CUDA graph shapes:  19%|█▉        | 25/131 [00:17<01:13,  1.45it/s]Capturing CUDA graph shapes:  20%|█▉        | 26/131 [00:18<01:12,  1.45it/s]Capturing CUDA graph shapes:  21%|██        | 27/131 [00:19<01:12,  1.42it/s]Capturing CUDA graph shapes:  21%|██▏       | 28/131 [00:19<01:12,  1.41it/s]Capturing CUDA graph shapes:  22%|██▏       | 29/131 [00:20<01:12,  1.41it/s]Capturing CUDA graph shapes:  23%|██▎       | 30/131 [00:21<01:12,  1.40it/s]Capturing CUDA graph shapes:  24%|██▎       | 31/131 [00:22<01:11,  1.40it/s]Capturing CUDA graph shapes:  24%|██▍       | 32/131 [00:22<01:08,  1.44it/s]Capturing CUDA graph shapes:  25%|██▌       | 33/131 [00:23<01:08,  1.43it/s]Capturing CUDA graph shapes:  26%|██▌       | 34/131 [00:24<01:07,  1.45it/s]Capturing CUDA graph shapes:  27%|██▋       | 35/131 [00:24<01:05,  1.46it/s]Capturing CUDA graph shapes:  27%|██▋       | 36/131 [00:25<01:04,  1.47it/s]Capturing CUDA graph shapes:  28%|██▊       | 37/131 [00:26<01:03,  1.48it/s]Capturing CUDA graph shapes:  29%|██▉       | 38/131 [00:26<01:02,  1.48it/s]Capturing CUDA graph shapes:  30%|██▉       | 39/131 [00:27<01:01,  1.49it/s]Capturing CUDA graph shapes:  31%|███       | 40/131 [00:28<01:00,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 41/131 [00:28<01:00,  1.50it/s]Capturing CUDA graph shapes:  32%|███▏      | 42/131 [00:29<01:00,  1.47it/s]Capturing CUDA graph shapes:  33%|███▎      | 43/131 [00:30<00:59,  1.47it/s]Capturing CUDA graph shapes:  34%|███▎      | 44/131 [00:30<00:59,  1.46it/s]Capturing CUDA graph shapes:  34%|███▍      | 45/131 [00:31<00:58,  1.47it/s]Capturing CUDA graph shapes:  35%|███▌      | 46/131 [00:32<00:57,  1.47it/s]Capturing CUDA graph shapes:  36%|███▌      | 47/131 [00:32<00:56,  1.49it/s]Capturing CUDA graph shapes:  37%|███▋      | 48/131 [00:33<00:56,  1.47it/s]Capturing CUDA graph shapes:  37%|███▋      | 49/131 [00:34<00:55,  1.48it/s]Capturing CUDA graph shapes:  38%|███▊      | 50/131 [00:34<00:55,  1.47it/s]Capturing CUDA graph shapes:  39%|███▉      | 51/131 [00:35<00:54,  1.47it/s]Capturing CUDA graph shapes:  40%|███▉      | 52/131 [00:36<00:53,  1.48it/s]Capturing CUDA graph shapes:  40%|████      | 53/131 [00:36<00:51,  1.52it/s]Capturing CUDA graph shapes:  41%|████      | 54/131 [00:37<00:51,  1.50it/s]Capturing CUDA graph shapes:  42%|████▏     | 55/131 [00:38<00:50,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 56/131 [00:38<00:50,  1.49it/s]Capturing CUDA graph shapes:  44%|████▎     | 57/131 [00:39<00:49,  1.49it/s]Capturing CUDA graph shapes:  44%|████▍     | 58/131 [00:40<00:49,  1.48it/s]Capturing CUDA graph shapes:  45%|████▌     | 59/131 [00:40<00:47,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 60/131 [00:41<00:47,  1.51it/s]Capturing CUDA graph shapes:  47%|████▋     | 61/131 [00:42<00:46,  1.51it/s]Capturing CUDA graph shapes:  47%|████▋     | 62/131 [00:42<00:45,  1.50it/s]Capturing CUDA graph shapes:  48%|████▊     | 63/131 [00:43<00:45,  1.51it/s]Capturing CUDA graph shapes:  49%|████▉     | 64/131 [00:44<00:44,  1.51it/s]Capturing CUDA graph shapes:  50%|████▉     | 65/131 [00:44<00:43,  1.50it/s]Capturing CUDA graph shapes:  50%|█████     | 66/131 [00:45<00:42,  1.53it/s]Capturing CUDA graph shapes:  51%|█████     | 67/131 [00:46<00:42,  1.52it/s]Capturing CUDA graph shapes:  52%|█████▏    | 68/131 [00:46<00:41,  1.53it/s]Capturing CUDA graph shapes:  53%|█████▎    | 69/131 [00:47<00:40,  1.51it/s]Capturing CUDA graph shapes:  53%|█████▎    | 70/131 [00:48<00:39,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 71/131 [00:48<00:38,  1.55it/s]Capturing CUDA graph shapes:  55%|█████▍    | 72/131 [00:49<00:38,  1.55it/s]Capturing CUDA graph shapes:  56%|█████▌    | 73/131 [00:50<00:37,  1.53it/s]Capturing CUDA graph shapes:  56%|█████▋    | 74/131 [00:50<00:37,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 75/131 [00:51<00:37,  1.51it/s]Capturing CUDA graph shapes:  58%|█████▊    | 76/131 [00:52<00:36,  1.51it/s]Capturing CUDA graph shapes:  59%|█████▉    | 77/131 [00:52<00:35,  1.50it/s]Capturing CUDA graph shapes:  60%|█████▉    | 78/131 [00:53<00:35,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 79/131 [00:54<00:34,  1.50it/s]Capturing CUDA graph shapes:  61%|██████    | 80/131 [00:54<00:33,  1.51it/s]Capturing CUDA graph shapes:  62%|██████▏   | 81/131 [00:55<00:33,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 82/131 [00:56<00:32,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 83/131 [00:56<00:31,  1.52it/s]Capturing CUDA graph shapes:  64%|██████▍   | 84/131 [00:57<00:30,  1.53it/s]Capturing CUDA graph shapes:  65%|██████▍   | 85/131 [00:58<00:29,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 86/131 [00:58<00:29,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▋   | 87/131 [00:59<00:28,  1.56it/s]Capturing CUDA graph shapes:  67%|██████▋   | 88/131 [00:59<00:27,  1.54it/s]Capturing CUDA graph shapes:  68%|██████▊   | 89/131 [01:00<00:26,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 90/131 [01:01<00:26,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▉   | 91/131 [01:01<00:25,  1.55it/s]Capturing CUDA graph shapes:  70%|███████   | 92/131 [01:02<00:25,  1.54it/s]Capturing CUDA graph shapes:  71%|███████   | 93/131 [01:03<00:24,  1.54it/s]Capturing CUDA graph shapes:  72%|███████▏  | 94/131 [01:03<00:23,  1.55it/s]Capturing CUDA graph shapes:  73%|███████▎  | 95/131 [01:04<00:23,  1.53it/s]Capturing CUDA graph shapes:  73%|███████▎  | 96/131 [01:05<00:22,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 97/131 [01:05<00:22,  1.54it/s]Capturing CUDA graph shapes:  75%|███████▍  | 98/131 [01:06<00:21,  1.54it/s]Capturing CUDA graph shapes:  76%|███████▌  | 99/131 [01:07<00:20,  1.54it/s]Capturing CUDA graph shapes:  76%|███████▋  | 100/131 [01:07<00:19,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 101/131 [01:08<00:18,  1.59it/s]Capturing CUDA graph shapes:  78%|███████▊  | 102/131 [01:08<00:18,  1.58it/s]Capturing CUDA graph shapes:  79%|███████▊  | 103/131 [01:09<00:17,  1.56it/s]Capturing CUDA graph shapes:  79%|███████▉  | 104/131 [01:10<00:17,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 105/131 [01:10<00:15,  1.63it/s]Capturing CUDA graph shapes:  81%|████████  | 106/131 [01:11<00:15,  1.61it/s]Capturing CUDA graph shapes:  82%|████████▏ | 107/131 [01:12<00:15,  1.59it/s]Capturing CUDA graph shapes:  82%|████████▏ | 108/131 [01:12<00:14,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 109/131 [01:13<00:13,  1.62it/s]Capturing CUDA graph shapes:  84%|████████▍ | 110/131 [01:13<00:12,  1.63it/s]Capturing CUDA graph shapes:  85%|████████▍ | 111/131 [01:14<00:12,  1.65it/s]Capturing CUDA graph shapes:  85%|████████▌ | 112/131 [01:15<00:11,  1.63it/s]Capturing CUDA graph shapes:  86%|████████▋ | 113/131 [01:15<00:11,  1.63it/s]Capturing CUDA graph shapes:  87%|████████▋ | 114/131 [01:16<00:10,  1.62it/s]Capturing CUDA graph shapes:  88%|████████▊ | 115/131 [01:16<00:09,  1.62it/s]Capturing CUDA graph shapes:  89%|████████▊ | 116/131 [01:17<00:09,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▉ | 117/131 [01:18<00:08,  1.62it/s]Capturing CUDA graph shapes:  90%|█████████ | 118/131 [01:18<00:07,  1.64it/s]Capturing CUDA graph shapes:  91%|█████████ | 119/131 [01:19<00:07,  1.61it/s]Capturing CUDA graph shapes:  92%|█████████▏| 120/131 [01:20<00:06,  1.64it/s]Capturing CUDA graph shapes:  92%|█████████▏| 121/131 [01:20<00:06,  1.60it/s]Capturing CUDA graph shapes:  93%|█████████▎| 122/131 [01:21<00:05,  1.64it/s]Capturing CUDA graph shapes:  94%|█████████▍| 123/131 [01:21<00:04,  1.61it/s]Capturing CUDA graph shapes:  95%|█████████▍| 124/131 [01:22<00:04,  1.64it/s]Capturing CUDA graph shapes:  95%|█████████▌| 125/131 [01:23<00:03,  1.65it/s]Capturing CUDA graph shapes:  96%|█████████▌| 126/131 [01:23<00:03,  1.65it/s]Capturing CUDA graph shapes:  97%|█████████▋| 127/131 [01:24<00:02,  1.65it/s]Capturing CUDA graph shapes:  98%|█████████▊| 128/131 [01:24<00:01,  1.62it/s]Capturing CUDA graph shapes:  98%|█████████▊| 129/131 [01:25<00:01,  1.62it/s]Capturing CUDA graph shapes:  99%|█████████▉| 130/131 [01:26<00:00,  1.63it/s][1;36m(VllmWorkerProcess pid=2718745)[0;0m INFO 04-23 12:35:12 model_runner.py:1535] Graph capturing finished in 87 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2718744)[0;0m INFO 04-23 12:35:12 model_runner.py:1535] Graph capturing finished in 87 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2718746)[0;0m INFO 04-23 12:35:12 model_runner.py:1535] Graph capturing finished in 87 secs, took 3.72 GiB
Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:27<00:00,  1.41it/s]Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:27<00:00,  1.50it/s]
INFO 04-23 12:35:12 model_runner.py:1535] Graph capturing finished in 87 secs, took 3.72 GiB
INFO 04-23 12:35:12 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 114.31 seconds
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 18125 examples [00:03, 4949.43 examples/s]Generating train split: 18125 examples [00:03, 4931.13 examples/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclExamplesTestResults.py:128: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: cycle
Processed prompts:   0%|          | 0/2717 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2717 [02:12<99:55:45, 132.45s/it, est. speed input: 17.36 toks/s, output: 0.02 toks/s]Processed prompts:  24%|██▍       | 663/2717 [04:25<11:41,  2.93it/s, est. speed input: 8226.90 toks/s, output: 4.99 toks/s]Processed prompts:  48%|████▊     | 1304/2717 [06:42<06:16,  3.75it/s, est. speed input: 10860.02 toks/s, output: 6.49 toks/s]Processed prompts:  66%|██████▋   | 1801/2717 [08:58<04:06,  3.71it/s, est. speed input: 12111.05 toks/s, output: 6.69 toks/s]Processed prompts:  82%|████████▏ | 2223/2717 [11:14<02:21,  3.49it/s, est. speed input: 12832.07 toks/s, output: 6.59 toks/s]Processed prompts:  96%|█████████▌| 2607/2717 [11:57<00:25,  4.26it/s, est. speed input: 15045.86 toks/s, output: 7.27 toks/s]Processed prompts: 100%|██████████| 2717/2717 [11:57<00:00,  3.79it/s, est. speed input: 15953.81 toks/s, output: 7.58 toks/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclExamplesTestResults.py:128: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: connectivity
Processed prompts:   0%|          | 0/2687 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2687 [02:09<96:56:38, 129.93s/it, est. speed input: 22.15 toks/s, output: 0.02 toks/s]Processed prompts:  38%|███▊      | 1025/2687 [04:23<06:04,  4.56it/s, est. speed input: 8292.41 toks/s, output: 7.78 toks/s]Processed prompts:  69%|██████▉   | 1853/2687 [06:39<02:37,  5.28it/s, est. speed input: 11035.48 toks/s, output: 9.29 toks/s]Processed prompts:  87%|████████▋ | 2350/2687 [08:12<01:03,  5.29it/s, est. speed input: 13303.90 toks/s, output: 9.54 toks/s]Processed prompts: 100%|██████████| 2687/2687 [08:12<00:00,  5.46it/s, est. speed input: 16261.66 toks/s, output: 10.91 toks/s]
Evaluating task: bipartite
Processed prompts:   0%|          | 0/2013 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2013 [02:14<75:24:22, 134.92s/it, est. speed input: 9.09 toks/s, output: 0.01 toks/s]Processed prompts:  26%|██▌       | 522/2013 [04:29<10:53,  2.28it/s, est. speed input: 8023.62 toks/s, output: 3.88 toks/s]Processed prompts:  54%|█████▍    | 1092/2013 [06:46<04:53,  3.14it/s, est. speed input: 10641.02 toks/s, output: 5.37 toks/s]Processed prompts:  73%|███████▎  | 1476/2013 [09:03<02:58,  3.01it/s, est. speed input: 11886.26 toks/s, output: 5.43 toks/s]Processed prompts:  93%|█████████▎| 1865/2013 [10:02<00:39,  3.72it/s, est. speed input: 14277.47 toks/s, output: 6.19 toks/s]Processed prompts: 100%|██████████| 2013/2013 [10:02<00:00,  3.34it/s, est. speed input: 15780.20 toks/s, output: 6.68 toks/s]
Evaluating task: topology
Processed prompts:   0%|          | 0/872 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/872 [02:16<33:04:58, 136.74s/it, est. speed input: 67.71 toks/s, output: 0.01 toks/s]Processed prompts:  48%|████▊     | 420/872 [04:37<04:14,  1.77it/s, est. speed input: 7751.76 toks/s, output: 3.03 toks/s]Processed prompts:  76%|███████▌  | 660/872 [06:44<01:55,  1.83it/s, est. speed input: 10543.68 toks/s, output: 3.27 toks/s]Processed prompts: 100%|██████████| 872/872 [06:44<00:00,  2.16it/s, est. speed input: 15201.91 toks/s, output: 4.32 toks/s]
Evaluating task: shortest
Processed prompts:   0%|          | 0/1392 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/1392 [02:18<53:42:20, 138.99s/it, est. speed input: 58.53 toks/s, output: 0.01 toks/s]Processed prompts:  24%|██▍       | 340/1392 [04:39<12:16,  1.43it/s, est. speed input: 7643.70 toks/s, output: 2.43 toks/s]Processed prompts:  48%|████▊     | 662/1392 [06:58<06:38,  1.83it/s, est. speed input: 10180.71 toks/s, output: 3.16 toks/s]Processed prompts:  68%|██████▊   | 948/1392 [09:18<03:51,  1.92it/s, est. speed input: 11422.96 toks/s, output: 3.39 toks/s]Processed prompts:  89%|████████▉ | 1242/1392 [10:35<01:03,  2.36it/s, est. speed input: 13365.91 toks/s, output: 3.91 toks/s]Processed prompts: 100%|██████████| 1392/1392 [10:35<00:00,  2.19it/s, est. speed input: 15190.31 toks/s, output: 4.38 toks/s]
Evaluating task: triangle
Processed prompts:   0%|          | 0/2756 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2756 [02:16<104:12:15, 136.17s/it, est. speed input: 28.34 toks/s, output: 0.01 toks/s]Processed prompts:  27%|██▋       | 733/2756 [04:30<10:34,  3.19it/s, est. speed input: 8159.79 toks/s, output: 5.42 toks/s] Processed prompts:  54%|█████▍    | 1487/2756 [06:47<04:57,  4.26it/s, est. speed input: 10846.68 toks/s, output: 7.31 toks/s]Processed prompts:  73%|███████▎  | 1999/2756 [09:04<03:07,  4.04it/s, est. speed input: 12074.66 toks/s, output: 7.34 toks/s]Processed prompts:  90%|████████▉ | 2473/2756 [10:32<01:04,  4.41it/s, est. speed input: 13819.72 toks/s, output: 7.82 toks/s]Processed prompts: 100%|██████████| 2756/2756 [10:32<00:00,  4.36it/s, est. speed input: 15986.41 toks/s, output: 8.72 toks/s]
Evaluating task: flow
Processed prompts:   0%|          | 0/405 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/405 [02:19<15:38:26, 139.37s/it, est. speed input: 16.01 toks/s, output: 0.01 toks/s]Processed prompts: 100%|█████████▉| 403/405 [02:21<00:00,  4.06it/s, est. speed input: 15222.23 toks/s, output: 5.71 toks/s]Processed prompts: 100%|██████████| 405/405 [02:21<00:00,  2.87it/s, est. speed input: 15328.03 toks/s, output: 5.74 toks/s]
Evaluating task: hamilton
Processed prompts:   0%|          | 0/2097 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2097 [02:15<79:07:29, 135.90s/it, est. speed input: 9.89 toks/s, output: 0.01 toks/s]Processed prompts:  35%|███▍      | 725/2097 [04:32<07:18,  3.13it/s, est. speed input: 8087.51 toks/s, output: 5.32 toks/s]Processed prompts:  60%|█████▉    | 1252/2097 [06:51<04:06,  3.43it/s, est. speed input: 10621.37 toks/s, output: 6.09 toks/s]Processed prompts:  75%|███████▍  | 1566/2097 [09:12<02:59,  2.95it/s, est. speed input: 11766.03 toks/s, output: 5.67 toks/s]Processed prompts:  87%|████████▋ | 1827/2097 [11:34<01:46,  2.55it/s, est. speed input: 12399.51 toks/s, output: 5.26 toks/s]Processed prompts: 100%|█████████▉| 2087/2097 [11:40<00:02,  3.42it/s, est. speed input: 15299.42 toks/s, output: 5.96 toks/s]Processed prompts: 100%|██████████| 2097/2097 [11:40<00:00,  2.99it/s, est. speed input: 15431.21 toks/s, output: 5.98 toks/s]
Evaluating task: substructure
Processed prompts:   0%|          | 0/3186 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/3186 [02:15<120:07:50, 135.78s/it, est. speed input: 10.95 toks/s, output: 0.01 toks/s]Processed prompts:  18%|█▊        | 571/3186 [04:32<17:42,  2.46it/s, est. speed input: 8012.50 toks/s, output: 4.19 toks/s] Processed prompts:  33%|███▎      | 1047/3186 [06:51<12:16,  2.90it/s, est. speed input: 10551.19 toks/s, output: 5.08 toks/s]Processed prompts:  46%|████▌     | 1460/3186 [09:09<09:45,  2.95it/s, est. speed input: 11839.05 toks/s, output: 5.32 toks/s]Processed prompts:  59%|█████▉    | 1885/3186 [11:26<07:14,  3.00it/s, est. speed input: 12591.55 toks/s, output: 5.49 toks/s]Processed prompts:  71%|███████▏  | 2277/3186 [13:47<05:11,  2.92it/s, est. speed input: 13036.65 toks/s, output: 5.50 toks/s]Processed prompts:  82%|████████▏ | 2606/3186 [16:07<03:31,  2.74it/s, est. speed input: 13364.51 toks/s, output: 5.39 toks/s]Processed prompts:  91%|█████████ | 2904/3186 [18:20<01:49,  2.58it/s, est. speed input: 13682.49 toks/s, output: 5.28 toks/s]Processed prompts: 100%|██████████| 3186/3186 [18:20<00:00,  2.90it/s, est. speed input: 15518.34 toks/s, output: 5.79 toks/s]
Accuracy data has been saved to 'task_accuracies.txt'.
INFO 04-23 14:08:09 multiproc_worker_utils.py:140] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=2718744)[0;0m INFO 04-23 14:08:09 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2718745)[0;0m INFO 04-23 14:08:09 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2718746)[0;0m INFO 04-23 14:08:09 multiproc_worker_utils.py:247] Worker exiting
[rank0]:[W423 14:08:31.533141312 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/home/zch/anaconda3/envs/GraphICL/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO 04-23 14:08:51 config.py:510] This model supports multiple tasks: {'classify', 'embed', 'score', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 04-23 14:08:51 config.py:1310] Defaulting to use mp for distributed inference
INFO 04-23 14:08:51 llm_engine.py:234] Initializing an LLM engine (v0.6.6) with config: model='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', speculative_config=None, tokenizer='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[1024,1016,1008,1000,992,984,976,968,960,952,944,936,928,920,912,904,896,888,880,872,864,856,848,840,832,824,816,808,800,792,784,776,768,760,752,744,736,728,720,712,704,696,688,680,672,664,656,648,640,632,624,616,608,600,592,584,576,568,560,552,544,536,528,520,512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":1024}, use_cached_outputs=False, 
WARNING 04-23 14:08:51 multiproc_worker_utils.py:312] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 04-23 14:08:51 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=2719982)[0;0m INFO 04-23 14:08:53 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2719982)[0;0m INFO 04-23 14:08:53 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2719983)[0;0m INFO 04-23 14:08:53 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2719983)[0;0m INFO 04-23 14:08:53 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2719981)[0;0m INFO 04-23 14:08:53 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2719981)[0;0m INFO 04-23 14:08:53 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
INFO 04-23 14:08:53 selector.py:120] Using Flash Attention backend.
INFO 04-23 14:08:54 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2719981)[0;0m INFO 04-23 14:08:54 utils.py:918] Found nccl from library libnccl.so.2
INFO 04-23 14:08:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2719983)[0;0m INFO 04-23 14:08:54 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2719981)[0;0m INFO 04-23 14:08:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2719982)[0;0m INFO 04-23 14:08:54 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2719983)[0;0m INFO 04-23 14:08:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2719982)[0;0m INFO 04-23 14:08:54 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2719981)[0;0m WARNING 04-23 14:08:55 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 04-23 14:08:55 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2719983)[0;0m WARNING 04-23 14:08:55 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2719982)[0;0m WARNING 04-23 14:08:55 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 04-23 14:08:55 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_cfe8f5ad'), local_subscribe_port=57725, remote_subscribe_port=None)
INFO 04-23 14:08:55 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2719982)[0;0m INFO 04-23 14:08:55 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2719981)[0;0m INFO 04-23 14:08:55 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2719983)[0;0m INFO 04-23 14:08:55 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.65it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.83it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.68it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.75it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.74it/s]

[1;36m(VllmWorkerProcess pid=2719981)[0;0m INFO 04-23 14:08:57 model_runner.py:1099] Loading model weights took 3.5546 GB
INFO 04-23 14:08:57 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2719983)[0;0m INFO 04-23 14:08:57 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2719982)[0;0m INFO 04-23 14:08:57 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2719981)[0;0m INFO 04-23 14:09:01 worker.py:241] Memory profiling takes 4.12 seconds
[1;36m(VllmWorkerProcess pid=2719981)[0;0m INFO 04-23 14:09:01 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2719981)[0;0m INFO 04-23 14:09:01 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
[1;36m(VllmWorkerProcess pid=2719982)[0;0m INFO 04-23 14:09:01 worker.py:241] Memory profiling takes 4.18 seconds
[1;36m(VllmWorkerProcess pid=2719982)[0;0m INFO 04-23 14:09:01 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2719982)[0;0m INFO 04-23 14:09:01 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
[1;36m(VllmWorkerProcess pid=2719983)[0;0m INFO 04-23 14:09:01 worker.py:241] Memory profiling takes 4.19 seconds
[1;36m(VllmWorkerProcess pid=2719983)[0;0m INFO 04-23 14:09:01 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2719983)[0;0m INFO 04-23 14:09:01 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.57GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.94GiB.
INFO 04-23 14:09:02 worker.py:241] Memory profiling takes 4.32 seconds
INFO 04-23 14:09:02 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
INFO 04-23 14:09:02 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.68GiB; PyTorch activation peak memory takes 5.74GiB; the rest of the memory reserved for KV Cache is 28.05GiB.
INFO 04-23 14:09:02 distributed_gpu_executor.py:57] # GPU blocks: 131317, # CPU blocks: 74898
INFO 04-23 14:09:02 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 64.12x
[1;36m(VllmWorkerProcess pid=2719982)[0;0m INFO 04-23 14:09:25 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-23 14:09:25 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2719983)[0;0m INFO 04-23 14:09:25 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2719981)[0;0m INFO 04-23 14:09:25 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/131 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|          | 1/131 [00:00<01:47,  1.21it/s]Capturing CUDA graph shapes:   2%|▏         | 2/131 [00:01<01:35,  1.36it/s]Capturing CUDA graph shapes:   2%|▏         | 3/131 [00:02<01:34,  1.36it/s]Capturing CUDA graph shapes:   3%|▎         | 4/131 [00:02<01:34,  1.34it/s]Capturing CUDA graph shapes:   4%|▍         | 5/131 [00:03<01:31,  1.37it/s]Capturing CUDA graph shapes:   5%|▍         | 6/131 [00:04<01:32,  1.36it/s]Capturing CUDA graph shapes:   5%|▌         | 7/131 [00:05<01:31,  1.35it/s]Capturing CUDA graph shapes:   6%|▌         | 8/131 [00:05<01:29,  1.37it/s]Capturing CUDA graph shapes:   7%|▋         | 9/131 [00:06<01:29,  1.36it/s]Capturing CUDA graph shapes:   8%|▊         | 10/131 [00:07<01:27,  1.38it/s]Capturing CUDA graph shapes:   8%|▊         | 11/131 [00:08<01:26,  1.38it/s]Capturing CUDA graph shapes:   9%|▉         | 12/131 [00:08<01:26,  1.37it/s]Capturing CUDA graph shapes:  10%|▉         | 13/131 [00:09<01:25,  1.38it/s]Capturing CUDA graph shapes:  11%|█         | 14/131 [00:10<01:24,  1.38it/s]Capturing CUDA graph shapes:  11%|█▏        | 15/131 [00:10<01:24,  1.37it/s]Capturing CUDA graph shapes:  12%|█▏        | 16/131 [00:11<01:24,  1.36it/s]Capturing CUDA graph shapes:  13%|█▎        | 17/131 [00:12<01:22,  1.37it/s]Capturing CUDA graph shapes:  14%|█▎        | 18/131 [00:13<01:23,  1.36it/s]Capturing CUDA graph shapes:  15%|█▍        | 19/131 [00:13<01:22,  1.35it/s]Capturing CUDA graph shapes:  15%|█▌        | 20/131 [00:14<01:21,  1.36it/s]Capturing CUDA graph shapes:  16%|█▌        | 21/131 [00:15<01:21,  1.35it/s]Capturing CUDA graph shapes:  17%|█▋        | 22/131 [00:16<01:19,  1.37it/s]Capturing CUDA graph shapes:  18%|█▊        | 23/131 [00:16<01:19,  1.36it/s]Capturing CUDA graph shapes:  18%|█▊        | 24/131 [00:17<01:17,  1.38it/s]Capturing CUDA graph shapes:  19%|█▉        | 25/131 [00:18<01:15,  1.40it/s]Capturing CUDA graph shapes:  20%|█▉        | 26/131 [00:19<01:16,  1.38it/s]Capturing CUDA graph shapes:  21%|██        | 27/131 [00:19<01:13,  1.41it/s]Capturing CUDA graph shapes:  21%|██▏       | 28/131 [00:20<01:11,  1.43it/s]Capturing CUDA graph shapes:  22%|██▏       | 29/131 [00:21<01:11,  1.43it/s]Capturing CUDA graph shapes:  23%|██▎       | 30/131 [00:21<01:11,  1.40it/s]Capturing CUDA graph shapes:  24%|██▎       | 31/131 [00:22<01:10,  1.41it/s]Capturing CUDA graph shapes:  24%|██▍       | 32/131 [00:23<01:10,  1.40it/s]Capturing CUDA graph shapes:  25%|██▌       | 33/131 [00:23<01:09,  1.41it/s]Capturing CUDA graph shapes:  26%|██▌       | 34/131 [00:24<01:08,  1.41it/s]Capturing CUDA graph shapes:  27%|██▋       | 35/131 [00:25<01:07,  1.43it/s]Capturing CUDA graph shapes:  27%|██▋       | 36/131 [00:26<01:07,  1.42it/s]Capturing CUDA graph shapes:  28%|██▊       | 37/131 [00:26<01:05,  1.43it/s]Capturing CUDA graph shapes:  29%|██▉       | 38/131 [00:27<01:05,  1.42it/s]Capturing CUDA graph shapes:  30%|██▉       | 39/131 [00:28<01:02,  1.47it/s]Capturing CUDA graph shapes:  31%|███       | 40/131 [00:28<01:02,  1.45it/s]Capturing CUDA graph shapes:  31%|███▏      | 41/131 [00:29<01:02,  1.44it/s]Capturing CUDA graph shapes:  32%|███▏      | 42/131 [00:30<01:01,  1.45it/s]Capturing CUDA graph shapes:  33%|███▎      | 43/131 [00:30<01:01,  1.44it/s]Capturing CUDA graph shapes:  34%|███▎      | 44/131 [00:31<01:01,  1.42it/s]Capturing CUDA graph shapes:  34%|███▍      | 45/131 [00:32<00:59,  1.45it/s]Capturing CUDA graph shapes:  35%|███▌      | 46/131 [00:32<00:58,  1.44it/s]Capturing CUDA graph shapes:  36%|███▌      | 47/131 [00:33<00:58,  1.44it/s]Capturing CUDA graph shapes:  37%|███▋      | 48/131 [00:34<00:57,  1.45it/s]Capturing CUDA graph shapes:  37%|███▋      | 49/131 [00:35<00:56,  1.45it/s]Capturing CUDA graph shapes:  38%|███▊      | 50/131 [00:35<00:56,  1.44it/s]Capturing CUDA graph shapes:  39%|███▉      | 51/131 [00:36<00:55,  1.44it/s]Capturing CUDA graph shapes:  40%|███▉      | 52/131 [00:37<00:54,  1.45it/s]Capturing CUDA graph shapes:  40%|████      | 53/131 [00:37<00:53,  1.46it/s]Capturing CUDA graph shapes:  41%|████      | 54/131 [00:38<00:52,  1.45it/s]Capturing CUDA graph shapes:  42%|████▏     | 55/131 [00:39<00:51,  1.46it/s]Capturing CUDA graph shapes:  43%|████▎     | 56/131 [00:39<00:51,  1.47it/s]Capturing CUDA graph shapes:  44%|████▎     | 57/131 [00:40<00:50,  1.46it/s]Capturing CUDA graph shapes:  44%|████▍     | 58/131 [00:41<00:49,  1.47it/s]Capturing CUDA graph shapes:  45%|████▌     | 59/131 [00:41<00:48,  1.48it/s]Capturing CUDA graph shapes:  46%|████▌     | 60/131 [00:42<00:47,  1.48it/s]Capturing CUDA graph shapes:  47%|████▋     | 61/131 [00:43<00:47,  1.47it/s]Capturing CUDA graph shapes:  47%|████▋     | 62/131 [00:43<00:46,  1.48it/s]Capturing CUDA graph shapes:  48%|████▊     | 63/131 [00:44<00:46,  1.46it/s]Capturing CUDA graph shapes:  49%|████▉     | 64/131 [00:45<00:44,  1.49it/s]Capturing CUDA graph shapes:  50%|████▉     | 65/131 [00:45<00:43,  1.53it/s]Capturing CUDA graph shapes:  50%|█████     | 66/131 [00:46<00:43,  1.51it/s]Capturing CUDA graph shapes:  51%|█████     | 67/131 [00:47<00:41,  1.53it/s]Capturing CUDA graph shapes:  52%|█████▏    | 68/131 [00:47<00:41,  1.51it/s]Capturing CUDA graph shapes:  53%|█████▎    | 69/131 [00:48<00:41,  1.50it/s]Capturing CUDA graph shapes:  53%|█████▎    | 70/131 [00:49<00:41,  1.49it/s]Capturing CUDA graph shapes:  54%|█████▍    | 71/131 [00:49<00:40,  1.47it/s]Capturing CUDA graph shapes:  55%|█████▍    | 72/131 [00:50<00:40,  1.46it/s]Capturing CUDA graph shapes:  56%|█████▌    | 73/131 [00:51<00:39,  1.47it/s]Capturing CUDA graph shapes:  56%|█████▋    | 74/131 [00:51<00:38,  1.47it/s]Capturing CUDA graph shapes:  57%|█████▋    | 75/131 [00:52<00:37,  1.50it/s]Capturing CUDA graph shapes:  58%|█████▊    | 76/131 [00:53<00:36,  1.51it/s]Capturing CUDA graph shapes:  59%|█████▉    | 77/131 [00:53<00:34,  1.54it/s]Capturing CUDA graph shapes:  60%|█████▉    | 78/131 [00:54<00:34,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 79/131 [00:55<00:33,  1.55it/s]Capturing CUDA graph shapes:  61%|██████    | 80/131 [00:55<00:31,  1.59it/s]Capturing CUDA graph shapes:  62%|██████▏   | 81/131 [00:56<00:30,  1.61it/s]Capturing CUDA graph shapes:  63%|██████▎   | 82/131 [00:56<00:30,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 83/131 [00:57<00:29,  1.61it/s]Capturing CUDA graph shapes:  64%|██████▍   | 84/131 [00:58<00:28,  1.64it/s]Capturing CUDA graph shapes:  65%|██████▍   | 85/131 [00:58<00:28,  1.64it/s]Capturing CUDA graph shapes:  66%|██████▌   | 86/131 [00:59<00:27,  1.66it/s]Capturing CUDA graph shapes:  66%|██████▋   | 87/131 [00:59<00:26,  1.63it/s]Capturing CUDA graph shapes:  67%|██████▋   | 88/131 [01:00<00:25,  1.68it/s]Capturing CUDA graph shapes:  68%|██████▊   | 89/131 [01:01<00:24,  1.69it/s]Capturing CUDA graph shapes:  69%|██████▊   | 90/131 [01:01<00:24,  1.67it/s]Capturing CUDA graph shapes:  69%|██████▉   | 91/131 [01:02<00:24,  1.64it/s]Capturing CUDA graph shapes:  70%|███████   | 92/131 [01:02<00:23,  1.67it/s]Capturing CUDA graph shapes:  71%|███████   | 93/131 [01:03<00:22,  1.67it/s]Capturing CUDA graph shapes:  72%|███████▏  | 94/131 [01:04<00:21,  1.68it/s]Capturing CUDA graph shapes:  73%|███████▎  | 95/131 [01:04<00:21,  1.67it/s]Capturing CUDA graph shapes:  73%|███████▎  | 96/131 [01:05<00:21,  1.65it/s]Capturing CUDA graph shapes:  74%|███████▍  | 97/131 [01:05<00:20,  1.68it/s]Capturing CUDA graph shapes:  75%|███████▍  | 98/131 [01:06<00:19,  1.70it/s]Capturing CUDA graph shapes:  76%|███████▌  | 99/131 [01:07<00:18,  1.70it/s]Capturing CUDA graph shapes:  76%|███████▋  | 100/131 [01:07<00:18,  1.72it/s]Capturing CUDA graph shapes:  77%|███████▋  | 101/131 [01:08<00:17,  1.74it/s]Capturing CUDA graph shapes:  78%|███████▊  | 102/131 [01:08<00:16,  1.73it/s]Capturing CUDA graph shapes:  79%|███████▊  | 103/131 [01:09<00:16,  1.71it/s]Capturing CUDA graph shapes:  79%|███████▉  | 104/131 [01:09<00:15,  1.70it/s]Capturing CUDA graph shapes:  80%|████████  | 105/131 [01:10<00:15,  1.73it/s]Capturing CUDA graph shapes:  81%|████████  | 106/131 [01:11<00:14,  1.74it/s]Capturing CUDA graph shapes:  82%|████████▏ | 107/131 [01:11<00:13,  1.73it/s]Capturing CUDA graph shapes:  82%|████████▏ | 108/131 [01:12<00:13,  1.72it/s]Capturing CUDA graph shapes:  83%|████████▎ | 109/131 [01:12<00:12,  1.71it/s]Capturing CUDA graph shapes:  84%|████████▍ | 110/131 [01:13<00:12,  1.70it/s]Capturing CUDA graph shapes:  85%|████████▍ | 111/131 [01:14<00:11,  1.72it/s]Capturing CUDA graph shapes:  85%|████████▌ | 112/131 [01:14<00:10,  1.76it/s]Capturing CUDA graph shapes:  86%|████████▋ | 113/131 [01:15<00:10,  1.75it/s]Capturing CUDA graph shapes:  87%|████████▋ | 114/131 [01:15<00:09,  1.77it/s]Capturing CUDA graph shapes:  88%|████████▊ | 115/131 [01:16<00:09,  1.76it/s]Capturing CUDA graph shapes:  89%|████████▊ | 116/131 [01:16<00:08,  1.75it/s]Capturing CUDA graph shapes:  89%|████████▉ | 117/131 [01:17<00:07,  1.77it/s]Capturing CUDA graph shapes:  90%|█████████ | 118/131 [01:18<00:07,  1.75it/s]Capturing CUDA graph shapes:  91%|█████████ | 119/131 [01:18<00:06,  1.76it/s]Capturing CUDA graph shapes:  92%|█████████▏| 120/131 [01:19<00:06,  1.79it/s]Capturing CUDA graph shapes:  92%|█████████▏| 121/131 [01:19<00:05,  1.76it/s]Capturing CUDA graph shapes:  93%|█████████▎| 122/131 [01:20<00:05,  1.78it/s]Capturing CUDA graph shapes:  94%|█████████▍| 123/131 [01:20<00:04,  1.78it/s]Capturing CUDA graph shapes:  95%|█████████▍| 124/131 [01:21<00:03,  1.76it/s]Capturing CUDA graph shapes:  95%|█████████▌| 125/131 [01:21<00:03,  1.77it/s]Capturing CUDA graph shapes:  96%|█████████▌| 126/131 [01:22<00:02,  1.73it/s]Capturing CUDA graph shapes:  97%|█████████▋| 127/131 [01:23<00:02,  1.73it/s]Capturing CUDA graph shapes:  98%|█████████▊| 128/131 [01:23<00:01,  1.72it/s]Capturing CUDA graph shapes:  98%|█████████▊| 129/131 [01:24<00:01,  1.75it/s]Capturing CUDA graph shapes:  99%|█████████▉| 130/131 [01:24<00:00,  1.76it/s][1;36m(VllmWorkerProcess pid=2719983)[0;0m INFO 04-23 14:10:50 model_runner.py:1535] Graph capturing finished in 86 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2719981)[0;0m INFO 04-23 14:10:50 model_runner.py:1535] Graph capturing finished in 86 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2719982)[0;0m INFO 04-23 14:10:50 model_runner.py:1535] Graph capturing finished in 86 secs, took 3.72 GiB
Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:25<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:25<00:00,  1.53it/s]
INFO 04-23 14:10:50 model_runner.py:1535] Graph capturing finished in 86 secs, took 3.72 GiB
INFO 04-23 14:10:50 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 113.12 seconds
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 18125 examples [00:01, 11118.13 examples/s]Generating train split: 18125 examples [00:01, 11062.06 examples/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclExamplesTestResults.py:128: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: cycle
Processed prompts:   0%|          | 0/2717 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2717 [02:08<96:36:44, 128.06s/it, est. speed input: 36.91 toks/s, output: 0.02 toks/s]Processed prompts:  38%|███▊      | 1025/2717 [04:21<06:08,  4.60it/s, est. speed input: 8391.21 toks/s, output: 7.84 toks/s]Processed prompts:  67%|██████▋   | 1833/2717 [06:36<02:48,  5.24it/s, est. speed input: 11099.01 toks/s, output: 9.24 toks/s]Processed prompts:  89%|████████▉ | 2429/2717 [07:59<00:49,  5.80it/s, est. speed input: 13692.09 toks/s, output: 10.13 toks/s]Processed prompts: 100%|██████████| 2717/2717 [07:59<00:00,  5.66it/s, est. speed input: 16426.63 toks/s, output: 11.33 toks/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclExamplesTestResults.py:128: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: connectivity
Processed prompts:   0%|          | 0/2687 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2687 [01:13<54:31:21, 73.08s/it, est. speed input: 26.64 toks/s, output: 0.03 toks/s]Processed prompts:  38%|███▊      | 1025/2687 [02:57<04:12,  6.58it/s, est. speed input: 7450.53 toks/s, output: 11.54 toks/s]Processed prompts:  76%|███████▋  | 2049/2687 [04:51<01:21,  7.82it/s, est. speed input: 10743.80 toks/s, output: 14.07 toks/s]Processed prompts: 100%|██████████| 2687/2687 [04:51<00:00,  9.22it/s, est. speed input: 17067.75 toks/s, output: 18.45 toks/s]
Evaluating task: bipartite
Processed prompts:   0%|          | 0/2013 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2013 [02:12<74:18:02, 132.94s/it, est. speed input: 6.80 toks/s, output: 0.02 toks/s]Processed prompts:  47%|████▋     | 950/2013 [04:26<04:14,  4.18it/s, est. speed input: 8355.78 toks/s, output: 7.12 toks/s]Processed prompts:  80%|████████  | 1620/2013 [06:07<01:16,  5.14it/s, est. speed input: 11988.93 toks/s, output: 8.81 toks/s]Processed prompts: 100%|██████████| 2013/2013 [06:07<00:00,  5.47it/s, est. speed input: 16317.98 toks/s, output: 10.95 toks/s]
Evaluating task: topology
Processed prompts:   0%|          | 0/872 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/872 [02:16<33:02:41, 136.58s/it, est. speed input: 67.79 toks/s, output: 0.01 toks/s]Processed prompts:  58%|█████▊    | 502/872 [04:35<02:53,  2.13it/s, est. speed input: 7845.51 toks/s, output: 3.64 toks/s]Processed prompts:  87%|████████▋ | 758/872 [05:44<00:43,  2.61it/s, est. speed input: 12417.37 toks/s, output: 4.40 toks/s]Processed prompts: 100%|██████████| 872/872 [05:44<00:00,  2.53it/s, est. speed input: 15373.23 toks/s, output: 5.06 toks/s]
Evaluating task: shortest
Processed prompts:   0%|          | 0/1392 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/1392 [02:17<53:12:16, 137.70s/it, est. speed input: 53.54 toks/s, output: 0.01 toks/s]Processed prompts:  32%|███▏      | 448/1392 [04:34<08:11,  1.92it/s, est. speed input: 7832.07 toks/s, output: 3.26 toks/s]Processed prompts:  64%|██████▎   | 885/1392 [06:52<03:23,  2.49it/s, est. speed input: 10409.55 toks/s, output: 4.29 toks/s]Processed prompts:  90%|█████████ | 1255/1392 [07:58<00:42,  3.24it/s, est. speed input: 13438.19 toks/s, output: 5.25 toks/s]Processed prompts: 100%|██████████| 1392/1392 [07:58<00:00,  2.91it/s, est. speed input: 15505.23 toks/s, output: 5.82 toks/s]
Evaluating task: triangle
Processed prompts:   0%|          | 0/2756 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2756 [01:32<70:41:20, 92.37s/it, est. speed input: 52.29 toks/s, output: 0.02 toks/s]Processed prompts:  37%|███▋      | 1025/2756 [03:34<05:15,  5.48it/s, est. speed input: 7528.45 toks/s, output: 9.55 toks/s]Processed prompts:  74%|███████▍  | 2049/2756 [05:49<01:47,  6.56it/s, est. speed input: 10617.38 toks/s, output: 11.74 toks/s]Processed prompts:  99%|█████████▉| 2741/2756 [05:52<00:01, 10.09it/s, est. speed input: 16740.57 toks/s, output: 15.54 toks/s]Processed prompts: 100%|██████████| 2756/2756 [05:52<00:00,  7.81it/s, est. speed input: 16888.50 toks/s, output: 15.63 toks/s]
Evaluating task: flow
Processed prompts:   0%|          | 0/405 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/405 [01:39<11:06:48, 99.03s/it, est. speed input: 14.52 toks/s, output: 0.02 toks/s]Processed prompts: 100%|██████████| 405/405 [01:39<00:00,  4.09it/s, est. speed input: 15620.68 toks/s, output: 8.18 toks/s]
Evaluating task: hamilton
Processed prompts:   0%|          | 0/2097 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2097 [02:13<77:28:38, 133.07s/it, est. speed input: 6.58 toks/s, output: 0.02 toks/s]Processed prompts:  47%|████▋     | 986/2097 [04:28<04:17,  4.31it/s, est. speed input: 8363.74 toks/s, output: 7.34 toks/s]Processed prompts:  72%|███████▏  | 1509/2097 [06:47<02:24,  4.06it/s, est. speed input: 10839.84 toks/s, output: 7.41 toks/s]Processed prompts:  90%|█████████ | 1890/2097 [08:06<00:48,  4.26it/s, est. speed input: 13445.03 toks/s, output: 7.77 toks/s]Processed prompts: 100%|██████████| 2097/2097 [08:06<00:00,  4.31it/s, est. speed input: 15969.09 toks/s, output: 8.62 toks/s]
Evaluating task: substructure
Processed prompts:   0%|          | 0/3186 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/3186 [02:13<118:23:59, 133.83s/it, est. speed input: 8.74 toks/s, output: 0.01 toks/s]Processed prompts:  26%|██▌       | 821/3186 [04:29<10:59,  3.59it/s, est. speed input: 8292.04 toks/s, output: 6.10 toks/s]Processed prompts:  42%|████▏     | 1351/3186 [06:44<08:11,  3.73it/s, est. speed input: 10873.84 toks/s, output: 6.67 toks/s]Processed prompts:  61%|██████    | 1949/3186 [09:01<05:10,  3.99it/s, est. speed input: 12158.67 toks/s, output: 7.20 toks/s]Processed prompts:  75%|███████▌  | 2401/3186 [11:18<03:30,  3.73it/s, est. speed input: 12874.90 toks/s, output: 7.07 toks/s]Processed prompts:  86%|████████▋ | 2752/3186 [13:38<02:10,  3.31it/s, est. speed input: 13292.91 toks/s, output: 6.73 toks/s]Processed prompts:  96%|█████████▌| 3064/3186 [14:27<00:32,  3.76it/s, est. speed input: 14987.24 toks/s, output: 7.06 toks/s]Processed prompts: 100%|██████████| 3186/3186 [14:27<00:00,  3.67it/s, est. speed input: 15867.38 toks/s, output: 7.34 toks/s]
Accuracy data has been saved to 'task_accuracies.txt'.
INFO 04-23 15:15:24 multiproc_worker_utils.py:140] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=2719981)[0;0m INFO 04-23 15:15:24 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2719982)[0;0m INFO 04-23 15:15:24 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2719983)[0;0m INFO 04-23 15:15:24 multiproc_worker_utils.py:247] Worker exiting
[rank0]:[W423 15:15:43.585712699 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/home/zch/anaconda3/envs/GraphICL/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO 04-23 15:16:02 config.py:510] This model supports multiple tasks: {'reward', 'embed', 'score', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 04-23 15:16:02 config.py:1310] Defaulting to use mp for distributed inference
INFO 04-23 15:16:02 llm_engine.py:234] Initializing an LLM engine (v0.6.6) with config: model='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', speculative_config=None, tokenizer='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[1024,1016,1008,1000,992,984,976,968,960,952,944,936,928,920,912,904,896,888,880,872,864,856,848,840,832,824,816,808,800,792,784,776,768,760,752,744,736,728,720,712,704,696,688,680,672,664,656,648,640,632,624,616,608,600,592,584,576,568,560,552,544,536,528,520,512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":1024}, use_cached_outputs=False, 
WARNING 04-23 15:16:03 multiproc_worker_utils.py:312] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 04-23 15:16:03 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=2720979)[0;0m INFO 04-23 15:16:04 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2720979)[0;0m INFO 04-23 15:16:04 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2720980)[0;0m INFO 04-23 15:16:04 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2720980)[0;0m INFO 04-23 15:16:04 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
INFO 04-23 15:16:04 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2720981)[0;0m INFO 04-23 15:16:04 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2720981)[0;0m INFO 04-23 15:16:04 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2720980)[0;0m INFO 04-23 15:16:05 utils.py:918] Found nccl from library libnccl.so.2
INFO 04-23 15:16:05 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2720979)[0;0m INFO 04-23 15:16:05 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2720980)[0;0m INFO 04-23 15:16:05 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 04-23 15:16:05 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2720979)[0;0m INFO 04-23 15:16:05 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2720981)[0;0m INFO 04-23 15:16:05 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2720981)[0;0m INFO 04-23 15:16:05 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2720980)[0;0m WARNING 04-23 15:16:06 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2720979)[0;0m WARNING 04-23 15:16:06 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 04-23 15:16:06 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2720981)[0;0m WARNING 04-23 15:16:06 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 04-23 15:16:06 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_8f11be83'), local_subscribe_port=46823, remote_subscribe_port=None)
INFO 04-23 15:16:06 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2720979)[0;0m INFO 04-23 15:16:06 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2720980)[0;0m INFO 04-23 15:16:06 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2720981)[0;0m INFO 04-23 15:16:06 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.50it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.68it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.51it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.56it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.56it/s]

[1;36m(VllmWorkerProcess pid=2720979)[0;0m INFO 04-23 15:16:08 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2720980)[0;0m INFO 04-23 15:16:08 model_runner.py:1099] Loading model weights took 3.5546 GB
INFO 04-23 15:16:08 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2720981)[0;0m INFO 04-23 15:16:08 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2720980)[0;0m INFO 04-23 15:16:12 worker.py:241] Memory profiling takes 4.19 seconds
[1;36m(VllmWorkerProcess pid=2720980)[0;0m INFO 04-23 15:16:12 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2720980)[0;0m INFO 04-23 15:16:12 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
[1;36m(VllmWorkerProcess pid=2720979)[0;0m INFO 04-23 15:16:12 worker.py:241] Memory profiling takes 4.20 seconds
[1;36m(VllmWorkerProcess pid=2720979)[0;0m INFO 04-23 15:16:12 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2720979)[0;0m INFO 04-23 15:16:12 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
[1;36m(VllmWorkerProcess pid=2720981)[0;0m INFO 04-23 15:16:12 worker.py:241] Memory profiling takes 4.23 seconds
[1;36m(VllmWorkerProcess pid=2720981)[0;0m INFO 04-23 15:16:12 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2720981)[0;0m INFO 04-23 15:16:12 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.57GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.94GiB.
INFO 04-23 15:16:12 worker.py:241] Memory profiling takes 4.44 seconds
INFO 04-23 15:16:12 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
INFO 04-23 15:16:12 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.68GiB; PyTorch activation peak memory takes 5.74GiB; the rest of the memory reserved for KV Cache is 28.05GiB.
INFO 04-23 15:16:13 distributed_gpu_executor.py:57] # GPU blocks: 131317, # CPU blocks: 74898
INFO 04-23 15:16:13 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 64.12x
[1;36m(VllmWorkerProcess pid=2720981)[0;0m INFO 04-23 15:16:35 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-23 15:16:35 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2720979)[0;0m INFO 04-23 15:16:35 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2720980)[0;0m INFO 04-23 15:16:35 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/131 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|          | 1/131 [00:00<01:47,  1.21it/s]Capturing CUDA graph shapes:   2%|▏         | 2/131 [00:01<01:35,  1.35it/s]Capturing CUDA graph shapes:   2%|▏         | 3/131 [00:02<01:35,  1.35it/s]Capturing CUDA graph shapes:   3%|▎         | 4/131 [00:03<01:34,  1.34it/s]Capturing CUDA graph shapes:   4%|▍         | 5/131 [00:03<01:33,  1.35it/s]Capturing CUDA graph shapes:   5%|▍         | 6/131 [00:04<01:33,  1.33it/s]Capturing CUDA graph shapes:   5%|▌         | 7/131 [00:05<01:30,  1.36it/s]Capturing CUDA graph shapes:   6%|▌         | 8/131 [00:05<01:27,  1.41it/s]Capturing CUDA graph shapes:   7%|▋         | 9/131 [00:06<01:27,  1.40it/s]Capturing CUDA graph shapes:   8%|▊         | 10/131 [00:07<01:24,  1.43it/s]Capturing CUDA graph shapes:   8%|▊         | 11/131 [00:07<01:24,  1.42it/s]Capturing CUDA graph shapes:   9%|▉         | 12/131 [00:08<01:26,  1.38it/s]Capturing CUDA graph shapes:  10%|▉         | 13/131 [00:09<01:24,  1.40it/s]Capturing CUDA graph shapes:  11%|█         | 14/131 [00:10<01:23,  1.40it/s]Capturing CUDA graph shapes:  11%|█▏        | 15/131 [00:10<01:22,  1.40it/s]Capturing CUDA graph shapes:  12%|█▏        | 16/131 [00:11<01:22,  1.39it/s]Capturing CUDA graph shapes:  13%|█▎        | 17/131 [00:12<01:21,  1.41it/s]Capturing CUDA graph shapes:  14%|█▎        | 18/131 [00:13<01:21,  1.38it/s]Capturing CUDA graph shapes:  15%|█▍        | 19/131 [00:13<01:21,  1.38it/s]Capturing CUDA graph shapes:  15%|█▌        | 20/131 [00:14<01:20,  1.38it/s]Capturing CUDA graph shapes:  16%|█▌        | 21/131 [00:15<01:17,  1.41it/s]Capturing CUDA graph shapes:  17%|█▋        | 22/131 [00:15<01:18,  1.39it/s]Capturing CUDA graph shapes:  18%|█▊        | 23/131 [00:16<01:16,  1.42it/s]Capturing CUDA graph shapes:  18%|█▊        | 24/131 [00:17<01:14,  1.44it/s]Capturing CUDA graph shapes:  19%|█▉        | 25/131 [00:17<01:12,  1.45it/s]Capturing CUDA graph shapes:  20%|█▉        | 26/131 [00:18<01:12,  1.45it/s]Capturing CUDA graph shapes:  21%|██        | 27/131 [00:19<01:11,  1.46it/s]Capturing CUDA graph shapes:  21%|██▏       | 28/131 [00:20<01:11,  1.43it/s]Capturing CUDA graph shapes:  22%|██▏       | 29/131 [00:20<01:11,  1.42it/s]Capturing CUDA graph shapes:  23%|██▎       | 30/131 [00:21<01:11,  1.41it/s]Capturing CUDA graph shapes:  24%|██▎       | 31/131 [00:22<01:11,  1.40it/s]Capturing CUDA graph shapes:  24%|██▍       | 32/131 [00:22<01:08,  1.45it/s]Capturing CUDA graph shapes:  25%|██▌       | 33/131 [00:23<01:08,  1.43it/s]Capturing CUDA graph shapes:  26%|██▌       | 34/131 [00:24<01:07,  1.43it/s]Capturing CUDA graph shapes:  27%|██▋       | 35/131 [00:24<01:07,  1.43it/s]Capturing CUDA graph shapes:  27%|██▋       | 36/131 [00:25<01:06,  1.43it/s]Capturing CUDA graph shapes:  28%|██▊       | 37/131 [00:26<01:06,  1.42it/s]Capturing CUDA graph shapes:  29%|██▉       | 38/131 [00:27<01:04,  1.44it/s]Capturing CUDA graph shapes:  30%|██▉       | 39/131 [00:27<01:02,  1.48it/s]Capturing CUDA graph shapes:  31%|███       | 40/131 [00:28<01:02,  1.46it/s]Capturing CUDA graph shapes:  31%|███▏      | 41/131 [00:29<01:02,  1.45it/s]Capturing CUDA graph shapes:  32%|███▏      | 42/131 [00:29<01:00,  1.47it/s]Capturing CUDA graph shapes:  33%|███▎      | 43/131 [00:30<00:59,  1.48it/s]Capturing CUDA graph shapes:  34%|███▎      | 44/131 [00:31<00:59,  1.46it/s]Capturing CUDA graph shapes:  34%|███▍      | 45/131 [00:31<00:59,  1.45it/s]Capturing CUDA graph shapes:  35%|███▌      | 46/131 [00:32<00:57,  1.47it/s]Capturing CUDA graph shapes:  36%|███▌      | 47/131 [00:33<00:57,  1.46it/s]Capturing CUDA graph shapes:  37%|███▋      | 48/131 [00:33<00:57,  1.45it/s]Capturing CUDA graph shapes:  37%|███▋      | 49/131 [00:34<00:57,  1.43it/s]Capturing CUDA graph shapes:  38%|███▊      | 50/131 [00:35<00:54,  1.49it/s]Capturing CUDA graph shapes:  39%|███▉      | 51/131 [00:35<00:54,  1.48it/s]Capturing CUDA graph shapes:  40%|███▉      | 52/131 [00:36<00:54,  1.46it/s]Capturing CUDA graph shapes:  40%|████      | 53/131 [00:37<00:53,  1.45it/s]Capturing CUDA graph shapes:  41%|████      | 54/131 [00:37<00:52,  1.47it/s]Capturing CUDA graph shapes:  42%|████▏     | 55/131 [00:38<00:52,  1.46it/s]Capturing CUDA graph shapes:  43%|████▎     | 56/131 [00:39<00:50,  1.50it/s]Capturing CUDA graph shapes:  44%|████▎     | 57/131 [00:39<00:49,  1.48it/s]Capturing CUDA graph shapes:  44%|████▍     | 58/131 [00:40<00:49,  1.48it/s]Capturing CUDA graph shapes:  45%|████▌     | 59/131 [00:41<00:48,  1.48it/s]Capturing CUDA graph shapes:  46%|████▌     | 60/131 [00:41<00:48,  1.48it/s]Capturing CUDA graph shapes:  47%|████▋     | 61/131 [00:42<00:47,  1.47it/s]Capturing CUDA graph shapes:  47%|████▋     | 62/131 [00:43<00:46,  1.47it/s]Capturing CUDA graph shapes:  48%|████▊     | 63/131 [00:44<00:45,  1.48it/s]Capturing CUDA graph shapes:  49%|████▉     | 64/131 [00:44<00:45,  1.46it/s]Capturing CUDA graph shapes:  50%|████▉     | 65/131 [00:45<00:44,  1.49it/s]Capturing CUDA graph shapes:  50%|█████     | 66/131 [00:45<00:42,  1.53it/s]Capturing CUDA graph shapes:  51%|█████     | 67/131 [00:46<00:42,  1.51it/s]Capturing CUDA graph shapes:  52%|█████▏    | 68/131 [00:47<00:42,  1.50it/s]Capturing CUDA graph shapes:  53%|█████▎    | 69/131 [00:47<00:40,  1.54it/s]Capturing CUDA graph shapes:  53%|█████▎    | 70/131 [00:48<00:40,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 71/131 [00:49<00:39,  1.53it/s]Capturing CUDA graph shapes:  55%|█████▍    | 72/131 [00:49<00:39,  1.51it/s]Capturing CUDA graph shapes:  56%|█████▌    | 73/131 [00:50<00:37,  1.56it/s]Capturing CUDA graph shapes:  56%|█████▋    | 74/131 [00:51<00:37,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 75/131 [00:51<00:36,  1.54it/s]Capturing CUDA graph shapes:  58%|█████▊    | 76/131 [00:52<00:36,  1.52it/s]Capturing CUDA graph shapes:  59%|█████▉    | 77/131 [00:53<00:34,  1.58it/s]Capturing CUDA graph shapes:  60%|█████▉    | 78/131 [00:53<00:34,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 79/131 [00:54<00:34,  1.52it/s]Capturing CUDA graph shapes:  61%|██████    | 80/131 [00:55<00:33,  1.51it/s]Capturing CUDA graph shapes:  62%|██████▏   | 81/131 [00:55<00:33,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 82/131 [00:56<00:31,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 83/131 [00:57<00:31,  1.54it/s]Capturing CUDA graph shapes:  64%|██████▍   | 84/131 [00:57<00:31,  1.51it/s]Capturing CUDA graph shapes:  65%|██████▍   | 85/131 [00:58<00:29,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 86/131 [00:58<00:28,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▋   | 87/131 [00:59<00:28,  1.54it/s]Capturing CUDA graph shapes:  67%|██████▋   | 88/131 [01:00<00:28,  1.51it/s]Capturing CUDA graph shapes:  68%|██████▊   | 89/131 [01:00<00:27,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 90/131 [01:01<00:26,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▉   | 91/131 [01:02<00:26,  1.53it/s]Capturing CUDA graph shapes:  70%|███████   | 92/131 [01:02<00:25,  1.54it/s]Capturing CUDA graph shapes:  71%|███████   | 93/131 [01:03<00:24,  1.58it/s]Capturing CUDA graph shapes:  72%|███████▏  | 94/131 [01:04<00:23,  1.58it/s]Capturing CUDA graph shapes:  73%|███████▎  | 95/131 [01:04<00:22,  1.61it/s]Capturing CUDA graph shapes:  73%|███████▎  | 96/131 [01:05<00:22,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 97/131 [01:05<00:21,  1.61it/s]Capturing CUDA graph shapes:  75%|███████▍  | 98/131 [01:06<00:20,  1.62it/s]Capturing CUDA graph shapes:  76%|███████▌  | 99/131 [01:07<00:20,  1.58it/s]Capturing CUDA graph shapes:  76%|███████▋  | 100/131 [01:07<00:19,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 101/131 [01:08<00:18,  1.60it/s]Capturing CUDA graph shapes:  78%|███████▊  | 102/131 [01:09<00:18,  1.58it/s]Capturing CUDA graph shapes:  79%|███████▊  | 103/131 [01:09<00:17,  1.56it/s]Capturing CUDA graph shapes:  79%|███████▉  | 104/131 [01:10<00:17,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 105/131 [01:11<00:16,  1.58it/s]Capturing CUDA graph shapes:  81%|████████  | 106/131 [01:11<00:15,  1.58it/s]Capturing CUDA graph shapes:  82%|████████▏ | 107/131 [01:12<00:15,  1.56it/s]Capturing CUDA graph shapes:  82%|████████▏ | 108/131 [01:13<00:14,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 109/131 [01:13<00:13,  1.60it/s]Capturing CUDA graph shapes:  84%|████████▍ | 110/131 [01:14<00:13,  1.58it/s]Capturing CUDA graph shapes:  85%|████████▍ | 111/131 [01:14<00:12,  1.57it/s]Capturing CUDA graph shapes:  85%|████████▌ | 112/131 [01:15<00:12,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▋ | 113/131 [01:16<00:11,  1.57it/s]Capturing CUDA graph shapes:  87%|████████▋ | 114/131 [01:16<00:10,  1.57it/s]Capturing CUDA graph shapes:  88%|████████▊ | 115/131 [01:17<00:10,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 116/131 [01:18<00:09,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▉ | 117/131 [01:18<00:08,  1.58it/s]Capturing CUDA graph shapes:  90%|█████████ | 118/131 [01:19<00:08,  1.61it/s]Capturing CUDA graph shapes:  91%|█████████ | 119/131 [01:19<00:07,  1.60it/s]Capturing CUDA graph shapes:  92%|█████████▏| 120/131 [01:20<00:06,  1.62it/s]Capturing CUDA graph shapes:  92%|█████████▏| 121/131 [01:21<00:06,  1.58it/s]Capturing CUDA graph shapes:  93%|█████████▎| 122/131 [01:21<00:05,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 123/131 [01:22<00:04,  1.62it/s]Capturing CUDA graph shapes:  95%|█████████▍| 124/131 [01:23<00:04,  1.63it/s]Capturing CUDA graph shapes:  95%|█████████▌| 125/131 [01:23<00:03,  1.63it/s]Capturing CUDA graph shapes:  96%|█████████▌| 126/131 [01:24<00:03,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 127/131 [01:24<00:02,  1.61it/s]Capturing CUDA graph shapes:  98%|█████████▊| 128/131 [01:25<00:01,  1.61it/s]Capturing CUDA graph shapes:  98%|█████████▊| 129/131 [01:26<00:01,  1.61it/s]Capturing CUDA graph shapes:  99%|█████████▉| 130/131 [01:26<00:00,  1.62it/s][1;36m(VllmWorkerProcess pid=2720980)[0;0m INFO 04-23 15:18:03 model_runner.py:1535] Graph capturing finished in 87 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2720979)[0;0m INFO 04-23 15:18:03 model_runner.py:1535] Graph capturing finished in 87 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2720981)[0;0m INFO 04-23 15:18:03 model_runner.py:1535] Graph capturing finished in 88 secs, took 3.72 GiB
Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:27<00:00,  1.43it/s]Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:27<00:00,  1.50it/s]
INFO 04-23 15:18:03 model_runner.py:1535] Graph capturing finished in 88 secs, took 3.72 GiB
INFO 04-23 15:18:03 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 114.80 seconds
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 18125 examples [00:03, 5941.17 examples/s]Generating train split: 18125 examples [00:03, 5919.85 examples/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclExamplesTestResults.py:128: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: cycle
Processed prompts:   0%|          | 0/2717 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2717 [02:12<99:39:56, 132.10s/it, est. speed input: 15.22 toks/s, output: 0.02 toks/s]Processed prompts:  28%|██▊       | 767/2717 [04:25<09:35,  3.39it/s, est. speed input: 8298.86 toks/s, output: 5.77 toks/s]Processed prompts:  53%|█████▎    | 1446/2717 [06:42<05:07,  4.13it/s, est. speed input: 10911.25 toks/s, output: 7.19 toks/s]Processed prompts:  72%|███████▏  | 1951/2717 [08:58<03:13,  3.96it/s, est. speed input: 12162.67 toks/s, output: 7.25 toks/s]Processed prompts:  88%|████████▊ | 2395/2717 [10:50<01:21,  3.95it/s, est. speed input: 13354.86 toks/s, output: 7.36 toks/s]Processed prompts: 100%|██████████| 2717/2717 [10:50<00:00,  4.17it/s, est. speed input: 16048.52 toks/s, output: 8.35 toks/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclExamplesTestResults.py:128: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: connectivity
Processed prompts:   0%|          | 0/2687 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2687 [01:47<79:53:23, 107.08s/it, est. speed input: 23.24 toks/s, output: 0.02 toks/s]Processed prompts:  38%|███▊      | 1025/2687 [04:00<05:37,  4.93it/s, est. speed input: 7644.55 toks/s, output: 8.54 toks/s]Processed prompts:  73%|███████▎  | 1968/2687 [06:16<02:01,  5.90it/s, est. speed input: 10811.64 toks/s, output: 10.47 toks/s]Processed prompts:  93%|█████████▎| 2494/2687 [07:02<00:27,  6.94it/s, est. speed input: 14749.54 toks/s, output: 11.82 toks/s]Processed prompts: 100%|██████████| 2687/2687 [07:02<00:00,  6.36it/s, est. speed input: 16471.61 toks/s, output: 12.73 toks/s]
Evaluating task: bipartite
Processed prompts:   0%|          | 0/2013 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2013 [02:15<75:28:04, 135.03s/it, est. speed input: 11.25 toks/s, output: 0.01 toks/s]Processed prompts:  32%|███▏      | 645/2013 [04:29<08:06,  2.81it/s, est. speed input: 8080.14 toks/s, output: 4.78 toks/s]Processed prompts:  62%|██████▏   | 1254/2013 [06:46<03:32,  3.57it/s, est. speed input: 10724.62 toks/s, output: 6.18 toks/s]Processed prompts:  84%|████████▎ | 1681/2013 [08:37<01:30,  3.67it/s, est. speed input: 12566.88 toks/s, output: 6.50 toks/s]Processed prompts: 100%|██████████| 2013/2013 [08:37<00:00,  3.89it/s, est. speed input: 15899.34 toks/s, output: 7.79 toks/s]
Evaluating task: topology
Processed prompts:   0%|          | 0/872 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/872 [02:16<32:59:38, 136.37s/it, est. speed input: 67.90 toks/s, output: 0.01 toks/s]Processed prompts:  54%|█████▎    | 468/872 [04:36<03:23,  1.99it/s, est. speed input: 7807.20 toks/s, output: 3.39 toks/s]Processed prompts:  82%|████████▏ | 713/872 [06:13<01:12,  2.18it/s, est. speed input: 11429.01 toks/s, output: 3.82 toks/s]Processed prompts: 100%|██████████| 872/872 [06:13<00:00,  2.33it/s, est. speed input: 15311.59 toks/s, output: 4.67 toks/s]
Evaluating task: shortest
Processed prompts:   0%|          | 0/1392 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/1392 [02:19<53:42:49, 139.01s/it, est. speed input: 54.66 toks/s, output: 0.01 toks/s]Processed prompts:  26%|██▌       | 359/1392 [04:36<11:16,  1.53it/s, est. speed input: 7728.20 toks/s, output: 2.59 toks/s]Processed prompts:  52%|█████▏    | 729/1392 [06:54<05:22,  2.05it/s, est. speed input: 10308.41 toks/s, output: 3.52 toks/s]Processed prompts:  78%|███████▊  | 1080/1392 [09:14<02:19,  2.24it/s, est. speed input: 11554.50 toks/s, output: 3.90 toks/s]Processed prompts:  98%|█████████▊| 1365/1392 [09:30<00:08,  3.18it/s, est. speed input: 14933.39 toks/s, output: 4.78 toks/s]Processed prompts: 100%|██████████| 1392/1392 [09:30<00:00,  2.44it/s, est. speed input: 15344.73 toks/s, output: 4.88 toks/s]
Evaluating task: triangle
Processed prompts:   0%|          | 0/2756 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2756 [02:13<102:01:17, 133.31s/it, est. speed input: 49.66 toks/s, output: 0.02 toks/s]Processed prompts:  32%|███▏      | 888/2756 [04:26<07:56,  3.92it/s, est. speed input: 8380.56 toks/s, output: 6.66 toks/s] Processed prompts:  62%|██████▏   | 1701/2756 [06:42<03:36,  4.86it/s, est. speed input: 11051.55 toks/s, output: 8.44 toks/s]Processed prompts:  81%|████████  | 2239/2756 [08:58<01:54,  4.51it/s, est. speed input: 12301.92 toks/s, output: 8.32 toks/s]Processed prompts:  99%|█████████▊| 2715/2756 [09:11<00:06,  6.17it/s, est. speed input: 15933.97 toks/s, output: 9.85 toks/s]Processed prompts: 100%|██████████| 2756/2756 [09:11<00:00,  5.00it/s, est. speed input: 16284.16 toks/s, output: 10.00 toks/s]
Evaluating task: flow
Processed prompts:   0%|          | 0/405 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/405 [02:00<13:30:53, 120.43s/it, est. speed input: 71.79 toks/s, output: 0.02 toks/s]Processed prompts: 100%|██████████| 405/405 [02:00<00:00,  3.36it/s, est. speed input: 15575.07 toks/s, output: 6.73 toks/s]
Evaluating task: hamilton
Processed prompts:   0%|          | 0/2097 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2097 [02:14<78:18:17, 134.49s/it, est. speed input: 5.99 toks/s, output: 0.01 toks/s]Processed prompts:  38%|███▊      | 794/2097 [04:30<06:17,  3.45it/s, est. speed input: 8215.27 toks/s, output: 5.88 toks/s]Processed prompts:  64%|██████▎   | 1333/2097 [06:48<03:28,  3.66it/s, est. speed input: 10732.98 toks/s, output: 6.53 toks/s]Processed prompts:  79%|███████▊  | 1651/2097 [09:08<02:23,  3.10it/s, est. speed input: 11862.42 toks/s, output: 6.02 toks/s]Processed prompts:  92%|█████████▏| 1932/2097 [10:32<00:52,  3.17it/s, est. speed input: 13654.22 toks/s, output: 6.11 toks/s]Processed prompts: 100%|██████████| 2097/2097 [10:32<00:00,  3.32it/s, est. speed input: 15645.28 toks/s, output: 6.64 toks/s]
Evaluating task: substructure
Processed prompts:   0%|          | 0/3186 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/3186 [02:14<118:40:03, 134.13s/it, est. speed input: 26.06 toks/s, output: 0.01 toks/s]Processed prompts:  21%|██        | 675/3186 [04:31<14:19,  2.92it/s, est. speed input: 8137.64 toks/s, output: 4.98 toks/s] Processed prompts:  36%|███▋      | 1155/3186 [06:46<10:33,  3.20it/s, est. speed input: 10732.30 toks/s, output: 5.68 toks/s]Processed prompts:  50%|█████     | 1593/3186 [09:02<08:16,  3.21it/s, est. speed input: 12013.92 toks/s, output: 5.87 toks/s]Processed prompts:  65%|██████▌   | 2076/3186 [11:19<05:34,  3.32it/s, est. speed input: 12772.79 toks/s, output: 6.11 toks/s]Processed prompts:  77%|███████▋  | 2442/3186 [13:39<04:01,  3.08it/s, est. speed input: 13210.30 toks/s, output: 5.96 toks/s]Processed prompts:  86%|████████▌ | 2745/3186 [15:58<02:38,  2.79it/s, est. speed input: 13518.32 toks/s, output: 5.73 toks/s]Processed prompts:  95%|█████████▌| 3034/3186 [17:12<00:50,  3.01it/s, est. speed input: 14601.28 toks/s, output: 5.88 toks/s]Processed prompts: 100%|██████████| 3186/3186 [17:12<00:00,  3.08it/s, est. speed input: 15699.32 toks/s, output: 6.17 toks/s]
Accuracy data has been saved to 'task_accuracies.txt'.
INFO 04-23 16:41:26 multiproc_worker_utils.py:140] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=2720980)[0;0m INFO 04-23 16:41:26 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2720979)[0;0m INFO 04-23 16:41:26 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2720981)[0;0m INFO 04-23 16:41:26 multiproc_worker_utils.py:247] Worker exiting
[rank0]:[W423 16:41:47.227774464 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/home/zch/anaconda3/envs/GraphICL/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO 04-23 16:42:07 config.py:510] This model supports multiple tasks: {'generate', 'reward', 'score', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 04-23 16:42:07 config.py:1310] Defaulting to use mp for distributed inference
INFO 04-23 16:42:07 llm_engine.py:234] Initializing an LLM engine (v0.6.6) with config: model='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', speculative_config=None, tokenizer='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[1024,1016,1008,1000,992,984,976,968,960,952,944,936,928,920,912,904,896,888,880,872,864,856,848,840,832,824,816,808,800,792,784,776,768,760,752,744,736,728,720,712,704,696,688,680,672,664,656,648,640,632,624,616,608,600,592,584,576,568,560,552,544,536,528,520,512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":1024}, use_cached_outputs=False, 
WARNING 04-23 16:42:07 multiproc_worker_utils.py:312] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 04-23 16:42:07 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 04-23 16:42:08 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2722031)[0;0m INFO 04-23 16:42:08 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2722031)[0;0m INFO 04-23 16:42:08 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2722033)[0;0m INFO 04-23 16:42:08 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2722033)[0;0m INFO 04-23 16:42:08 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2722032)[0;0m INFO 04-23 16:42:08 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2722032)[0;0m INFO 04-23 16:42:08 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2722032)[0;0m INFO 04-23 16:42:09 utils.py:918] Found nccl from library libnccl.so.2
INFO 04-23 16:42:09 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2722032)[0;0m INFO 04-23 16:42:09 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 04-23 16:42:09 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2722031)[0;0m INFO 04-23 16:42:09 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2722033)[0;0m INFO 04-23 16:42:09 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2722033)[0;0m INFO 04-23 16:42:09 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2722031)[0;0m INFO 04-23 16:42:09 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2722033)[0;0m WARNING 04-23 16:42:09 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2722031)[0;0m WARNING 04-23 16:42:09 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2722032)[0;0m WARNING 04-23 16:42:09 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 04-23 16:42:09 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 04-23 16:42:09 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_7c484a5f'), local_subscribe_port=42333, remote_subscribe_port=None)
INFO 04-23 16:42:09 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2722031)[0;0m INFO 04-23 16:42:09 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2722032)[0;0m INFO 04-23 16:42:09 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2722033)[0;0m INFO 04-23 16:42:09 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.39it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.50it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.37it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.45it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.44it/s]

INFO 04-23 16:42:11 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2722031)[0;0m INFO 04-23 16:42:11 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2722033)[0;0m INFO 04-23 16:42:11 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2722032)[0;0m INFO 04-23 16:42:11 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2722033)[0;0m INFO 04-23 16:42:16 worker.py:241] Memory profiling takes 4.15 seconds
[1;36m(VllmWorkerProcess pid=2722033)[0;0m INFO 04-23 16:42:16 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2722033)[0;0m INFO 04-23 16:42:16 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.57GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.94GiB.
[1;36m(VllmWorkerProcess pid=2722032)[0;0m INFO 04-23 16:42:16 worker.py:241] Memory profiling takes 4.26 seconds
[1;36m(VllmWorkerProcess pid=2722032)[0;0m INFO 04-23 16:42:16 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2722032)[0;0m INFO 04-23 16:42:16 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
INFO 04-23 16:42:16 worker.py:241] Memory profiling takes 4.29 seconds
INFO 04-23 16:42:16 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
INFO 04-23 16:42:16 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.68GiB; PyTorch activation peak memory takes 5.74GiB; the rest of the memory reserved for KV Cache is 28.05GiB.
[1;36m(VllmWorkerProcess pid=2722031)[0;0m INFO 04-23 16:42:16 worker.py:241] Memory profiling takes 4.40 seconds
[1;36m(VllmWorkerProcess pid=2722031)[0;0m INFO 04-23 16:42:16 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2722031)[0;0m INFO 04-23 16:42:16 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
INFO 04-23 16:42:16 distributed_gpu_executor.py:57] # GPU blocks: 131317, # CPU blocks: 74898
INFO 04-23 16:42:16 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 64.12x
[1;36m(VllmWorkerProcess pid=2722031)[0;0m INFO 04-23 16:42:39 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2722033)[0;0m INFO 04-23 16:42:39 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-23 16:42:39 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2722032)[0;0m INFO 04-23 16:42:39 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/131 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|          | 1/131 [00:00<01:15,  1.72it/s]Capturing CUDA graph shapes:   2%|▏         | 2/131 [00:01<01:37,  1.32it/s]Capturing CUDA graph shapes:   2%|▏         | 3/131 [00:02<01:30,  1.41it/s]Capturing CUDA graph shapes:   3%|▎         | 4/131 [00:02<01:26,  1.47it/s]Capturing CUDA graph shapes:   4%|▍         | 5/131 [00:03<01:21,  1.54it/s]Capturing CUDA graph shapes:   5%|▍         | 6/131 [00:03<01:19,  1.58it/s]Capturing CUDA graph shapes:   5%|▌         | 7/131 [00:04<01:17,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 8/131 [00:05<01:15,  1.64it/s]Capturing CUDA graph shapes:   7%|▋         | 9/131 [00:05<01:15,  1.62it/s]Capturing CUDA graph shapes:   8%|▊         | 10/131 [00:06<01:13,  1.64it/s]Capturing CUDA graph shapes:   8%|▊         | 11/131 [00:06<01:13,  1.64it/s]Capturing CUDA graph shapes:   9%|▉         | 12/131 [00:07<01:11,  1.66it/s]Capturing CUDA graph shapes:  10%|▉         | 13/131 [00:08<01:10,  1.68it/s]Capturing CUDA graph shapes:  11%|█         | 14/131 [00:08<01:09,  1.69it/s]Capturing CUDA graph shapes:  11%|█▏        | 15/131 [00:09<01:11,  1.62it/s]Capturing CUDA graph shapes:  12%|█▏        | 16/131 [00:09<01:08,  1.69it/s]Capturing CUDA graph shapes:  13%|█▎        | 17/131 [00:10<01:06,  1.72it/s]Capturing CUDA graph shapes:  14%|█▎        | 18/131 [00:11<01:06,  1.71it/s]Capturing CUDA graph shapes:  15%|█▍        | 19/131 [00:11<01:05,  1.70it/s]Capturing CUDA graph shapes:  15%|█▌        | 20/131 [00:12<01:04,  1.72it/s]Capturing CUDA graph shapes:  16%|█▌        | 21/131 [00:12<01:06,  1.66it/s]Capturing CUDA graph shapes:  17%|█▋        | 22/131 [00:13<01:04,  1.69it/s]Capturing CUDA graph shapes:  18%|█▊        | 23/131 [00:14<01:03,  1.70it/s]Capturing CUDA graph shapes:  18%|█▊        | 24/131 [00:14<01:02,  1.70it/s]Capturing CUDA graph shapes:  19%|█▉        | 25/131 [00:15<01:02,  1.69it/s]Capturing CUDA graph shapes:  20%|█▉        | 26/131 [00:15<01:02,  1.68it/s]Capturing CUDA graph shapes:  21%|██        | 27/131 [00:16<01:00,  1.73it/s]Capturing CUDA graph shapes:  21%|██▏       | 28/131 [00:16<01:00,  1.70it/s]Capturing CUDA graph shapes:  22%|██▏       | 29/131 [00:17<01:00,  1.68it/s]Capturing CUDA graph shapes:  23%|██▎       | 30/131 [00:18<00:58,  1.72it/s]Capturing CUDA graph shapes:  24%|██▎       | 31/131 [00:18<00:58,  1.70it/s]Capturing CUDA graph shapes:  24%|██▍       | 32/131 [00:19<00:57,  1.72it/s]Capturing CUDA graph shapes:  25%|██▌       | 33/131 [00:19<00:57,  1.72it/s]Capturing CUDA graph shapes:  26%|██▌       | 34/131 [00:20<00:55,  1.73it/s]Capturing CUDA graph shapes:  27%|██▋       | 35/131 [00:21<00:55,  1.74it/s]Capturing CUDA graph shapes:  27%|██▋       | 36/131 [00:21<00:53,  1.78it/s]Capturing CUDA graph shapes:  28%|██▊       | 37/131 [00:22<00:52,  1.77it/s]Capturing CUDA graph shapes:  29%|██▉       | 38/131 [00:22<00:53,  1.74it/s]Capturing CUDA graph shapes:  30%|██▉       | 39/131 [00:23<00:52,  1.74it/s]Capturing CUDA graph shapes:  31%|███       | 40/131 [00:23<00:50,  1.79it/s]Capturing CUDA graph shapes:  31%|███▏      | 41/131 [00:24<00:52,  1.71it/s]Capturing CUDA graph shapes:  32%|███▏      | 42/131 [00:25<00:50,  1.77it/s]Capturing CUDA graph shapes:  33%|███▎      | 43/131 [00:25<00:49,  1.77it/s]Capturing CUDA graph shapes:  34%|███▎      | 44/131 [00:26<00:49,  1.76it/s]Capturing CUDA graph shapes:  34%|███▍      | 45/131 [00:26<00:49,  1.74it/s]Capturing CUDA graph shapes:  35%|███▌      | 46/131 [00:27<00:48,  1.76it/s]Capturing CUDA graph shapes:  36%|███▌      | 47/131 [00:27<00:47,  1.76it/s]Capturing CUDA graph shapes:  37%|███▋      | 48/131 [00:28<00:46,  1.80it/s]Capturing CUDA graph shapes:  37%|███▋      | 49/131 [00:29<00:47,  1.72it/s]Capturing CUDA graph shapes:  38%|███▊      | 50/131 [00:29<00:45,  1.79it/s]Capturing CUDA graph shapes:  39%|███▉      | 51/131 [00:30<00:44,  1.81it/s]Capturing CUDA graph shapes:  40%|███▉      | 52/131 [00:30<00:42,  1.84it/s]Capturing CUDA graph shapes:  40%|████      | 53/131 [00:31<00:43,  1.81it/s]Capturing CUDA graph shapes:  41%|████      | 54/131 [00:31<00:43,  1.76it/s]Capturing CUDA graph shapes:  42%|████▏     | 55/131 [00:32<00:43,  1.76it/s]Capturing CUDA graph shapes:  43%|████▎     | 56/131 [00:32<00:41,  1.79it/s]Capturing CUDA graph shapes:  44%|████▎     | 57/131 [00:33<00:41,  1.78it/s]Capturing CUDA graph shapes:  44%|████▍     | 58/131 [00:33<00:40,  1.82it/s]Capturing CUDA graph shapes:  45%|████▌     | 59/131 [00:34<00:41,  1.76it/s]Capturing CUDA graph shapes:  46%|████▌     | 60/131 [00:35<00:39,  1.80it/s]Capturing CUDA graph shapes:  47%|████▋     | 61/131 [00:35<00:38,  1.80it/s]Capturing CUDA graph shapes:  47%|████▋     | 62/131 [00:36<00:38,  1.81it/s]Capturing CUDA graph shapes:  48%|████▊     | 63/131 [00:36<00:36,  1.84it/s]Capturing CUDA graph shapes:  49%|████▉     | 64/131 [00:37<00:36,  1.83it/s]Capturing CUDA graph shapes:  50%|████▉     | 65/131 [00:37<00:36,  1.83it/s]Capturing CUDA graph shapes:  50%|█████     | 66/131 [00:38<00:34,  1.86it/s]Capturing CUDA graph shapes:  51%|█████     | 67/131 [00:38<00:34,  1.86it/s]Capturing CUDA graph shapes:  52%|█████▏    | 68/131 [00:39<00:33,  1.85it/s]Capturing CUDA graph shapes:  53%|█████▎    | 69/131 [00:40<00:34,  1.81it/s]Capturing CUDA graph shapes:  53%|█████▎    | 70/131 [00:40<00:33,  1.84it/s]Capturing CUDA graph shapes:  54%|█████▍    | 71/131 [00:41<00:32,  1.84it/s]Capturing CUDA graph shapes:  55%|█████▍    | 72/131 [00:41<00:32,  1.79it/s]Capturing CUDA graph shapes:  56%|█████▌    | 73/131 [00:42<00:31,  1.86it/s]Capturing CUDA graph shapes:  56%|█████▋    | 74/131 [00:42<00:30,  1.86it/s]Capturing CUDA graph shapes:  57%|█████▋    | 75/131 [00:43<00:30,  1.84it/s]Capturing CUDA graph shapes:  58%|█████▊    | 76/131 [00:43<00:29,  1.86it/s]Capturing CUDA graph shapes:  59%|█████▉    | 77/131 [00:44<00:28,  1.88it/s]Capturing CUDA graph shapes:  60%|█████▉    | 78/131 [00:44<00:28,  1.86it/s]Capturing CUDA graph shapes:  60%|██████    | 79/131 [00:45<00:28,  1.83it/s]Capturing CUDA graph shapes:  61%|██████    | 80/131 [00:45<00:27,  1.86it/s]Capturing CUDA graph shapes:  62%|██████▏   | 81/131 [00:46<00:27,  1.85it/s]Capturing CUDA graph shapes:  63%|██████▎   | 82/131 [00:46<00:26,  1.87it/s]Capturing CUDA graph shapes:  63%|██████▎   | 83/131 [00:47<00:25,  1.87it/s]Capturing CUDA graph shapes:  64%|██████▍   | 84/131 [00:48<00:25,  1.85it/s]Capturing CUDA graph shapes:  65%|██████▍   | 85/131 [00:48<00:24,  1.88it/s]Capturing CUDA graph shapes:  66%|██████▌   | 86/131 [00:49<00:23,  1.92it/s]Capturing CUDA graph shapes:  66%|██████▋   | 87/131 [00:49<00:23,  1.88it/s]Capturing CUDA graph shapes:  67%|██████▋   | 88/131 [00:50<00:21,  1.96it/s]Capturing CUDA graph shapes:  68%|██████▊   | 89/131 [00:50<00:21,  1.96it/s]Capturing CUDA graph shapes:  69%|██████▊   | 90/131 [00:51<00:21,  1.94it/s]Capturing CUDA graph shapes:  69%|██████▉   | 91/131 [00:51<00:20,  2.00it/s]Capturing CUDA graph shapes:  70%|███████   | 92/131 [00:52<00:19,  2.03it/s]Capturing CUDA graph shapes:  71%|███████   | 93/131 [00:52<00:19,  1.95it/s]Capturing CUDA graph shapes:  72%|███████▏  | 94/131 [00:53<00:18,  2.00it/s]Capturing CUDA graph shapes:  73%|███████▎  | 95/131 [00:53<00:18,  1.95it/s]Capturing CUDA graph shapes:  73%|███████▎  | 96/131 [00:54<00:18,  1.88it/s]Capturing CUDA graph shapes:  74%|███████▍  | 97/131 [00:54<00:17,  1.95it/s]Capturing CUDA graph shapes:  75%|███████▍  | 98/131 [00:55<00:17,  1.92it/s]Capturing CUDA graph shapes:  76%|███████▌  | 99/131 [00:55<00:15,  2.00it/s]Capturing CUDA graph shapes:  76%|███████▋  | 100/131 [00:56<00:15,  2.02it/s]Capturing CUDA graph shapes:  77%|███████▋  | 101/131 [00:56<00:14,  2.03it/s]Capturing CUDA graph shapes:  78%|███████▊  | 102/131 [00:57<00:14,  1.99it/s]Capturing CUDA graph shapes:  79%|███████▊  | 103/131 [00:57<00:13,  2.03it/s]Capturing CUDA graph shapes:  79%|███████▉  | 104/131 [00:58<00:13,  2.05it/s]Capturing CUDA graph shapes:  80%|████████  | 105/131 [00:58<00:12,  2.06it/s]Capturing CUDA graph shapes:  81%|████████  | 106/131 [00:59<00:12,  2.05it/s]Capturing CUDA graph shapes:  82%|████████▏ | 107/131 [00:59<00:11,  2.03it/s]Capturing CUDA graph shapes:  82%|████████▏ | 108/131 [01:00<00:11,  2.06it/s]Capturing CUDA graph shapes:  83%|████████▎ | 109/131 [01:00<00:10,  2.14it/s]Capturing CUDA graph shapes:  84%|████████▍ | 110/131 [01:00<00:09,  2.13it/s]Capturing CUDA graph shapes:  85%|████████▍ | 111/131 [01:01<00:09,  2.08it/s]Capturing CUDA graph shapes:  85%|████████▌ | 112/131 [01:01<00:09,  2.07it/s]Capturing CUDA graph shapes:  86%|████████▋ | 113/131 [01:02<00:08,  2.08it/s]Capturing CUDA graph shapes:  87%|████████▋ | 114/131 [01:02<00:08,  2.05it/s]Capturing CUDA graph shapes:  88%|████████▊ | 115/131 [01:03<00:07,  2.09it/s]Capturing CUDA graph shapes:  89%|████████▊ | 116/131 [01:03<00:07,  2.10it/s]Capturing CUDA graph shapes:  89%|████████▉ | 117/131 [01:04<00:06,  2.09it/s]Capturing CUDA graph shapes:  90%|█████████ | 118/131 [01:04<00:06,  2.10it/s]Capturing CUDA graph shapes:  91%|█████████ | 119/131 [01:05<00:05,  2.05it/s]Capturing CUDA graph shapes:  92%|█████████▏| 120/131 [01:05<00:05,  2.04it/s]Capturing CUDA graph shapes:  92%|█████████▏| 121/131 [01:06<00:04,  2.03it/s]Capturing CUDA graph shapes:  93%|█████████▎| 122/131 [01:06<00:04,  2.06it/s]Capturing CUDA graph shapes:  94%|█████████▍| 123/131 [01:07<00:03,  2.07it/s]Capturing CUDA graph shapes:  95%|█████████▍| 124/131 [01:07<00:03,  2.10it/s]Capturing CUDA graph shapes:  95%|█████████▌| 125/131 [01:08<00:02,  2.11it/s]Capturing CUDA graph shapes:  96%|█████████▌| 126/131 [01:08<00:02,  2.05it/s]Capturing CUDA graph shapes:  97%|█████████▋| 127/131 [01:09<00:01,  2.09it/s]Capturing CUDA graph shapes:  98%|█████████▊| 128/131 [01:09<00:01,  2.06it/s]Capturing CUDA graph shapes:  98%|█████████▊| 129/131 [01:10<00:00,  2.12it/s]Capturing CUDA graph shapes:  99%|█████████▉| 130/131 [01:10<00:00,  2.12it/s][1;36m(VllmWorkerProcess pid=2722033)[0;0m INFO 04-23 16:43:50 model_runner.py:1535] Graph capturing finished in 71 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2722032)[0;0m INFO 04-23 16:43:50 model_runner.py:1535] Graph capturing finished in 71 secs, took 3.72 GiB
Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:11<00:00,  1.76it/s]Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:11<00:00,  1.83it/s]
INFO 04-23 16:43:50 model_runner.py:1535] Graph capturing finished in 71 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2722031)[0;0m INFO 04-23 16:43:51 model_runner.py:1535] Graph capturing finished in 71 secs, took 3.72 GiB
INFO 04-23 16:43:51 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 99.15 seconds
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 18125 examples [00:02, 8874.89 examples/s]Generating train split: 18125 examples [00:02, 8834.57 examples/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclExamplesTestResults.py:128: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: cycle
Processed prompts:   0%|          | 0/2717 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2717 [01:53<85:18:18, 113.07s/it, est. speed input: 12.42 toks/s, output: 0.02 toks/s]Processed prompts:  38%|███▊      | 1025/2717 [04:06<05:50,  4.83it/s, est. speed input: 7942.57 toks/s, output: 8.33 toks/s]Processed prompts:  72%|███████▏  | 1957/2717 [06:22<02:11,  5.80it/s, est. speed input: 10939.83 toks/s, output: 10.24 toks/s]Processed prompts:  95%|█████████▌| 2593/2717 [06:50<00:15,  7.86it/s, est. speed input: 15468.55 toks/s, output: 12.62 toks/s]Processed prompts: 100%|██████████| 2717/2717 [06:50<00:00,  6.61it/s, est. speed input: 16583.33 toks/s, output: 13.23 toks/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclExamplesTestResults.py:128: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: connectivity
Processed prompts:   0%|          | 0/2687 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2687 [01:09<51:50:07, 69.47s/it, est. speed input: 22.77 toks/s, output: 0.03 toks/s]Processed prompts:  38%|███▊      | 1025/2687 [02:46<03:56,  7.02it/s, est. speed input: 7569.94 toks/s, output: 12.30 toks/s]Processed prompts:  76%|███████▌  | 2048/2687 [03:03<01:30,  7.02it/s, est. speed input: 17677.22 toks/s, output: 24.57 toks/s]Processed prompts:  76%|███████▋  | 2049/2687 [04:40<01:19,  8.07it/s, est. speed input: 10516.33 toks/s, output: 14.61 toks/s]Processed prompts: 100%|██████████| 2687/2687 [04:40<00:00,  9.58it/s, est. speed input: 17036.89 toks/s, output: 19.16 toks/s]
Evaluating task: bipartite
Processed prompts:   0%|          | 0/2013 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2013 [01:41<56:51:22, 101.73s/it, est. speed input: 8.81 toks/s, output: 0.02 toks/s]Processed prompts:  51%|█████     | 1025/2013 [03:55<03:17,  5.00it/s, est. speed input: 7465.70 toks/s, output: 8.71 toks/s]Processed prompts:  94%|█████████▍| 1893/2013 [04:21<00:12,  9.34it/s, est. speed input: 15172.62 toks/s, output: 14.47 toks/s]Processed prompts: 100%|██████████| 2013/2013 [04:21<00:00,  7.69it/s, est. speed input: 16771.29 toks/s, output: 15.38 toks/s]
Evaluating task: topology
Processed prompts:   0%|          | 0/872 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/872 [02:18<33:25:36, 138.16s/it, est. speed input: 20.69 toks/s, output: 0.01 toks/s]Processed prompts:  64%|██████▍   | 561/872 [04:37<02:10,  2.38it/s, est. speed input: 7832.07 toks/s, output: 4.04 toks/s]Processed prompts:  99%|█████████▉| 866/872 [04:40<00:01,  4.14it/s, est. speed input: 15277.20 toks/s, output: 6.17 toks/s]Processed prompts: 100%|██████████| 872/872 [04:40<00:00,  3.10it/s, est. speed input: 15448.53 toks/s, output: 6.21 toks/s]
Evaluating task: shortest
Processed prompts:   0%|          | 0/1392 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/1392 [02:16<52:35:29, 136.11s/it, est. speed input: 71.57 toks/s, output: 0.01 toks/s]Processed prompts:  40%|███▉      | 552/1392 [04:33<05:54,  2.37it/s, est. speed input: 7913.49 toks/s, output: 4.03 toks/s]Processed prompts:  77%|███████▋  | 1074/1392 [06:39<01:41,  3.14it/s, est. speed input: 10817.34 toks/s, output: 5.38 toks/s]Processed prompts: 100%|██████████| 1392/1392 [06:39<00:00,  3.48it/s, est. speed input: 15644.72 toks/s, output: 6.97 toks/s]
Evaluating task: triangle
Processed prompts:   0%|          | 0/2756 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2756 [01:13<56:06:28, 73.32s/it, est. speed input: 120.93 toks/s, output: 0.03 toks/s]Processed prompts:  37%|███▋      | 1025/2756 [02:48<04:06,  7.01it/s, est. speed input: 7922.36 toks/s, output: 12.19 toks/s]Processed prompts:  74%|███████▍  | 2048/2756 [03:03<01:41,  7.01it/s, est. speed input: 17817.56 toks/s, output: 24.35 toks/s]Processed prompts:  74%|███████▍  | 2049/2756 [04:44<01:28,  7.95it/s, est. speed input: 10526.95 toks/s, output: 14.38 toks/s]Processed prompts: 100%|██████████| 2756/2756 [04:44<00:00,  9.67it/s, est. speed input: 17196.72 toks/s, output: 19.34 toks/s]
Evaluating task: flow
Processed prompts:   0%|          | 0/405 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/405 [01:22<9:18:23, 82.93s/it, est. speed input: 15.78 toks/s, output: 0.02 toks/s]Processed prompts: 100%|██████████| 405/405 [01:22<00:00,  4.88it/s, est. speed input: 15923.14 toks/s, output: 9.77 toks/s]
Evaluating task: hamilton
Processed prompts:   0%|          | 0/2097 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2097 [02:07<74:20:41, 127.69s/it, est. speed input: 6.08 toks/s, output: 0.02 toks/s]Processed prompts:  49%|████▉     | 1025/2097 [04:24<03:56,  4.53it/s, est. speed input: 8209.61 toks/s, output: 7.75 toks/s]Processed prompts:  77%|███████▋  | 1610/2097 [06:40<01:50,  4.42it/s, est. speed input: 10825.55 toks/s, output: 8.03 toks/s]Processed prompts:  98%|█████████▊| 2053/2097 [06:54<00:07,  6.16it/s, est. speed input: 15643.09 toks/s, output: 9.90 toks/s]Processed prompts: 100%|██████████| 2097/2097 [06:54<00:00,  5.06it/s, est. speed input: 16148.84 toks/s, output: 10.12 toks/s]
Evaluating task: substructure
Processed prompts:   0%|          | 0/3186 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/3186 [02:13<118:08:55, 133.54s/it, est. speed input: 9.47 toks/s, output: 0.01 toks/s]Processed prompts:  28%|██▊       | 908/3186 [04:29<09:36,  3.95it/s, est. speed input: 8319.02 toks/s, output: 6.73 toks/s]Processed prompts:  49%|████▉     | 1570/3186 [06:44<06:08,  4.39it/s, est. speed input: 10965.29 toks/s, output: 7.76 toks/s]Processed prompts:  71%|███████   | 2250/3186 [09:00<03:22,  4.63it/s, est. speed input: 12267.01 toks/s, output: 8.32 toks/s]Processed prompts:  84%|████████▍ | 2685/3186 [11:19<02:02,  4.08it/s, est. speed input: 12929.87 toks/s, output: 7.90 toks/s]Processed prompts:  96%|█████████▌| 3051/3186 [12:07<00:29,  4.63it/s, est. speed input: 15018.32 toks/s, output: 8.39 toks/s]Processed prompts: 100%|██████████| 3186/3186 [12:07<00:00,  4.38it/s, est. speed input: 16036.68 toks/s, output: 8.76 toks/s]
Accuracy data has been saved to 'task_accuracies.txt'.
INFO 04-23 17:37:54 multiproc_worker_utils.py:140] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=2722031)[0;0m INFO 04-23 17:37:54 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2722033)[0;0m INFO 04-23 17:37:54 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2722032)[0;0m INFO 04-23 17:37:54 multiproc_worker_utils.py:247] Worker exiting
[rank0]:[W423 17:38:15.723479425 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/home/zch/anaconda3/envs/GraphICL/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO 04-23 17:38:35 config.py:510] This model supports multiple tasks: {'reward', 'score', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 04-23 17:38:35 config.py:1310] Defaulting to use mp for distributed inference
INFO 04-23 17:38:35 llm_engine.py:234] Initializing an LLM engine (v0.6.6) with config: model='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', speculative_config=None, tokenizer='/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[1024,1016,1008,1000,992,984,976,968,960,952,944,936,928,920,912,904,896,888,880,872,864,856,848,840,832,824,816,808,800,792,784,776,768,760,752,744,736,728,720,712,704,696,688,680,672,664,656,648,640,632,624,616,608,600,592,584,576,568,560,552,544,536,528,520,512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":1024}, use_cached_outputs=False, 
WARNING 04-23 17:38:35 multiproc_worker_utils.py:312] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 04-23 17:38:35 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 04-23 17:38:37 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2722952)[0;0m INFO 04-23 17:38:37 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2722952)[0;0m INFO 04-23 17:38:37 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2722953)[0;0m INFO 04-23 17:38:37 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2722953)[0;0m INFO 04-23 17:38:37 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2722951)[0;0m INFO 04-23 17:38:37 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2722951)[0;0m INFO 04-23 17:38:37 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
INFO 04-23 17:38:38 utils.py:918] Found nccl from library libnccl.so.2
INFO 04-23 17:38:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2722951)[0;0m INFO 04-23 17:38:38 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2722952)[0;0m INFO 04-23 17:38:38 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2722951)[0;0m INFO 04-23 17:38:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2722952)[0;0m INFO 04-23 17:38:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2722953)[0;0m INFO 04-23 17:38:38 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2722953)[0;0m INFO 04-23 17:38:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2722952)[0;0m WARNING 04-23 17:38:38 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 04-23 17:38:38 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2722951)[0;0m WARNING 04-23 17:38:38 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2722953)[0;0m WARNING 04-23 17:38:38 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 04-23 17:38:38 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_487636fd'), local_subscribe_port=46673, remote_subscribe_port=None)
INFO 04-23 17:38:38 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2722952)[0;0m INFO 04-23 17:38:38 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2722951)[0;0m INFO 04-23 17:38:38 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
[1;36m(VllmWorkerProcess pid=2722953)[0;0m INFO 04-23 17:38:38 model_runner.py:1094] Starting to load model /home/zch/Code/model/Qwen2.5-7B/Qwen/Qwen2___5-7B-Instruct...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.50it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.59it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.43it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.48it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.48it/s]

INFO 04-23 17:38:40 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2722951)[0;0m INFO 04-23 17:38:40 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2722952)[0;0m INFO 04-23 17:38:40 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2722953)[0;0m INFO 04-23 17:38:41 model_runner.py:1099] Loading model weights took 3.5546 GB
[1;36m(VllmWorkerProcess pid=2722952)[0;0m INFO 04-23 17:38:45 worker.py:241] Memory profiling takes 4.16 seconds
[1;36m(VllmWorkerProcess pid=2722952)[0;0m INFO 04-23 17:38:45 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2722952)[0;0m INFO 04-23 17:38:45 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
[1;36m(VllmWorkerProcess pid=2722951)[0;0m INFO 04-23 17:38:45 worker.py:241] Memory profiling takes 4.18 seconds
[1;36m(VllmWorkerProcess pid=2722951)[0;0m INFO 04-23 17:38:45 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2722951)[0;0m INFO 04-23 17:38:45 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.64GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.86GiB.
[1;36m(VllmWorkerProcess pid=2722953)[0;0m INFO 04-23 17:38:45 worker.py:241] Memory profiling takes 4.29 seconds
[1;36m(VllmWorkerProcess pid=2722953)[0;0m INFO 04-23 17:38:45 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
[1;36m(VllmWorkerProcess pid=2722953)[0;0m INFO 04-23 17:38:45 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.57GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 31.94GiB.
INFO 04-23 17:38:45 worker.py:241] Memory profiling takes 4.30 seconds
INFO 04-23 17:38:45 worker.py:241] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 38.03GiB
INFO 04-23 17:38:45 worker.py:241] model weights take 3.55GiB; non_torch_memory takes 0.68GiB; PyTorch activation peak memory takes 5.74GiB; the rest of the memory reserved for KV Cache is 28.05GiB.
INFO 04-23 17:38:45 distributed_gpu_executor.py:57] # GPU blocks: 131317, # CPU blocks: 74898
INFO 04-23 17:38:45 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 64.12x
[1;36m(VllmWorkerProcess pid=2722953)[0;0m INFO 04-23 17:39:08 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2722952)[0;0m INFO 04-23 17:39:08 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2722951)[0;0m INFO 04-23 17:39:08 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-23 17:39:08 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/131 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|          | 1/131 [00:00<01:14,  1.74it/s]Capturing CUDA graph shapes:   2%|▏         | 2/131 [00:01<01:18,  1.65it/s]Capturing CUDA graph shapes:   2%|▏         | 3/131 [00:01<01:19,  1.61it/s]Capturing CUDA graph shapes:   3%|▎         | 4/131 [00:02<01:18,  1.61it/s]Capturing CUDA graph shapes:   4%|▍         | 5/131 [00:03<01:18,  1.61it/s]Capturing CUDA graph shapes:   5%|▍         | 6/131 [00:03<01:18,  1.59it/s]Capturing CUDA graph shapes:   5%|▌         | 7/131 [00:04<01:17,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 8/131 [00:04<01:17,  1.59it/s]Capturing CUDA graph shapes:   7%|▋         | 9/131 [00:05<01:17,  1.58it/s]Capturing CUDA graph shapes:   8%|▊         | 10/131 [00:06<01:16,  1.59it/s]Capturing CUDA graph shapes:   8%|▊         | 11/131 [00:06<01:15,  1.58it/s]Capturing CUDA graph shapes:   9%|▉         | 12/131 [00:07<01:14,  1.59it/s]Capturing CUDA graph shapes:  10%|▉         | 13/131 [00:08<01:13,  1.60it/s]Capturing CUDA graph shapes:  11%|█         | 14/131 [00:08<01:13,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 15/131 [00:09<01:13,  1.59it/s]Capturing CUDA graph shapes:  12%|█▏        | 16/131 [00:10<01:12,  1.60it/s]Capturing CUDA graph shapes:  13%|█▎        | 17/131 [00:10<01:11,  1.59it/s]Capturing CUDA graph shapes:  14%|█▎        | 18/131 [00:11<01:11,  1.59it/s]Capturing CUDA graph shapes:  15%|█▍        | 19/131 [00:11<01:10,  1.59it/s]Capturing CUDA graph shapes:  15%|█▌        | 20/131 [00:12<01:09,  1.59it/s]Capturing CUDA graph shapes:  16%|█▌        | 21/131 [00:13<01:09,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 22/131 [00:13<01:07,  1.61it/s]Capturing CUDA graph shapes:  18%|█▊        | 23/131 [00:14<01:07,  1.60it/s]Capturing CUDA graph shapes:  18%|█▊        | 24/131 [00:15<01:06,  1.60it/s]Capturing CUDA graph shapes:  19%|█▉        | 25/131 [00:15<01:06,  1.60it/s]Capturing CUDA graph shapes:  20%|█▉        | 26/131 [00:16<01:05,  1.61it/s]Capturing CUDA graph shapes:  21%|██        | 27/131 [00:16<01:04,  1.61it/s]Capturing CUDA graph shapes:  21%|██▏       | 28/131 [00:17<01:03,  1.62it/s]Capturing CUDA graph shapes:  22%|██▏       | 29/131 [00:18<01:04,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 30/131 [00:18<01:02,  1.62it/s]Capturing CUDA graph shapes:  24%|██▎       | 31/131 [00:19<01:01,  1.62it/s]Capturing CUDA graph shapes:  24%|██▍       | 32/131 [00:19<01:01,  1.62it/s]Capturing CUDA graph shapes:  25%|██▌       | 33/131 [00:20<00:59,  1.64it/s]Capturing CUDA graph shapes:  26%|██▌       | 34/131 [00:21<00:59,  1.64it/s]Capturing CUDA graph shapes:  27%|██▋       | 35/131 [00:21<00:57,  1.67it/s]Capturing CUDA graph shapes:  27%|██▋       | 36/131 [00:22<00:57,  1.66it/s]Capturing CUDA graph shapes:  28%|██▊       | 37/131 [00:22<00:56,  1.66it/s]Capturing CUDA graph shapes:  29%|██▉       | 38/131 [00:23<00:55,  1.67it/s]Capturing CUDA graph shapes:  30%|██▉       | 39/131 [00:24<00:55,  1.66it/s]Capturing CUDA graph shapes:  31%|███       | 40/131 [00:24<00:54,  1.66it/s]Capturing CUDA graph shapes:  31%|███▏      | 41/131 [00:25<00:54,  1.66it/s]Capturing CUDA graph shapes:  32%|███▏      | 42/131 [00:25<00:53,  1.66it/s]Capturing CUDA graph shapes:  33%|███▎      | 43/131 [00:26<00:52,  1.66it/s]Capturing CUDA graph shapes:  34%|███▎      | 44/131 [00:27<00:52,  1.66it/s]Capturing CUDA graph shapes:  34%|███▍      | 45/131 [00:27<00:51,  1.67it/s]Capturing CUDA graph shapes:  35%|███▌      | 46/131 [00:28<00:51,  1.66it/s]Capturing CUDA graph shapes:  36%|███▌      | 47/131 [00:28<00:49,  1.69it/s]Capturing CUDA graph shapes:  37%|███▋      | 48/131 [00:29<00:49,  1.69it/s]Capturing CUDA graph shapes:  37%|███▋      | 49/131 [00:30<00:48,  1.69it/s]Capturing CUDA graph shapes:  38%|███▊      | 50/131 [00:30<00:47,  1.69it/s]Capturing CUDA graph shapes:  39%|███▉      | 51/131 [00:31<00:47,  1.68it/s]Capturing CUDA graph shapes:  40%|███▉      | 52/131 [00:31<00:46,  1.69it/s]Capturing CUDA graph shapes:  40%|████      | 53/131 [00:32<00:46,  1.68it/s]Capturing CUDA graph shapes:  41%|████      | 54/131 [00:33<00:45,  1.70it/s]Capturing CUDA graph shapes:  42%|████▏     | 55/131 [00:33<00:45,  1.68it/s]Capturing CUDA graph shapes:  43%|████▎     | 56/131 [00:34<00:44,  1.70it/s]Capturing CUDA graph shapes:  44%|████▎     | 57/131 [00:34<00:43,  1.69it/s]Capturing CUDA graph shapes:  44%|████▍     | 58/131 [00:35<00:43,  1.68it/s]Capturing CUDA graph shapes:  45%|████▌     | 59/131 [00:36<00:43,  1.67it/s]Capturing CUDA graph shapes:  46%|████▌     | 60/131 [00:36<00:41,  1.70it/s]Capturing CUDA graph shapes:  47%|████▋     | 61/131 [00:37<00:41,  1.70it/s]Capturing CUDA graph shapes:  47%|████▋     | 62/131 [00:37<00:40,  1.69it/s]Capturing CUDA graph shapes:  48%|████▊     | 63/131 [00:38<00:39,  1.70it/s]Capturing CUDA graph shapes:  49%|████▉     | 64/131 [00:38<00:39,  1.71it/s]Capturing CUDA graph shapes:  50%|████▉     | 65/131 [00:39<00:38,  1.72it/s]Capturing CUDA graph shapes:  50%|█████     | 66/131 [00:40<00:37,  1.71it/s]Capturing CUDA graph shapes:  51%|█████     | 67/131 [00:40<00:36,  1.73it/s]Capturing CUDA graph shapes:  52%|█████▏    | 68/131 [00:41<00:36,  1.72it/s]Capturing CUDA graph shapes:  53%|█████▎    | 69/131 [00:41<00:35,  1.74it/s]Capturing CUDA graph shapes:  53%|█████▎    | 70/131 [00:42<00:35,  1.74it/s]Capturing CUDA graph shapes:  54%|█████▍    | 71/131 [00:43<00:34,  1.73it/s]Capturing CUDA graph shapes:  55%|█████▍    | 72/131 [00:43<00:34,  1.73it/s]Capturing CUDA graph shapes:  56%|█████▌    | 73/131 [00:44<00:33,  1.75it/s]Capturing CUDA graph shapes:  56%|█████▋    | 74/131 [00:44<00:32,  1.75it/s]Capturing CUDA graph shapes:  57%|█████▋    | 75/131 [00:45<00:32,  1.74it/s]Capturing CUDA graph shapes:  58%|█████▊    | 76/131 [00:45<00:31,  1.74it/s]Capturing CUDA graph shapes:  59%|█████▉    | 77/131 [00:46<00:31,  1.74it/s]Capturing CUDA graph shapes:  60%|█████▉    | 78/131 [00:47<00:30,  1.76it/s]Capturing CUDA graph shapes:  60%|██████    | 79/131 [00:47<00:29,  1.74it/s]Capturing CUDA graph shapes:  61%|██████    | 80/131 [00:48<00:29,  1.75it/s]Capturing CUDA graph shapes:  62%|██████▏   | 81/131 [00:48<00:28,  1.75it/s]Capturing CUDA graph shapes:  63%|██████▎   | 82/131 [00:49<00:27,  1.78it/s]Capturing CUDA graph shapes:  63%|██████▎   | 83/131 [00:49<00:26,  1.78it/s]Capturing CUDA graph shapes:  64%|██████▍   | 84/131 [00:50<00:26,  1.77it/s]Capturing CUDA graph shapes:  65%|██████▍   | 85/131 [00:50<00:25,  1.79it/s]Capturing CUDA graph shapes:  66%|██████▌   | 86/131 [00:51<00:25,  1.78it/s]Capturing CUDA graph shapes:  66%|██████▋   | 87/131 [00:52<00:24,  1.79it/s]Capturing CUDA graph shapes:  67%|██████▋   | 88/131 [00:52<00:24,  1.79it/s]Capturing CUDA graph shapes:  68%|██████▊   | 89/131 [00:53<00:23,  1.79it/s]Capturing CUDA graph shapes:  69%|██████▊   | 90/131 [00:53<00:22,  1.79it/s]Capturing CUDA graph shapes:  69%|██████▉   | 91/131 [00:54<00:22,  1.81it/s]Capturing CUDA graph shapes:  70%|███████   | 92/131 [00:54<00:21,  1.82it/s]Capturing CUDA graph shapes:  71%|███████   | 93/131 [00:55<00:21,  1.81it/s]Capturing CUDA graph shapes:  72%|███████▏  | 94/131 [00:55<00:20,  1.81it/s]Capturing CUDA graph shapes:  73%|███████▎  | 95/131 [00:56<00:20,  1.80it/s]Capturing CUDA graph shapes:  73%|███████▎  | 96/131 [00:57<00:19,  1.83it/s]Capturing CUDA graph shapes:  74%|███████▍  | 97/131 [00:57<00:18,  1.82it/s]Capturing CUDA graph shapes:  75%|███████▍  | 98/131 [00:58<00:18,  1.82it/s]Capturing CUDA graph shapes:  76%|███████▌  | 99/131 [00:58<00:17,  1.82it/s]Capturing CUDA graph shapes:  76%|███████▋  | 100/131 [00:59<00:17,  1.81it/s]Capturing CUDA graph shapes:  77%|███████▋  | 101/131 [00:59<00:16,  1.83it/s]Capturing CUDA graph shapes:  78%|███████▊  | 102/131 [01:00<00:15,  1.83it/s]Capturing CUDA graph shapes:  79%|███████▊  | 103/131 [01:00<00:15,  1.83it/s]Capturing CUDA graph shapes:  79%|███████▉  | 104/131 [01:01<00:14,  1.84it/s]Capturing CUDA graph shapes:  80%|████████  | 105/131 [01:01<00:14,  1.84it/s]Capturing CUDA graph shapes:  81%|████████  | 106/131 [01:02<00:13,  1.84it/s]Capturing CUDA graph shapes:  82%|████████▏ | 107/131 [01:03<00:12,  1.85it/s]Capturing CUDA graph shapes:  82%|████████▏ | 108/131 [01:03<00:12,  1.84it/s]Capturing CUDA graph shapes:  83%|████████▎ | 109/131 [01:04<00:11,  1.84it/s]Capturing CUDA graph shapes:  84%|████████▍ | 110/131 [01:04<00:11,  1.84it/s]Capturing CUDA graph shapes:  85%|████████▍ | 111/131 [01:05<00:10,  1.84it/s]Capturing CUDA graph shapes:  85%|████████▌ | 112/131 [01:05<00:10,  1.85it/s]Capturing CUDA graph shapes:  86%|████████▋ | 113/131 [01:06<00:09,  1.83it/s]Capturing CUDA graph shapes:  87%|████████▋ | 114/131 [01:06<00:09,  1.83it/s]Capturing CUDA graph shapes:  88%|████████▊ | 115/131 [01:07<00:08,  1.86it/s]Capturing CUDA graph shapes:  89%|████████▊ | 116/131 [01:07<00:08,  1.86it/s]Capturing CUDA graph shapes:  89%|████████▉ | 117/131 [01:08<00:07,  1.84it/s]Capturing CUDA graph shapes:  90%|█████████ | 118/131 [01:08<00:07,  1.85it/s]Capturing CUDA graph shapes:  91%|█████████ | 119/131 [01:09<00:06,  1.86it/s]Capturing CUDA graph shapes:  92%|█████████▏| 120/131 [01:10<00:05,  1.85it/s]Capturing CUDA graph shapes:  92%|█████████▏| 121/131 [01:10<00:05,  1.83it/s]Capturing CUDA graph shapes:  93%|█████████▎| 122/131 [01:11<00:04,  1.82it/s]Capturing CUDA graph shapes:  94%|█████████▍| 123/131 [01:11<00:04,  1.83it/s]Capturing CUDA graph shapes:  95%|█████████▍| 124/131 [01:12<00:03,  1.85it/s]Capturing CUDA graph shapes:  95%|█████████▌| 125/131 [01:12<00:03,  1.84it/s]Capturing CUDA graph shapes:  96%|█████████▌| 126/131 [01:13<00:02,  1.87it/s]Capturing CUDA graph shapes:  97%|█████████▋| 127/131 [01:13<00:02,  1.88it/s]Capturing CUDA graph shapes:  98%|█████████▊| 128/131 [01:14<00:01,  1.88it/s]Capturing CUDA graph shapes:  98%|█████████▊| 129/131 [01:14<00:01,  1.87it/s]Capturing CUDA graph shapes:  99%|█████████▉| 130/131 [01:15<00:00,  1.88it/s][1;36m(VllmWorkerProcess pid=2722952)[0;0m INFO 04-23 17:40:24 model_runner.py:1535] Graph capturing finished in 76 secs, took 3.72 GiB
Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:16<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 131/131 [01:16<00:00,  1.72it/s]
INFO 04-23 17:40:24 model_runner.py:1535] Graph capturing finished in 76 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2722951)[0;0m INFO 04-23 17:40:24 model_runner.py:1535] Graph capturing finished in 76 secs, took 3.72 GiB
[1;36m(VllmWorkerProcess pid=2722953)[0;0m INFO 04-23 17:40:24 model_runner.py:1535] Graph capturing finished in 76 secs, took 3.72 GiB
INFO 04-23 17:40:24 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 103.44 seconds
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 18125 examples [00:01, 10385.77 examples/s]Generating train split: 18125 examples [00:01, 10333.60 examples/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclExamplesTestResults.py:128: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: cycle
Processed prompts:   0%|          | 0/2717 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2717 [02:11<99:12:16, 131.49s/it, est. speed input: 9.41 toks/s, output: 0.02 toks/s]Processed prompts:  32%|███▏      | 870/2717 [04:24<07:58,  3.86it/s, est. speed input: 8404.72 toks/s, output: 6.57 toks/s]Processed prompts:  59%|█████▉    | 1601/2717 [06:39<04:03,  4.58it/s, est. speed input: 11044.74 toks/s, output: 8.01 toks/s]Processed prompts:  79%|███████▉  | 2147/2717 [08:55<02:10,  4.35it/s, est. speed input: 12275.04 toks/s, output: 8.01 toks/s]Processed prompts:  96%|█████████▌| 2608/2717 [09:30<00:19,  5.50it/s, est. speed input: 15297.36 toks/s, output: 9.15 toks/s]Processed prompts: 100%|██████████| 2717/2717 [09:30<00:00,  4.77it/s, est. speed input: 16236.51 toks/s, output: 9.53 toks/s]
/home/zch/Code/GraphICL/evaluation/code_GraphInstruct/deepseek/IclExamplesTestResults.py:128: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  main()
Evaluating task: connectivity
Processed prompts:   0%|          | 0/2687 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2687 [01:36<72:08:50, 96.70s/it, est. speed input: 21.03 toks/s, output: 0.02 toks/s]Processed prompts:  38%|███▊      | 1025/2687 [03:50<05:26,  5.09it/s, est. speed input: 7342.19 toks/s, output: 8.90 toks/s]Processed prompts:  75%|███████▌  | 2028/2687 [06:05<01:45,  6.23it/s, est. speed input: 10756.58 toks/s, output: 11.10 toks/s]Processed prompts:  97%|█████████▋| 2601/2687 [06:19<00:10,  8.59it/s, est. speed input: 16085.93 toks/s, output: 13.72 toks/s]Processed prompts: 100%|██████████| 2687/2687 [06:19<00:00,  7.09it/s, est. speed input: 16666.06 toks/s, output: 14.18 toks/s]
Evaluating task: bipartite
Processed prompts:   0%|          | 0/2013 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2013 [02:12<74:07:38, 132.63s/it, est. speed input: 6.60 toks/s, output: 0.02 toks/s]Processed prompts:  43%|████▎     | 858/2013 [04:27<05:06,  3.77it/s, est. speed input: 8290.45 toks/s, output: 6.43 toks/s]Processed prompts:  76%|███████▌  | 1530/2013 [06:42<01:51,  4.33it/s, est. speed input: 10934.23 toks/s, output: 7.61 toks/s]Processed prompts: 100%|██████████| 2013/2013 [06:42<00:00,  5.01it/s, est. speed input: 16252.47 toks/s, output: 10.01 toks/s]
Evaluating task: topology
Processed prompts:   0%|          | 0/872 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/872 [02:16<32:56:03, 136.12s/it, est. speed input: 68.03 toks/s, output: 0.01 toks/s]Processed prompts:  60%|██████    | 526/872 [04:36<02:35,  2.23it/s, est. speed input: 7841.89 toks/s, output: 3.81 toks/s]Processed prompts:  89%|████████▊ | 773/872 [05:29<00:35,  2.82it/s, est. speed input: 13000.02 toks/s, output: 4.69 toks/s]Processed prompts: 100%|██████████| 872/872 [05:29<00:00,  2.65it/s, est. speed input: 15414.91 toks/s, output: 5.29 toks/s]
Evaluating task: shortest
Processed prompts:   0%|          | 0/1392 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/1392 [02:16<52:50:30, 136.76s/it, est. speed input: 55.65 toks/s, output: 0.01 toks/s]Processed prompts:  30%|███       | 420/1392 [04:33<08:58,  1.80it/s, est. speed input: 7852.98 toks/s, output: 3.07 toks/s]Processed prompts:  60%|██████    | 839/1392 [06:51<03:53,  2.37it/s, est. speed input: 10414.20 toks/s, output: 4.07 toks/s]Processed prompts:  87%|████████▋ | 1217/1392 [08:22<01:00,  2.91it/s, est. speed input: 12792.79 toks/s, output: 4.85 toks/s]Processed prompts: 100%|██████████| 1392/1392 [08:22<00:00,  2.77it/s, est. speed input: 15520.51 toks/s, output: 5.54 toks/s]
Evaluating task: triangle
Processed prompts:   0%|          | 0/2756 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2756 [01:44<80:09:32, 104.75s/it, est. speed input: 84.64 toks/s, output: 0.02 toks/s]Processed prompts:  37%|███▋      | 1025/2756 [03:58<05:50,  4.94it/s, est. speed input: 7650.92 toks/s, output: 8.59 toks/s]Processed prompts:  72%|███████▏  | 1973/2756 [06:13<02:11,  5.96it/s, est. speed input: 10904.42 toks/s, output: 10.58 toks/s]Processed prompts:  93%|█████████▎| 2566/2756 [07:01<00:26,  7.22it/s, est. speed input: 14833.70 toks/s, output: 12.19 toks/s]Processed prompts: 100%|██████████| 2756/2756 [07:01<00:00,  6.54it/s, est. speed input: 16660.13 toks/s, output: 13.09 toks/s]
Evaluating task: flow
Processed prompts:   0%|          | 0/405 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/405 [01:44<11:44:42, 104.66s/it, est. speed input: 15.67 toks/s, output: 0.02 toks/s]Processed prompts: 100%|██████████| 405/405 [01:44<00:00,  3.87it/s, est. speed input: 15720.25 toks/s, output: 7.74 toks/s]
Evaluating task: hamilton
Processed prompts:   0%|          | 0/2097 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2097 [02:14<78:02:51, 134.05s/it, est. speed input: 9.25 toks/s, output: 0.01 toks/s]Processed prompts:  41%|████      | 853/2097 [04:29<05:33,  3.73it/s, est. speed input: 8268.05 toks/s, output: 6.34 toks/s]Processed prompts:  67%|██████▋   | 1403/2097 [06:46<02:59,  3.86it/s, est. speed input: 10812.17 toks/s, output: 6.91 toks/s]Processed prompts:  84%|████████▍ | 1757/2097 [09:04<01:41,  3.33it/s, est. speed input: 11968.93 toks/s, output: 6.45 toks/s]Processed prompts:  99%|█████████▉| 2085/2097 [09:11<00:02,  4.56it/s, est. speed input: 15676.08 toks/s, output: 7.56 toks/s]Processed prompts: 100%|██████████| 2097/2097 [09:11<00:00,  3.80it/s, est. speed input: 15844.96 toks/s, output: 7.60 toks/s]
Evaluating task: substructure
Processed prompts:   0%|          | 0/3186 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/3186 [02:13<118:02:02, 133.41s/it, est. speed input: 14.29 toks/s, output: 0.01 toks/s]Processed prompts:  25%|██▍       | 791/3186 [04:28<11:31,  3.46it/s, est. speed input: 8299.57 toks/s, output: 5.90 toks/s] Processed prompts:  42%|████▏     | 1337/3186 [06:43<08:16,  3.73it/s, est. speed input: 10906.68 toks/s, output: 6.63 toks/s]Processed prompts:  60%|██████    | 1927/3186 [08:59<05:16,  3.97it/s, est. speed input: 12204.27 toks/s, output: 7.15 toks/s]Processed prompts:  75%|███████▍  | 2388/3186 [11:16<03:32,  3.75it/s, est. speed input: 12921.99 toks/s, output: 7.06 toks/s]Processed prompts:  86%|████████▋ | 2748/3186 [13:34<02:10,  3.35it/s, est. speed input: 13351.47 toks/s, output: 6.74 toks/s]Processed prompts:  96%|█████████▌| 3064/3186 [14:28<00:32,  3.77it/s, est. speed input: 14991.92 toks/s, output: 7.06 toks/s]Processed prompts: 100%|██████████| 3186/3186 [14:28<00:00,  3.67it/s, est. speed input: 15928.20 toks/s, output: 7.34 toks/s]
Accuracy data has been saved to 'task_accuracies.txt'.
INFO 04-23 18:51:05 multiproc_worker_utils.py:140] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=2722951)[0;0m INFO 04-23 18:51:05 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2722953)[0;0m INFO 04-23 18:51:05 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2722952)[0;0m INFO 04-23 18:51:05 multiproc_worker_utils.py:247] Worker exiting
[rank0]:[W423 18:51:25.045963513 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/home/zch/anaconda3/envs/GraphICL/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
